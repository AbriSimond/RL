{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-04 18:01:42,405] Making new env: FrozenLake8x8-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "env =  gym.make('FrozenLake8x8-v0')# initialize environment\n",
    "#env =  gym.make('FrozenLake-v0')# initialize environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tf_RL_model:\n",
    "    def __init__(self):\n",
    "        self.action_table = {\n",
    "            'L' : 0,\n",
    "            'D' : 1,\n",
    "            'R' : 2,\n",
    "            'U' : 3\n",
    "        }\n",
    "        self.inv_action_table = {v: k for k, v in self.action_table.items()}\n",
    "        \n",
    "        # Learning paramters\n",
    "        self.learning_rate = 0.1\n",
    "        self.r_prob = 0.5\n",
    "        self.r_prob_decay = 0.98\n",
    "\n",
    "    def epsilon_greedy_action(self,state, ep):\n",
    "        if np.random.rand() < self.r_prob:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = sess.run(optimal_action, feed_dict={batch_state: [state],\n",
    "                                                        batch_stepNo: [ep]})[0]\n",
    "            #action = np.argmax(SV,1)[0]\n",
    "        return action\n",
    "    \n",
    "    def update_parameters(self,oldstate, newstate, action, reward, done, ep):\n",
    "        # Get the target values\n",
    "        if done == False:\n",
    "            target = reward + 0.99*sess.run(maxQ, \n",
    "                     feed_dict = {\n",
    "                    batch_state : newstate,\n",
    "                    batch_stepNo: ep\n",
    "                })\n",
    "        else:\n",
    "            target = reward\n",
    "        \n",
    "        \n",
    "         # Update parameters towards the target value\n",
    "        sess.run(optimizer, \n",
    "                 feed_dict = {\n",
    "                batch_state : oldstate,\n",
    "                batch_trueQ : target,\n",
    "                batch_action : action,\n",
    "                batch_stepNo : ep\n",
    "            })\n",
    "        return target\n",
    "    def get_optimal_policy(self):\n",
    "        optimal = sess.run(optimal_action, feed_dict = {batch_state : range(env.observation_space.n),\n",
    "                                                        batch_stepNo : np.repeat(100,env.observation_space.n)\n",
    "            })\n",
    "        optimal_name = [ self.inv_action_table[v] for v in optimal]\n",
    "\n",
    "        return np.reshape(optimal_name,(np.sqrt(env.observation_space.n), np.sqrt(env.observation_space.n)))\n",
    "    \n",
    "    def save_trained_policy(self):\n",
    "        optimal = sess.run(optimal_action, feed_dict = {batch_state : range(env.observation_space.n)\n",
    "            })\n",
    "        trained_policy = {}\n",
    "        for s in range(env.observation_space.n):\n",
    "            trained_policy[s] = sess.run(optimal_action, feed_dict = {batch_state : [s]})[0]\n",
    "        self.trained_policy = trained_policy\n",
    "        \n",
    "    def epsilon_decay(self):\n",
    "        self.r_prob *= self.r_prob_decay\n",
    "        self.r_prob = np.maximum(0.1,self.r_prob) # Never let random probability be less than 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### TF-MODEL SPECIFICATION\n",
    "model = tf_RL_model()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    batch_state = tf.placeholder(tf.int64, shape = ([None]), name = 'ph_state')\n",
    "    batch_stepNo = tf.placeholder(tf.int64, shape = ([None,]), name = 'ph_stepNo')\n",
    "    batch_action = tf.placeholder(tf.int64, shape = ([None]), name = 'ph_action')\n",
    "    batch_reward = tf.placeholder(tf.float32, shape = ([None]), name = 'ph_reward')\n",
    "    batch_trueQ = tf.placeholder(tf.float32, shape = ([None]), name = 'ph_trueQ')\n",
    "\n",
    "    #W = tf.Variable(tf.constant(value = np.linspace(start = 1, stop = env.observation_space.n*env.action_space.n, \n",
    "    #                                                num = env.observation_space.n*env.action_space.n, dtype='float32'), \n",
    "    #                shape = [env.observation_space.n, env.action_space.n]\n",
    "    #               ))\n",
    "    \n",
    "    hsize = 100\n",
    "    # One hot vectors of state and action indicies:\n",
    "                        \n",
    "    state = tf.one_hot(indices = batch_state,\n",
    "                           depth = env.observation_space.n,\n",
    "                          on_value = 1.0, off_value = 0.0)\n",
    "    W1 = tf.get_variable(shape = [env.observation_space.n, hsize],\n",
    "        initializer = tf.random_normal_initializer(mean=0, stddev= 0.1) , name = 'W1')  \n",
    "\n",
    "    Wstep = tf.get_variable(shape = [200, hsize],\n",
    "        initializer = tf.random_normal_initializer(mean=0, stddev= 0.1) , name = 'Wstep')\n",
    "    \n",
    "    stepNo = tf.one_hot(indices = batch_stepNo,\n",
    "                           depth = 200,\n",
    "                          on_value = 1.0, off_value = 0.0)\n",
    "    timelayer = tf.nn.relu6(tf.matmul(stepNo,Wstep))\n",
    "\n",
    "    hidden = tf.nn.relu(tf.matmul(state,W1)) + timelayer\n",
    "\n",
    "    \n",
    "    W2 = tf.get_variable(shape = [hsize, env.action_space.n],\n",
    "        initializer = tf.random_normal_initializer(mean=0, stddev= 0.1) , name = 'W2')  \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    Qval = tf.matmul(hidden,W2)\n",
    "\n",
    "    \n",
    "    action = tf.one_hot(indices = batch_action,\n",
    "                           depth = env.action_space.n,\n",
    "                          on_value = 1.0, off_value = 0.0)\n",
    "    \n",
    "    optimal_action = tf.argmax(Qval,1)\n",
    "    \n",
    "    maxQ = tf.reduce_max(Qval,1)\n",
    "    \n",
    "    Q_a = tf.reduce_sum(tf.mul(Qval, action),1) # only get non-zero chosen actions\n",
    "    loss = tf.nn.l2_loss( batch_trueQ - Q_a)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(model.learning_rate).minimize(loss)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 \t r_prob:0.5\t run_avg: 0.0\n",
      "[['L' 'R' 'D' 'R' 'D' 'L' 'L' 'L']\n",
      " ['L' 'D' 'L' 'L' 'R' 'L' 'R' 'L']\n",
      " ['L' 'R' 'L' 'L' 'L' 'L' 'R' 'L']\n",
      " ['L' 'L' 'D' 'L' 'L' 'U' 'L' 'L']\n",
      " ['L' 'R' 'L' 'D' 'L' 'L' 'L' 'D']\n",
      " ['R' 'L' 'L' 'L' 'R' 'L' 'D' 'D']\n",
      " ['L' 'L' 'D' 'L' 'L' 'L' 'L' 'D']\n",
      " ['D' 'L' 'L' 'L' 'D' 'L' 'L' 'L']]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(graph=graph)\n",
    "with sess as session:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Probability of random action\n",
    "    #\n",
    "    model.r_prob_decay = 0.99995\n",
    "    model.learning_rate = 0.1\n",
    "\n",
    "    # Logging variables\n",
    "    visits = np.zeros([env.observation_space.n])\n",
    "\n",
    "    run_reward = [] # running average initialize\n",
    "    num_episodes = 50000 # Total number of games\n",
    "    #old_optimal = model.get_optimal_policy() # Initial optimal strategy\n",
    "    \n",
    "    newweight = sess.run(tf.trainable_variables())[0]\n",
    "    for n in xrange(num_episodes + 1):\n",
    "        newstate = env.reset()\n",
    "        done = False\n",
    "\n",
    "        for ep in xrange(200):\n",
    "            # Current state\n",
    "            eprate = ep/200.0\n",
    "            oldstate = newstate\n",
    "            # Perform epsilon-greedy action:\n",
    "            action = model.epsilon_greedy_action(oldstate, eprate)\n",
    "            # Take action and observe state and reward\n",
    "            newstate, reward, done, info = env.step(action)\n",
    "            visits[newstate] += 1\n",
    "\n",
    "            # Reward modifications:\n",
    "            # if (done is True and reward == 0):\n",
    "            #    reward = -0.2\n",
    "            # reward -= 0.01 # cost of life\n",
    "            \n",
    "            \n",
    "            # Update parameters\n",
    "            tQ = model.update_parameters([oldstate], [newstate], [action], [reward],done, [eprate]) # brackets fordi det er enkelttall inn\n",
    "            \n",
    "            # Debugging:\n",
    "            if False:\n",
    "                oldweight = newweight\n",
    "                newweight = sess.run(tf.trainable_variables())[0]\n",
    "                diff = newweight - oldweight\n",
    "                print 'old: ' + str(oldstate) + '\\t new: ' + str(newstate) +\\\n",
    "                '\\t a: ' + str(action) + '\\t r: ' + str(reward) +\\\n",
    "                '\\t targetQ: ' + str(np.round(tQ[0],3)) + '\\t Qval: ' + str(np.round(oldweight[oldstate,action],3)) + '\\t Qdiff: ' + str(np.round(diff[oldstate,action],3)) \n",
    "\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            # print 'episode ' + str(n) + 'reached maximum steps. skipping...'\n",
    "\n",
    "        # Decay epsilon\n",
    "        model.epsilon_decay()\n",
    "        run_reward.append(reward)\n",
    "\n",
    "        if n % int(num_episodes/10) == 0:\n",
    "            \n",
    "            run_avg = np.mean(run_reward)\n",
    "            run_reward = []\n",
    "            print 'step: ' + str(n) + ' \\t r_prob:' + str(np.round(model.r_prob,2)) + '\\t run_avg: ' + str(np.round(run_avg,3))\n",
    "            print model.get_optimal_policy()\n",
    "            #print sess.run(tf.trainable_variables())[0]\n",
    "    #model.save_trained_policy()\n",
    "    save_path = saver.save(sess, 'tf-frozen8x8.ckpt')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Number of visits per episode:\n",
    "plt.plot(visits/num_episodes)\n",
    "#num_episodes/visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passive_policy = {\n",
    "    0 : 2,\n",
    "    1 : 2,\n",
    "    2 : 2,\n",
    "    3 : 2,\n",
    "    4 : 2,\n",
    "    5 : 2,\n",
    "    6 : 2,\n",
    "    7 : 1,\n",
    "    8 : 3,\n",
    "    9:3,\n",
    "    10:3,\n",
    "    11:3,\n",
    "    12:3,\n",
    "    13:3,\n",
    "    14:3,\n",
    "    15:2,\n",
    "    23:2,\n",
    "    31:2,\n",
    "    39:2,\n",
    "    47:2,\n",
    "    55:2,\n",
    "    64:2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.monitor.start('recordings', force=True)\n",
    "num_episodes = 1000\n",
    "R = []\n",
    "\n",
    "\n",
    "def moving_average(a, n = 100) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "\n",
    "model.r_prob = 0 # ensure that only the optimal solution is used\n",
    "\n",
    "for n in xrange(num_episodes):\n",
    "    newstate = env.reset()\n",
    "    done = False\n",
    "    k = 0\n",
    "    while not done:\n",
    "        # Current state\n",
    "        oldstate = newstate\n",
    "        \n",
    "        # Perform epsilon-greedy action:\n",
    "        action = model.trained_policy[oldstate]\n",
    "        #action = passive_policy[oldstate]\n",
    "        # Take action and observe state and reward\n",
    "        newstate, reward, done, info = env.step(action)\n",
    "        k += 1\n",
    "    R.append(reward)\n",
    "    print 'Episode: ' + str(n) +'\\t steps: ' + str(k) + '\\t state: ' + str(newstate), '\\t reward: ' + str(reward)\n",
    "    #if n % 1000 == 101:\n",
    "    #    MA = np.max(moving_average(R))\n",
    "    #    print 'step: ' + str(n) + '\\t MRA: ' + str(MA)\n",
    "    #    if MA > 0.99:\n",
    "    #        break\n",
    "env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MV = moving_average(R, n = 100)\n",
    "print np.max(MV)\n",
    "\n",
    "plt.plot(MV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gym.upload('/notebooks/hjem/RL/recordings', api_key='sk_znZbtlUTlu1nJNqFLRIyA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.monitor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
