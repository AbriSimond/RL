{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-07 08:23:41,479] Making new env: FrozenLake-v0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#env =  gym.make('FrozenLake8x8-v0')# initialize environment\n",
    "env =  gym.make('FrozenLake-v0')# initialize environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class tf_RL_model:\n",
    "    def __init__(self):\n",
    "        self.action_table = {\n",
    "            'L' : 0,\n",
    "            'D' : 1,\n",
    "            'R' : 2,\n",
    "            'U' : 3\n",
    "        }\n",
    "        self.inv_action_table = {v: k for k, v in self.action_table.items()}\n",
    "        \n",
    "        # Learning paramters\n",
    "        self.learning_rate = 0.1\n",
    "        self.r_prob = 0.5\n",
    "        self.r_prob_decay = 0.98\n",
    "\n",
    "    def epsilon_greedy_action(self,state, eprate):\n",
    "        if np.random.rand() < self.r_prob:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = sess.run(optimal_action, feed_dict={batch_state: [state],\n",
    "                                                        batch_eprate: eprate})[0]\n",
    "            #action = np.argmax(SV,1)[0]\n",
    "        return action\n",
    "    \n",
    "    def update_parameters(self,oldstate, newstate, action, reward, done, eprate):\n",
    "        # Get the target values\n",
    "        \n",
    "        Qtarget = sess.run(maxQ, \n",
    "                     feed_dict = {\n",
    "                    batch_state : newstate,\n",
    "                    batch_eprate: eprate\n",
    "                })#.reshape((batch_size,1))\n",
    "        \n",
    "        #print 'Qtarget:'\n",
    "        #print Qtarget\n",
    "        #print Qtarget.shape\n",
    "        #print 'reward:'\n",
    "        #print reward\n",
    "        #print reward.shape\n",
    "        \n",
    "        #print 'Done:'\n",
    "        #print done\n",
    "        #print done.shape\n",
    "        \n",
    "        target = reward + 0.99*Qtarget*(1-done)\n",
    "        \n",
    "        #print 'Target:'\n",
    "        #print target\n",
    "    \n",
    "    \n",
    "        # Update parameters towards the target value\n",
    "        sess.run(optimizer, \n",
    "                 feed_dict = {\n",
    "                batch_state : oldstate,\n",
    "                batch_trueQ : target,\n",
    "                batch_eprate : eprate,\n",
    "                batch_action : action\n",
    "            })\n",
    "        return target\n",
    "    \n",
    "    def get_optimal_policy(self):\n",
    "        optimal = sess.run(optimal_action, feed_dict = {batch_state : range(env.observation_space.n),\n",
    "                                                        batch_eprate : np.repeat(100,env.observation_space.n).reshape((100, 1))\n",
    "            })\n",
    "        optimal_name = [ self.inv_action_table[v] for v in optimal]\n",
    "\n",
    "        return np.reshape(optimal_name,(np.sqrt(env.observation_space.n), np.sqrt(env.observation_space.n)))\n",
    "    \n",
    "    def save_trained_policy(self):\n",
    "        optimal = sess.run(optimal_action, feed_dict = {batch_state : range(env.observation_space.n)\n",
    "            })\n",
    "        trained_policy = {}\n",
    "        for s in range(env.observation_space.n):\n",
    "            trained_policy[s] = sess.run(optimal_action, feed_dict = {batch_state : [s]})[0]\n",
    "        self.trained_policy = trained_policy\n",
    "        \n",
    "    def epsilon_decay(self):\n",
    "        self.r_prob *= self.r_prob_decay\n",
    "        self.r_prob = np.maximum(0.1,self.r_prob) # Never let random probability be less than 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### TF-MODEL SPECIFICATION\n",
    "model = tf_RL_model()\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    batch_state = tf.placeholder(tf.int64, shape = ([None,]), name = 'ph_state')\n",
    "    batch_eprate = tf.placeholder(tf.float32, shape = ([None,]), name = 'ph_eprate')\n",
    "    batch_action = tf.placeholder(tf.int64, shape = ([None,]), name = 'ph_action')\n",
    "    batch_reward = tf.placeholder(tf.float32, shape = ([None,]), name = 'ph_reward')\n",
    "    batch_trueQ = tf.placeholder(tf.float32, shape = ([None,]), name = 'ph_trueQ')\n",
    "    \n",
    "    \n",
    "    #W = tf.Variable(tf.constant(value = np.linspace(start = 1, stop = env.observation_space.n*env.action_space.n, \n",
    "    #                                                num = env.observation_space.n*env.action_space.n, dtype='float32'), \n",
    "    #                shape = [env.observation_space.n, env.action_space.n]\n",
    "    #               ))\n",
    "    \n",
    "    \n",
    "    # One hot vectors of state and action indicies:\n",
    "                        \n",
    "    state = tf.one_hot(indices = batch_state,\n",
    "                           depth = env.observation_space.n,\n",
    "                       on_value = 1.0, off_value = 0.0)\n",
    "    \n",
    "    eprateTF = tf.reshape(batch_eprate,[-1,1])\n",
    "    \n",
    "    superstate = tf.concat(1,[eprateTF, state])\n",
    "    hsize = 20\n",
    "    W1 = tf.get_variable(shape = [env.observation_space.n + 1, hsize],\n",
    "        initializer = tf.random_normal_initializer(mean=0, stddev= 0.1) , name = 'W1')  \n",
    "    \n",
    "\n",
    "    hidden = tf.nn.relu(tf.matmul(superstate,W1))\n",
    "    \n",
    "    W2 = tf.get_variable(shape = [hsize, env.action_space.n],\n",
    "        initializer = tf.random_normal_initializer(mean=0, stddev= 0.1) , name = 'W2')\n",
    "    \n",
    "    Qval = tf.matmul(hidden,W2)\n",
    "\n",
    "    \n",
    "    action = tf.one_hot(indices = batch_action,\n",
    "                           depth = env.action_space.n,\n",
    "                          on_value = 1.0, off_value = 0.0)\n",
    "    \n",
    "    optimal_action = tf.argmax(Qval,1)\n",
    "    maxQ = tf.reduce_max(Qval,1)\n",
    "    \n",
    "    mQ = tf.mul(Qval, action)\n",
    "    Q_a = tf.reduce_sum(mQ,1) # only get non-zero chosen actions\n",
    "    loss = tf.nn.l2_loss( batch_trueQ - Q_a)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(model.learning_rate).minimize(loss)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(graph=graph)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "#saver.restore(sess, 'tf-frozen8x8.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dict1 = {batch_state : [2.0,3], batch_eprate: [0.1, 0.1], batch_action : [1,2]}\n",
    "train_dict2 = {batch_state : [3.0,3], batch_eprate: [0.1, 0.1], batch_action : [1,2]}\n",
    "\n",
    "\n",
    "#model.update_parameters()\n",
    "\n",
    "#sess.run(superstate, feed_dict = train_dict)#.shape\n",
    "v1 =  sess.run(Q_a, feed_dict= train_dict1)#.shape\n",
    "print v1\n",
    "print '---------------------------------------------------------'\n",
    "v2 = sess.run(Q_a, feed_dict= train_dict2) #.shape\n",
    "print v2\n",
    "print '--- IS EQUAL ----'\n",
    "v1 == v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_array = train_array[(0,1),:]\n",
    "train_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "done = train_array[:,4]\n",
    "0.99*done*(1-done)\n",
    "\n",
    "print done.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Qtarget,reward, done, target = tQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "model.r_prob = 0\n",
    "#action = model.epsilon_greedy_action(oldstate, [eprate]) # Perform epsilon-greedy action:\n",
    "tQ = model.update_parameters(            oldstate = train_array[:,0],#.reshape((batch_size,1)), \n",
    "                                         newstate = train_array[:,1],#.reshape((batch_size,1)), \n",
    "                                         action   = train_array[:,2],#.reshape((batch_size,1)), \n",
    "                                         reward   = train_array[:,3],#.reshape((batch_size,1)),\n",
    "                                         done     = train_array[:,4],#.reshape((batch_size,1)), \n",
    "                                         eprate   = train_array[:,5] #.reshape((batch_size,1))\n",
    "                             ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.00285918465\n",
      "step: 0 \t r_prob:1.0\t run_avg: 0.0\n",
      "step: 100 \t r_prob:1.46440787585e+30\t run_avg: 0.0\n",
      "step: 200 \t r_prob:2.14142905631e+60\t run_avg: 0.0\n",
      "step: 300 \t r_prob:3.13144888036e+90\t run_avg: 0.03\n",
      "step: 400 \t r_prob:4.57917205401e+120\t run_avg: 0.0\n",
      "step: 500 \t r_prob:6.69620278067e+150\t run_avg: 0.0\n",
      "step: 600 \t r_prob:9.79197356007e+180\t run_avg: 0.01\n",
      "step: 700 \t r_prob:1.43189729077e+211\t run_avg: 0.0\n",
      "step: 800 \t r_prob:2.09388826343e+241\t run_avg: 0.0\n",
      "step: 900 \t r_prob:3.06192915371e+271\t run_avg: 0.01\n",
      "step: 1000 \t r_prob:4.47751215099e+301\t run_avg: 0.01\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session(graph=graph)\n",
    "\n",
    "\n",
    "mem_size = 500\n",
    "replay_memory = np.zeros([mem_size,6])\n",
    "batch_size = 10\n",
    "\n",
    "with sess as session:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    # Probability of random action\n",
    "    #\n",
    "    num_episodes = 1000 # Total number of games\n",
    "    model.learning_rate = 0.005\n",
    "    model.r_prob_decay = 2*np.exp(1/(0.7*num_episodes))\n",
    "    print model.r_prob_decay\n",
    "    # Logging variables\n",
    "    visits = np.zeros([env.observation_space.n])\n",
    "\n",
    "    run_reward = [] # running average initialize\n",
    "    #old_optimal = model.get_optimal_policy() # Initial optimal strategy\n",
    "    \n",
    "    newweight = sess.run(tf.trainable_variables())[0]\n",
    "    for n in xrange(num_episodes + 1):\n",
    "        newstate = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        ### WALK AROUND ###\n",
    "        maxsteps = 100\n",
    "        for ep in xrange(maxsteps):\n",
    "            \n",
    "            eprate = ep/maxsteps\n",
    "            oldstate = newstate\n",
    "            action = model.epsilon_greedy_action(oldstate, [eprate]) # Perform epsilon-greedy action:\n",
    "            newstate, reward, done, info = env.step(action) # Take action and observe state and reward\n",
    "            visits[newstate] += 1\n",
    "\n",
    "            # Reward modifications:\n",
    "            # if (done is True and reward == 0):\n",
    "            #    reward = -0.2\n",
    "            # reward -= 0.01 # cost of life\n",
    "            \n",
    "            # Save transition: \n",
    "            replay_memory[np.random.choice(mem_size),:] = oldstate, newstate, action, reward, done, eprate\n",
    "            \n",
    "            # Debugging:\n",
    "            if False and reward == 1.0:\n",
    "                oldweight = newweight\n",
    "                newweight = sess.run(tf.trainable_variables())[0]\n",
    "                diff = newweight - oldweight\n",
    "                print 'old: ' + str(oldstate) + '\\t new: ' + str(newstate) +\\\n",
    "                '\\t a: ' + str(action) + '\\t r: ' + str(reward) +\\\n",
    "                '\\t targetQ: ' + str(np.round(tQ[0],3)) + '\\t Qval: ' + str(np.round(oldweight[oldstate,action],3)) + '\\t Qdiff: ' + str(np.round(diff[oldstate,action],3))                         \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        model.epsilon_decay()\n",
    "        run_reward.append(reward)\n",
    "        \n",
    "        ### UPDATE PARAMETERS ###\n",
    "        if n > 1:\n",
    "            train_array = replay_memory[np.random.choice(mem_size, size = batch_size),:]\n",
    "            tQ = model.update_parameters(            \n",
    "                oldstate = train_array[:,0],#.reshape((batch_size,1)), \n",
    "                                         newstate = train_array[:,1],#.reshape((batch_size,1)), \n",
    "                                         action   = train_array[:,2],#.reshape((batch_size,1)), \n",
    "                                         reward   = train_array[:,3],#.reshape((batch_size,1)),\n",
    "                                         done     = train_array[:,4],#.reshape((batch_size,1)), \n",
    "                                         eprate   = train_array[:,5] #.reshape((batch_size,1))\n",
    "                             ) \n",
    "\n",
    "            # print 'episode ' + str(n) + 'reached maximum steps. skipping...'\n",
    "\n",
    "\n",
    "\n",
    "        if n % int(num_episodes/10) == 0:\n",
    "            run_avg = np.mean(run_reward)\n",
    "            run_reward = []\n",
    "            print 'step: ' + str(n) + ' \\t r_prob:' + str(np.round(model.r_prob,2)) + '\\t run_avg: ' + str(np.round(run_avg,4))\n",
    "            #print model.get_optimal_policy()\n",
    "            #print sess.run(tf.trainable_variables())#[0]\n",
    "    # model.save_trained_policy()\n",
    "    save_path = saver.save(sess, 'tf-frozen8x8.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Number of visits per episode:\n",
    "plt.plot(visits/num_episodes)\n",
    "#num_episodes/visits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passive_policy = {\n",
    "    0 : 2,\n",
    "    1 : 2,\n",
    "    2 : 2,\n",
    "    3 : 2,\n",
    "    4 : 2,\n",
    "    5 : 2,\n",
    "    6 : 2,\n",
    "    7 : 1,\n",
    "    8 : 3,\n",
    "    9:3,\n",
    "    10:3,\n",
    "    11:3,\n",
    "    12:3,\n",
    "    13:3,\n",
    "    14:3,\n",
    "    15:2,\n",
    "    23:2,\n",
    "    31:2,\n",
    "    39:2,\n",
    "    47:2,\n",
    "    55:2,\n",
    "    64:2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(graph=graph)\n",
    "saver.restore(sess, 'tf-frozen8x8.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env.monitor.start('recordings', force=True)\n",
    "num_episodes = 1000\n",
    "R = []\n",
    "\n",
    "\n",
    "def moving_average(a, n = 100) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "\n",
    "model.r_prob = 0 # ensure that only the optimal solution is used\n",
    "\n",
    "for n in xrange(num_episodes):\n",
    "    newstate = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    for ep in xrange(200):\n",
    "        # Current state\n",
    "        oldstate = newstate\n",
    "        \n",
    "        # Perform epsilon-greedy action:\n",
    "        eprate = ep/200.0\n",
    "        action = model.epsilon_greedy_action(oldstate, [eprate])\n",
    "        #action = passive_policy[oldstate]\n",
    "        # Take action and observe state and reward\n",
    "        newstate, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    R.append(reward)\n",
    "    if False and reward != 1.0:\n",
    "        print 'Episode: ' + str(n) +'\\t steps: ' + str(ep) + '\\t state: ' + str(newstate), '\\t reward: ' + str(reward)\n",
    "    #if n % 1000 == 101:\n",
    "    #    MA = np.max(moving_average(R))\n",
    "    #    print 'step: ' + str(n) + '\\t MRA: ' + str(MA)\n",
    "    #    if MA > 0.99:\n",
    "    #        break\n",
    "env.monitor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MV = moving_average(R, n = 100)\n",
    "print np.max(MV)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(MV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gym.upload('/notebooks/hjem/RL/recordings', api_key='sk_znZbtlUTlu1nJNqFLRIyA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env.monitor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
