{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float32)\n",
    "\n",
    "def run_episode(env,agent, n_episodes=1, ep = 0.05):\n",
    "    '''\n",
    "    A pure run_episode with an agent. Does nothing else.\n",
    "    Outputs 3 lists with one entry for every step: state, action and reward\n",
    "    '''\n",
    "    state, action, reward, value = [], [], [], []\n",
    "    episode_number = 0\n",
    "    observation = env.reset()\n",
    "    prev_x = None # used in computing the difference frame\n",
    "\n",
    "    while True:\n",
    "        # Process observation\n",
    "        cur_x = prepro(observation)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(80*80)\n",
    "        x = x.reshape((1, -1))\n",
    "        prev_x = cur_x\n",
    "\n",
    "        # Execute a step:\n",
    "        v, a = agent(x,ep=ep)\n",
    "        observation, r, done, info = env.step(a)\n",
    "\n",
    "        # Store variables from run\n",
    "        state.append(x) # note that this is the state before the action was taken\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        value.append(v)\n",
    "\n",
    "        if done:\n",
    "            episode_number += 1\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "\n",
    "            if episode_number >= n_episodes:\n",
    "                \n",
    "                reward = np.vstack(reward)\n",
    "                action = np.vstack(action)\n",
    "                state = np.vstack(state)\n",
    "                value = np.vstack(value)\n",
    "                return state,action,reward, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "inp = Input(shape=(80*80,))\n",
    "h = Dense(200, activation='relu')(inp)\n",
    "#h = Dense(500, activation='relu')(h)\n",
    "#h = Dense(200, activation='relu')(h)\n",
    "V = Dense(1,activation=\"tanh\", name = \"critic\")(h)\n",
    "pi = Dense(6,activation='softmax',name=\"policy-output\")(h)\n",
    "\n",
    "model = Model(inputs=inp, \n",
    "                  outputs=[V,pi])\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "def weighted_crossentropy(y_true,y_pred):\n",
    "    reward_true = y_true[:,0]\n",
    "    action_true = y_true[:,1:]\n",
    "    ce = K.categorical_crossentropy(y_pred, action_true)\n",
    "    wce = ce*reward_true\n",
    "    print ce\n",
    "    print wce\n",
    "    return wce\n",
    "\n",
    "model.compile(loss=[mean_squared_error, weighted_crossentropy],\n",
    "               optimizer=keras.optimizers.RMSprop(lr=0.005, rho=0.9, epsilon=1e-08, decay=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agent(state, ep = 0.05):\n",
    "    V, aprob = model.predict(state)\n",
    "    \n",
    "    if np.random.uniform() < ep:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(aprob)\n",
    "    return V,action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "#from keras.callbacks import \n",
    "env = gym.make(\"Pong-v0\")\n",
    "running_reward = -20.5\n",
    "batch = 0\n",
    "batch_ep = 5 #how many episodes to run per batch\n",
    "ep = 0.1 # probability of random action\n",
    "while True:\n",
    "    state,action,reward, value = run_episode(env, agent, n_episodes=batch_ep, ep = ep)\n",
    "\n",
    "    # Update model\n",
    "    Vt = np.roll(value,-1) # take lead of Vt\n",
    "    R = reward + (reward==0.0)*Vt*0.99 # Value is reward if done (same as reward!=0), else it is predicted value else\n",
    "    weight = (R-Vt)\n",
    "    pi_true_weights = np.hstack([weight, to_categorical(action)]) # Format truth value to be in accordance to weighted_crossentropy\n",
    "    h = model.fit(state, [R, pi_true_weights], verbose=0)\n",
    "    vloss = h.history.items()[2][1][0]\n",
    "    piloss = h.history.items()[1][1][0]\n",
    "    \n",
    "    # Book-keeping\n",
    "    batch +=1\n",
    "    reward_sum = np.sum(reward)/batch_ep\n",
    "    running_reward = running_reward * 0.99 + reward_sum * 0.01\n",
    "    print 'batch %d: tot reward: %.0f run mean: %.3f V: %.3f. V-loss: %.3f. pi-loss: %.3f.' %\\\n",
    "    (batch, reward_sum, running_reward, np.mean(value), vloss, piloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(model.predict(state)[1],0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
