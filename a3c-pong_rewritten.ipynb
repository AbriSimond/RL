{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float32)\n",
    "\n",
    "def run_episode(env,agent, n_episodes=1, ep = 0.05):\n",
    "    '''\n",
    "    A pure run_episode with an agent. Does nothing else.\n",
    "    Outputs 3 lists with one entry for every step: state, action and reward\n",
    "    '''\n",
    "    state, action, reward, value = [], [], [], []\n",
    "    episode_number = 0\n",
    "    observation = env.reset()\n",
    "    prev_x = None # used in computing the difference frame\n",
    "\n",
    "    while True:\n",
    "        # Process observation\n",
    "        cur_x = prepro(observation)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(80*80)\n",
    "        x = x.reshape((1, -1))\n",
    "        prev_x = cur_x\n",
    "\n",
    "        # Execute a step:\n",
    "        v, a = agent(x,ep=ep)\n",
    "        observation, r, done, info = env.step(a)\n",
    "\n",
    "        # Store variables from run\n",
    "        state.append(x) # note that this is the state before the action was taken\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        value.append(v)\n",
    "\n",
    "        if done:\n",
    "            episode_number += 1\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "\n",
    "            if episode_number >= n_episodes:\n",
    "                \n",
    "                reward = np.vstack(reward)\n",
    "                action = np.vstack(action)\n",
    "                state = np.vstack(state)\n",
    "                value = np.vstack(value)\n",
    "                return state,action,reward, value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Neg:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"mul_3:0\", shape=(?,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "inp = Input(shape=(80*80,))\n",
    "h = Dense(200, activation='relu')(inp)\n",
    "V = Dense(1,activation=\"tanh\", name = \"critic\")(h)\n",
    "pi = Dense(6,activation='softmax',name=\"policy-output\")(h)\n",
    "\n",
    "model = Model(inputs=inp, \n",
    "                  outputs=[V,pi])\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "def weighted_crossentropy(y_true,y_pred):\n",
    "    reward_true = y_true[:,0]\n",
    "    action_true = y_true[:,1:]\n",
    "    ce = K.categorical_crossentropy(y_pred, action_true)\n",
    "    wce = ce* reward_true\n",
    "    print ce\n",
    "    print wce\n",
    "    return wce\n",
    "\n",
    "model.compile(loss=[mean_squared_error, weighted_crossentropy],\n",
    "               optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agent(state, ep = 0.05):\n",
    "    V, aprob = model.predict(state)\n",
    "    \n",
    "    if np.random.uniform() < ep:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(aprob)\n",
    "    return V,action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-04-05 11:31:10,014] Making new env: Pong-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: reward total was -20.550000. running mean: -20.500500. value: -0.086372.\n",
      "batch 2: reward total was -21.000000. running mean: -20.505495. value: -0.153853.\n",
      "batch 3: reward total was -20.750000. running mean: -20.507940. value: -0.246154.\n",
      "batch 4: reward total was -20.500000. running mean: -20.507861. value: -0.259953.\n",
      "batch 5: reward total was -20.100000. running mean: -20.503782. value: -0.316833.\n",
      "batch 6: reward total was -21.000000. running mean: -20.508744. value: -0.262637.\n",
      "batch 7: reward total was -20.500000. running mean: -20.508657. value: -0.340091.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "env = gym.make(\"Pong-v0\")\n",
    "running_reward = -20.5\n",
    "batch = 0\n",
    "batch_ep = 20 #how many episodes to run per batch\n",
    "ep = 0.1\n",
    "while True:\n",
    "    state,action,reward, value = run_episode(env, agent, n_episodes=batch_ep, ep = ep)\n",
    "\n",
    "    # Update model\n",
    "    V_true = reward + (reward==0.0)*value # Value is reward if done (same as reward!=0), else it is predicted value else\n",
    "    pi_true_weights = np.hstack([reward, to_categorical(action)]) # Format truth value to be in accordance to weighted_crossentropy\n",
    "    totloss, vloss, piloss = model.train_on_batch(state, [V_true, pi_true_weights])\n",
    "    #print totloss, vloss, piloss\n",
    "    # Book-keeping\n",
    "    batch +=1\n",
    "    reward_sum = np.sum(reward)/batch_ep\n",
    "    running_reward = running_reward * 0.99 + reward_sum * 0.01\n",
    "    print 'batch %d: reward total was %f. running mean: %f. value: %f.' % (batch, reward_sum, running_reward, np.mean(value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
