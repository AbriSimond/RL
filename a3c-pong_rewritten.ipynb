{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
=======
   "execution_count": 1,
   "metadata": {},
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepro(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I = I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float32)#.ravel()\n",
    "\n",
    "def run_episode(env,agent, n_episodes=1, ep = 0.05):\n",
    "    '''\n",
    "    A pure run_episode with an agent. Does nothing else.\n",
    "    Outputs 4 lists with one entry for every step: state, action, reward and value\n",
    "    '''\n",
    "    state, action, reward, value = [], [], [], []\n",
    "    episode_number = 0\n",
    "    observation = env.reset()\n",
    "    prev_x = None # used in computing the difference frame\n",
    "\n",
    "    while True:\n",
    "        # Process observation\n",
    "        cur_x = prepro(observation)\n",
    "        x = cur_x - prev_x if prev_x is not None else np.zeros(80*80)\n",
    "        x = x.reshape((1, -1))\n",
    "        prev_x = cur_x\n",
    "\n",
    "        # Execute a step:\n",
    "        v, a = agent(x,ep=ep)\n",
    "        observation, r, done, info = env.step(a)\n",
    "\n",
    "        # Store variables from run\n",
    "        state.append(x) # note that this is the state before the action was taken\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        value.append(v)\n",
    "\n",
    "        if done:\n",
    "            episode_number += 1\n",
    "            observation = env.reset() # reset env\n",
    "            prev_x = None\n",
    "\n",
    "            if episode_number >= n_episodes:\n",
    "                \n",
    "                reward = np.vstack(reward)\n",
    "                action = np.vstack(action)\n",
    "                state = np.vstack(state)\n",
    "                value = np.vstack(value)\n",
    "                return state,action,reward, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_state(x):\n",
    "    plt.imshow(x.reshape(80,80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 59,
=======
   "execution_count": 4,
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Input, Model\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "inp = Input(shape=(80*80,))\n",
    "h = Dense(200, activation='relu')(inp)\n",
    "#h = Dense(500, activation='relu')(h)\n",
    "#h = Dense(200, activation='relu')(h)\n",
    "V = Dense(1,activation=\"tanh\", name = \"critic\")(h)\n",
    "pi = Dense(6,activation='softmax',name=\"policy-output\")(h)\n",
    "\n",
    "model = Model(inputs=inp, \n",
    "                  outputs=[V,pi])\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.losses import mean_squared_error\n",
    "\n",
    "def weighted_crossentropy(y_true,y_pred):\n",
    "    weight = y_true[:,0]\n",
    "    action_true = y_true[:,1:]\n",
    "    ce = K.categorical_crossentropy(y_pred, action_true)\n",
<<<<<<< HEAD
    "    wce = ce*reward_true\n",
=======
    "    wce = ce*weight\n",
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
    "    return wce\n",
    "\n",
    "def custom_accuracy(y_true,y_pred):\n",
    "    reward_true = y_true[:,0]\n",
    "    action_true = y_true[:,1:]\n",
    "    return keras.metrics.categorical_accuracy(action_true, y_pred)\n",
    "\n",
    "\n",
    "#tb = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "\n",
    "model.compile(loss=[mean_squared_error, weighted_crossentropy],\n",
<<<<<<< HEAD
    "               optimizer=keras.optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=1e-08, decay=0.0))"
=======
    "               optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "             )"
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
=======
   "execution_count": 5,
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def agent(state, ep = 0.05):\n",
    "    V, aprob = model.predict(state)\n",
    "    aprob = aprob.ravel()\n",
    "    # We know only two actions actually matters. delete prob to all others\n",
    "    \n",
    "    aprob[[0,1,4,5]] = 0\n",
    "    aprob = aprob /np.sum(aprob)\n",
    "\n",
    "    \n",
    "    if np.random.uniform() < ep:\n",
    "        action = np.random.choice([2,3])# env.action_space.sample()\n",
    "    else:\n",
<<<<<<< HEAD
    "        #action = np.argmax(aprob)\n",
    "        action = np.random.choice(6, p=aprob[0])\n",
=======
    "        action = np.random.choice(6, p=aprob)\n",
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
    "    return V,action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
=======
   "execution_count": 6,
   "metadata": {
    "collapsed": true
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "def discount_rewards(r):\n",
    "  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "  discounted_r = np.zeros_like(r)\n",
    "  running_add = 0\n",
    "  for t in reversed(xrange(0, r.size)):\n",
    "    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "    running_add = running_add * gamma + r[t]\n",
    "    discounted_r[t] = running_add\n",
    "  return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2017-04-21 06:03:33,396] Making new env: Pong-v0\n"
=======
      "[2017-04-21 08:47:38,719] Making new env: Pong-v0\n"
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "batch 1: total reward: -20.666667. run mean: -20.501667. value: -0.005250. V-loss: 0.018649. pi-loss: -0.041914.\n",
      "batch 2: total reward: -20.333333. run mean: -20.499983. value: -0.582483. V-loss: 0.003276. pi-loss: -0.955680.\n"
=======
      "batch 1: tot reward: -21 run mean: -20.505 V: -0.011. V-loss: 0.094. pi-loss: -0.104. act3: 0.47\n",
      "batch 2: tot reward: -21 run mean: -20.510 V: -0.759. V-loss: 0.028. pi-loss: -0.129. act3: 0.45\n",
      "batch 3: tot reward: -21 run mean: -20.515 V: -0.777. V-loss: 0.019. pi-loss: -0.373. act3: 0.42\n",
      "batch 4: tot reward: -20 run mean: -20.505 V: -0.765. V-loss: 0.223. pi-loss: -0.227. act3: 0.40\n",
      "batch 5: tot reward: -21 run mean: -20.510 V: -0.659. V-loss: 0.012. pi-loss: -1.536. act3: 0.40\n",
      "batch 6: tot reward: -21 run mean: -20.515 V: -0.786. V-loss: 0.009. pi-loss: -1.067. act3: 0.31\n",
      "batch 7: tot reward: -21 run mean: -20.519 V: -0.764. V-loss: 0.007. pi-loss: -1.265. act3: 0.27\n",
      "batch 8: tot reward: -21 run mean: -20.524 V: -0.792. V-loss: 0.007. pi-loss: -0.382. act3: 0.25\n",
      "batch 9: tot reward: -21 run mean: -20.529 V: -0.795. V-loss: 0.007. pi-loss: -1.402. act3: 0.18\n",
      "batch 10: tot reward: -21 run mean: -20.534 V: -0.798. V-loss: 0.007. pi-loss: -0.389. act3: 0.19\n",
      "batch 11: tot reward: -21 run mean: -20.538 V: -0.799. V-loss: 0.006. pi-loss: -1.519. act3: 0.16\n",
      "batch 12: tot reward: -21 run mean: -20.543 V: -0.791. V-loss: 0.006. pi-loss: -1.018. act3: 0.21\n",
      "batch 13: tot reward: -21 run mean: -20.548 V: -0.795. V-loss: 0.006. pi-loss: -0.969. act3: 0.26\n",
      "batch 14: tot reward: -21 run mean: -20.552 V: -0.799. V-loss: 0.006. pi-loss: -0.703. act3: 0.27\n",
      "batch 15: tot reward: -21 run mean: -20.557 V: -0.764. V-loss: 0.006. pi-loss: -1.244. act3: 0.26\n",
      "batch 16: tot reward: -21 run mean: -20.561 V: -0.803. V-loss: 0.006. pi-loss: -0.997. act3: 0.30\n",
      "batch 17: tot reward: -21 run mean: -20.565 V: -0.801. V-loss: 0.005. pi-loss: -1.005. act3: 0.29\n",
      "batch 18: tot reward: -21 run mean: -20.570 V: -0.790. V-loss: 0.008. pi-loss: -0.607. act3: 0.41\n",
      "batch 19: tot reward: -21 run mean: -20.574 V: -0.807. V-loss: 0.006. pi-loss: -0.353. act3: 0.37\n",
      "batch 20: tot reward: -21 run mean: -20.578 V: -0.804. V-loss: 0.005. pi-loss: -0.478. act3: 0.35\n",
      "batch 21: tot reward: -21 run mean: -20.582 V: -0.786. V-loss: 0.006. pi-loss: -0.448. act3: 0.37\n",
      "batch 22: tot reward: -20 run mean: -20.582 V: -0.770. V-loss: 0.107. pi-loss: -0.710. act3: 0.51\n",
      "batch 23: tot reward: -20 run mean: -20.581 V: -0.723. V-loss: 0.122. pi-loss: -0.997. act3: 0.54\n",
      "batch 24: tot reward: -21 run mean: -20.585 V: -0.646. V-loss: 0.054. pi-loss: -1.196. act3: 0.64\n",
      "batch 25: tot reward: -21 run mean: -20.589 V: -0.669. V-loss: 0.024. pi-loss: -0.444. act3: 0.74\n",
      "batch 26: tot reward: -21 run mean: -20.593 V: -0.743. V-loss: 0.022. pi-loss: -0.938. act3: 0.80\n",
      "batch 27: tot reward: -20 run mean: -20.592 V: -0.734. V-loss: 0.111. pi-loss: -0.555. act3: 0.73\n",
      "batch 28: tot reward: -21 run mean: -20.596 V: -0.694. V-loss: 0.027. pi-loss: 0.191. act3: 0.69\n",
      "batch 29: tot reward: -21 run mean: -20.600 V: -0.771. V-loss: 0.017. pi-loss: -1.193. act3: 0.75\n",
      "batch 30: tot reward: -21 run mean: -20.604 V: -0.730. V-loss: 0.033. pi-loss: -1.175. act3: 0.81\n",
      "batch 31: tot reward: -21 run mean: -20.608 V: -0.765. V-loss: 0.019. pi-loss: -1.860. act3: 0.70\n",
      "batch 32: tot reward: -21 run mean: -20.612 V: -0.711. V-loss: 0.030. pi-loss: -1.155. act3: 0.65\n",
      "batch 33: tot reward: -21 run mean: -20.616 V: -0.732. V-loss: 0.021. pi-loss: -1.734. act3: 0.64\n",
      "batch 34: tot reward: -21 run mean: -20.620 V: -0.767. V-loss: 0.033. pi-loss: -0.054. act3: 0.59\n",
      "batch 35: tot reward: -21 run mean: -20.624 V: -0.753. V-loss: 0.024. pi-loss: -1.690. act3: 0.61\n",
      "batch 36: tot reward: -21 run mean: -20.628 V: -0.745. V-loss: 0.027. pi-loss: 0.019. act3: 0.66\n",
      "batch 37: tot reward: -21 run mean: -20.631 V: -0.726. V-loss: 0.026. pi-loss: -1.196. act3: 0.54\n",
      "batch 38: tot reward: -21 run mean: -20.635 V: -0.733. V-loss: 0.019. pi-loss: -1.473. act3: 0.66\n",
      "batch 39: tot reward: -21 run mean: -20.639 V: -0.749. V-loss: 0.018. pi-loss: -0.878. act3: 0.59\n",
      "batch 40: tot reward: -21 run mean: -20.642 V: -0.736. V-loss: 0.032. pi-loss: 0.131. act3: 0.74\n",
      "batch 41: tot reward: -21 run mean: -20.646 V: -0.738. V-loss: 0.013. pi-loss: -0.695. act3: 0.78\n",
      "batch 42: tot reward: -21 run mean: -20.649 V: -0.727. V-loss: 0.013. pi-loss: -0.589. act3: 0.78\n",
      "batch 43: tot reward: -21 run mean: -20.653 V: -0.768. V-loss: 0.020. pi-loss: -1.084. act3: 0.71\n",
      "batch 44: tot reward: -21 run mean: -20.656 V: -0.743. V-loss: 0.020. pi-loss: -0.236. act3: 0.81\n",
      "batch 45: tot reward: -21 run mean: -20.660 V: -0.711. V-loss: 0.018. pi-loss: -0.750. act3: 0.87\n",
      "batch 46: tot reward: -21 run mean: -20.663 V: -0.740. V-loss: 0.015. pi-loss: 0.063. act3: 0.79\n",
      "batch 47: tot reward: -21 run mean: -20.667 V: -0.747. V-loss: 0.029. pi-loss: -0.453. act3: 0.74\n",
      "batch 48: tot reward: -21 run mean: -20.670 V: -0.727. V-loss: 0.017. pi-loss: 0.537. act3: 0.79\n",
      "batch 49: tot reward: -21 run mean: -20.673 V: -0.738. V-loss: 0.017. pi-loss: -0.182. act3: 0.81\n",
      "batch 50: tot reward: -21 run mean: -20.676 V: -0.761. V-loss: 0.016. pi-loss: -0.761. act3: 0.79\n",
      "batch 51: tot reward: -21 run mean: -20.680 V: -0.760. V-loss: 0.024. pi-loss: -1.623. act3: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in less\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 52: tot reward: -21 run mean: -20.683 V: -0.750. V-loss: 0.020. pi-loss: -1.038. act3: 0.65\n",
      "batch 53: tot reward: -21 run mean: -20.686 V: -0.735. V-loss: 0.012. pi-loss: -0.992. act3: 0.79\n",
      "batch 54: tot reward: -21 run mean: -20.689 V: -0.720. V-loss: 0.028. pi-loss: -1.090. act3: 0.79\n",
      "batch 55: tot reward: -21 run mean: -20.692 V: -0.761. V-loss: 0.015. pi-loss: -0.118. act3: 0.74\n",
      "batch 56: tot reward: -21 run mean: -20.695 V: -0.717. V-loss: 0.020. pi-loss: -1.020. act3: 0.82\n",
      "batch 57: tot reward: -21 run mean: -20.698 V: -0.763. V-loss: 0.013. pi-loss: -0.029. act3: 0.86\n",
      "batch 58: tot reward: -21 run mean: -20.701 V: -0.761. V-loss: 0.018. pi-loss: -0.484. act3: 0.81\n",
      "batch 59: tot reward: -21 run mean: -20.704 V: -0.780. V-loss: 0.019. pi-loss: 0.194. act3: 0.79\n",
      "batch 60: tot reward: -21 run mean: -20.707 V: -0.741. V-loss: 0.012. pi-loss: -1.061. act3: 0.86\n",
      "batch 61: tot reward: -21 run mean: -20.710 V: -0.767. V-loss: 0.013. pi-loss: -0.807. act3: 0.87\n",
      "batch 62: tot reward: -21 run mean: -20.713 V: -0.736. V-loss: 0.008. pi-loss: -1.338. act3: 0.89\n",
      "batch 63: tot reward: -21 run mean: -20.716 V: -0.770. V-loss: 0.017. pi-loss: -0.484. act3: 0.89\n",
      "batch 64: tot reward: -21 run mean: -20.719 V: -0.772. V-loss: 0.012. pi-loss: -0.745. act3: 0.89\n",
      "batch 65: tot reward: -21 run mean: -20.722 V: -0.758. V-loss: 0.013. pi-loss: -0.329. act3: 0.87\n",
      "batch 66: tot reward: -21 run mean: -20.725 V: -0.792. V-loss: 0.016. pi-loss: -0.492. act3: 0.90\n",
      "batch 67: tot reward: -21 run mean: -20.727 V: -0.751. V-loss: 0.010. pi-loss: -1.055. act3: 0.89\n",
      "batch 68: tot reward: -21 run mean: -20.730 V: -0.736. V-loss: 0.017. pi-loss: -0.870. act3: 0.84\n"
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
<<<<<<< HEAD
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-0cd67b584d06>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m \u001b[1;31m# probability of random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_ep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Update model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-ee5f5ce5b3fe>\u001b[0m in \u001b[0;36mrun_episode\u001b[1;34m(env, agent, n_episodes, ep)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# Execute a step:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-60-d93831c70482>\u001b[0m in \u001b[0;36magent\u001b[1;34m(state, ep)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mep\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1571\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1572\u001b[0m         return self._predict_loop(f, ins,\n\u001b[1;32m-> 1573\u001b[1;33m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[0;32m   1574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1575\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[1;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[0;32m   1201\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1204\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2101\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2102\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m-> 2103\u001b[1;33m                               feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m   2104\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[1;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m     \u001b[0mfetch_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;31m# Run request and get response.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, graph, fetches, feeds)\u001b[0m\n\u001b[0;32m    406\u001b[0m     \"\"\"\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    409\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m       \u001b[1;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \"\"\"\n\u001b[0;32m    336\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mfor_fetch\u001b[1;34m(fetch)\u001b[0m\n\u001b[0;32m    234\u001b[0m       \u001b[1;31m# Look for a handler in the registered expansions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_REGISTERED_EXPANSIONS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
=======
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-629ba217ce0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m \u001b[0;31m# probability of random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mvalue_plus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-7361981304c7>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, agent, n_episodes, ep)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Execute a step:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Store variables from run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/wrappers/time_limit.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_episode_started_at\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/core.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minfo\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \"\"\"\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gym/envs/atari/atari_env.pyc\u001b[0m in \u001b[0;36m_step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mreward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0male\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/atari_py/ale_python_interface.pyc\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "env = gym.make(\"Pong-v0\")\n",
    "running_reward = -20.5\n",
    "batch = 0\n",
<<<<<<< HEAD
    "batch_ep = 3 #how many episodes to run per batch\n",
    "ep = 0.1 # probability of random action\n",
=======
    "batch_ep = 2#how many episodes to run per batch\n",
    "ep = 0.05 # probability of random action\n",
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
    "while True:\n",
    "    state,action,reward, value = run_episode(env, agent, n_episodes=batch_ep, ep = ep)\n",
    "\n",
    "    value_plus = np.asarray(value.tolist() + [[0.0]])\n",
    "\n",
    "    # Update model\n",
    "    drs = discount_rewards(reward)\n",
    "    R =  reward + (reward==0.0)*value_plus[1:]*0.99 # Value is reward if done (same as reward!=0), else it is predicted value else\n",
    "    weight = (R-value_plus[:-1])\n",
    "    # Normalize the weights\n",
    "    weight -= np.mean(weight)\n",
    "    weight /= np.std(weight)\n",
    "    pi_true_weights = np.hstack([weight, to_categorical(action,num_classes=6)]) # Format truth value to be in accordance to weighted_crossentropy\n",
    "    h = model.fit(state, [drs, pi_true_weights], verbose=0, shuffle=True)\n",
    "    vloss = h.history.items()[2][1][0]\n",
    "    piloss = h.history.items()[1][1][0]\n",
    "    \n",
    "    # Book-keeping\n",
    "    batch +=1\n",
    "    reward_sum = np.sum(reward)/batch_ep\n",
    "    running_reward = running_reward * 0.99 + reward_sum * 0.01\n",
    "    print 'batch %d: tot reward: %.0f run mean: %.3f V: %.3f. V-loss: %.3f. pi-loss: %.3f. act3: %.2f' %\\\n",
    "    (batch, reward_sum, running_reward, np.mean(value), vloss, piloss, np.mean(pi_true_weights[:,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.tolist()[:5] + [[0.0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(reward==0.0)*Vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.roll(value,-1)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3697, 7)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_true_weights.shape"
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state.shape"
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13845624"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aprob[0][0]"
   ]
=======
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "V, aprob = model.predict(state[1,].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aprob.ravel()[[1,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = np.argmin(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for t in range(10):\n",
    "    print(value[i-t,])\n",
    "    plot_state(state[i-t,])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
>>>>>>> cda6849b3dfc122f7531c522d6a873c8a17e9149
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
