{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'right', 1: 'left', 2: 'up', 3: 'down'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snakai\n",
    "import agents\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "action2ind = {'right' : 0,\n",
    "             'left' : 1,\n",
    "             'up' : 2,\n",
    "             'down' : 3}\n",
    "ind2action = {val: key for key, val in action2ind.items()}\n",
    "ind2action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        #self.dense1 = nn.Linear(1600, 2048)\n",
    "        self.dense2 = nn.Linear(1600, 512)\n",
    "        self.head = nn.Linear(512, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #x = F.relu(self.bn2(self.conv2(x)))\n",
    "        #x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = F.relu(self.dense1(x))\n",
    "        x = F.relu(self.dense2(x))\n",
    "        return (self.head(x))\n",
    "    \n",
    "model = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_step(action):\n",
    "    model.train(mode=False)\n",
    "    if 'torch' in action.type():\n",
    "        action_pure = ind2action[action.numpy()[0][0]]\n",
    "        next_state, reward, ended = snake.step(action_pure)\n",
    "        next_state, reward, ended = torch.unsqueeze(torch.from_numpy(next_state),0).float(), FloatTensor([[reward]]), LongTensor([[ended]])\n",
    "        return next_state, reward, ended\n",
    "    else:\n",
    "        return snake.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_idx(a):\n",
    "    one = a == np.max(a)\n",
    "    col = np.argmax(one.max(axis=0))\n",
    "    row = np.argmax(one[:,col])\n",
    "    return row, col\n",
    "\n",
    "def simple_agent(state):\n",
    "    use_torch = 'torch' in state.type()\n",
    "    if use_torch:\n",
    "        state = state.numpy()\n",
    "        \n",
    "    y_player, x_player = max_idx(state[0,0,:,:])\n",
    "    y_apple, x_apple = max_idx(state[0,1,:,:])\n",
    "    action = \"down\"\n",
    "    if y_player < y_apple:\n",
    "        action = \"down\"\n",
    "    elif y_player > y_apple:\n",
    "        action = \"up\"\n",
    "    elif x_player < x_apple:\n",
    "        action = \"right\"\n",
    "    elif x_player > x_apple:\n",
    "        action = \"left\"\n",
    "    \n",
    "    action = action2ind[action]\n",
    "    if use_torch:\n",
    "        return LongTensor([[action]])\n",
    "    else: \n",
    "        return action\n",
    "    \n",
    "def random_agent(state):\n",
    "    return LongTensor([[random.randrange(4)]])\n",
    "\n",
    "def model_agent(state):\n",
    "    return model(Variable(state)).data.max(1)[1].view(1, 1)\n",
    "\n",
    "def epsilon_agent(state, th = 0.05):\n",
    "    if random.random() > th:\n",
    "        return model_agent(state)\n",
    "    else:\n",
    "        return random_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return None, 0\n",
    "\n",
    "    # fetch and concat batch:\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    model.train(mode=True)\n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "\n",
    "    ended_batch = torch.cat(batch.ended)\n",
    "    non_final_mask = ByteTensor(1 - ended_batch.numpy())\n",
    "    \n",
    "    non_final_next_states = Variable(torch.cat(\n",
    "    [state for end, state in zip(ended_batch.numpy().flatten(), batch.next_state) if end !=1])\n",
    "                                     ,volatile=True)\n",
    "    \n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    model.train(mode=False)\n",
    "    next_state_values = Variable(torch.zeros(batch_size).type(Tensor))\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Now, we don't want to mess up the loss with a volatile flag, so let's\n",
    "    # clear it. After this, we'll just end up with a Variable that has\n",
    "    # requires_grad=False\n",
    "    next_state_values.volatile = False\n",
    "    \n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch.view(batch_size).float()\n",
    "    #print(reward_batch)\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #for param in model.parameters():\n",
    "    #    param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss, expected_state_action_values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(snake, epsilon = 0.05):\n",
    "    cum_reward = 0.0\n",
    "    snake.on_init()\n",
    "    state, reward, ended = snake.on_feedback()\n",
    "    state = torch.unsqueeze(torch.from_numpy(state),0).float()\n",
    "    for i in range(ep_length):\n",
    "        action = epsilon_agent(state, th = epsilon)\n",
    "        next_state, reward, ended = torch_step(action)\n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        if ended.numpy()[0][0] == 1:\n",
    "            return cum_reward, i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "game_size = (10, 10)\n",
    "snake = snakai.Snake(render=False, game_size = game_size, time_reward = 0.01)\n",
    "\n",
    "ep_length = 10000\n",
    "num_episode = 100000\n",
    "avg_reward = -1.0\n",
    "avg_steps = 1.0\n",
    "best_reward = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 32\n",
    "ksize = 4\n",
    "batch_size = 64\n",
    "GAMMA = 0.99\n",
    "optimizer = optim.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=ksize, stride=1, padding = 0)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 2)\n",
    "        self.bn2 = nn.BatchNorm2d(ch)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 2)\n",
    "        self.bn3 = nn.BatchNorm2d(ch)\n",
    "        self.dense1 = nn.Linear(2592, 512)\n",
    "        self.head = nn.Linear(512, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        return (self.head(x))\n",
    "    \n",
    "model = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 32\n",
    "ksize = 4\n",
    "batch_size = 64\n",
    "GAMMA = 0.99\n",
    "optimizer = optim.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=ksize, stride=1, padding = 0)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 2)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 2)\n",
    "        self.dense1 = nn.Linear(2592, 512)\n",
    "        self.head = nn.Linear(512, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.dense1(x))\n",
    "        return (self.head(x))\n",
    "    \n",
    "model = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.07999999821186066, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_game(snake, epsilon = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:33.898656 episode 100: avg_reward: -0.342, steps: 3 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.476840 episode 200: avg_reward: -0.142, steps: 4 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.655435 episode 300: avg_reward: 0.001, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.435553 episode 400: avg_reward: 0.077, steps: 5 loss: 0.19, exp_val: -0.06\n",
      "0:00:35.403011 episode 500: avg_reward: 0.111, steps: 5 loss: 0.10, exp_val: 0.11\n",
      "saving model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type DQN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:35.405853 episode 600: avg_reward: 0.075, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.391613 episode 700: avg_reward: 0.053, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.551344 episode 800: avg_reward: 0.085, steps: 5 loss: 0.11, exp_val: 0.11\n",
      "0:00:35.517134 episode 900: avg_reward: 0.060, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.591884 episode 1000: avg_reward: 0.007, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.499971 episode 1100: avg_reward: 0.040, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.630427 episode 1200: avg_reward: 0.038, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.523032 episode 1300: avg_reward: 0.045, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.430479 episode 1400: avg_reward: 0.016, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.427241 episode 1500: avg_reward: 0.052, steps: 5 loss: 0.14, exp_val: 0.17\n",
      "0:00:35.467064 episode 1600: avg_reward: 0.075, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.468855 episode 1700: avg_reward: 0.031, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:34.456516 episode 1800: avg_reward: 0.034, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.436975 episode 1900: avg_reward: 0.048, steps: 5 loss: 0.17, exp_val: 0.17\n",
      "0:00:35.736747 episode 2000: avg_reward: 0.084, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.574891 episode 2100: avg_reward: 0.126, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.473682 episode 2200: avg_reward: 0.075, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.522139 episode 2300: avg_reward: 0.062, steps: 5 loss: 0.18, exp_val: -0.09\n",
      "0:00:35.543837 episode 2400: avg_reward: 0.080, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.583740 episode 2500: avg_reward: 0.086, steps: 5 loss: 0.20, exp_val: 0.12\n",
      "0:00:35.634348 episode 2600: avg_reward: 0.054, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.568567 episode 2700: avg_reward: 0.038, steps: 5 loss: 0.14, exp_val: -0.05\n",
      "0:00:35.646147 episode 2800: avg_reward: 0.122, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.390974 episode 2900: avg_reward: 0.100, steps: 5 loss: 0.10, exp_val: 0.09\n",
      "0:00:35.545687 episode 3000: avg_reward: 0.139, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "saving model..\n",
      "0:00:35.566985 episode 3100: avg_reward: 0.099, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.525026 episode 3200: avg_reward: 0.092, steps: 5 loss: 0.10, exp_val: -0.08\n",
      "0:00:35.480592 episode 3300: avg_reward: 0.118, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.439480 episode 3400: avg_reward: 0.122, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.488661 episode 3500: avg_reward: 0.053, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.691810 episode 3600: avg_reward: 0.032, steps: 5 loss: 0.16, exp_val: 0.26\n",
      "0:00:35.467684 episode 3700: avg_reward: 0.053, steps: 5 loss: 0.14, exp_val: 0.17\n",
      "0:00:35.479449 episode 3800: avg_reward: 0.058, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.458256 episode 3900: avg_reward: 0.065, steps: 5 loss: 0.17, exp_val: 0.23\n",
      "0:00:35.388126 episode 4000: avg_reward: 0.093, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.535393 episode 4100: avg_reward: 0.082, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.438107 episode 4200: avg_reward: 0.064, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.538047 episode 4300: avg_reward: 0.090, steps: 5 loss: 0.12, exp_val: 0.09\n",
      "0:00:35.521820 episode 4400: avg_reward: 0.020, steps: 5 loss: 0.23, exp_val: -0.01\n",
      "0:00:35.443501 episode 4500: avg_reward: 0.064, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.402820 episode 4600: avg_reward: 0.117, steps: 5 loss: 0.18, exp_val: 0.10\n",
      "0:00:35.563555 episode 4700: avg_reward: 0.057, steps: 5 loss: 0.21, exp_val: -0.09\n",
      "0:00:35.468345 episode 4800: avg_reward: 0.015, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.466358 episode 4900: avg_reward: 0.051, steps: 5 loss: 0.12, exp_val: 0.06\n",
      "0:00:36.056867 episode 5000: avg_reward: 0.033, steps: 5 loss: 0.11, exp_val: -0.03\n",
      "0:00:35.499226 episode 5100: avg_reward: 0.054, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.563432 episode 5200: avg_reward: 0.028, steps: 5 loss: 0.12, exp_val: 0.04\n",
      "0:00:35.410687 episode 5300: avg_reward: 0.076, steps: 5 loss: 0.14, exp_val: -0.04\n",
      "0:00:35.483685 episode 5400: avg_reward: 0.071, steps: 5 loss: 0.19, exp_val: 0.04\n",
      "0:00:35.402414 episode 5500: avg_reward: 0.061, steps: 5 loss: 0.12, exp_val: -0.10\n",
      "0:00:35.459509 episode 5600: avg_reward: 0.033, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.406457 episode 5700: avg_reward: 0.083, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.428277 episode 5800: avg_reward: 0.068, steps: 5 loss: 0.09, exp_val: 0.06\n",
      "0:00:35.486224 episode 5900: avg_reward: 0.099, steps: 5 loss: 0.16, exp_val: 0.20\n",
      "0:00:35.491065 episode 6000: avg_reward: 0.141, steps: 5 loss: 0.12, exp_val: 0.09\n",
      "saving model..\n",
      "0:00:35.557780 episode 6100: avg_reward: 0.071, steps: 5 loss: 0.15, exp_val: 0.22\n",
      "0:00:35.528162 episode 6200: avg_reward: 0.050, steps: 5 loss: 0.18, exp_val: 0.07\n",
      "0:00:35.393835 episode 6300: avg_reward: 0.038, steps: 5 loss: 0.14, exp_val: -0.10\n",
      "0:00:35.492997 episode 6400: avg_reward: 0.098, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.494281 episode 6500: avg_reward: 0.113, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.415567 episode 6600: avg_reward: 0.059, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.509291 episode 6700: avg_reward: 0.116, steps: 5 loss: 0.19, exp_val: 0.01\n",
      "0:00:35.707473 episode 6800: avg_reward: 0.084, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.626927 episode 6900: avg_reward: 0.128, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.432935 episode 7000: avg_reward: 0.068, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.482598 episode 7100: avg_reward: 0.116, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.482283 episode 7200: avg_reward: 0.097, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.386295 episode 7300: avg_reward: 0.060, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.449690 episode 7400: avg_reward: 0.066, steps: 5 loss: 0.14, exp_val: 0.09\n",
      "0:00:35.429568 episode 7500: avg_reward: 0.062, steps: 5 loss: 0.17, exp_val: -0.02\n",
      "0:00:35.418147 episode 7600: avg_reward: 0.061, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.518929 episode 7700: avg_reward: 0.123, steps: 5 loss: 0.15, exp_val: -0.02\n",
      "0:00:35.567530 episode 7800: avg_reward: 0.187, steps: 5 loss: 0.17, exp_val: -0.10\n",
      "0:00:35.503196 episode 7900: avg_reward: 0.118, steps: 5 loss: 0.18, exp_val: -0.01\n",
      "0:00:35.467508 episode 8000: avg_reward: 0.045, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.464874 episode 8100: avg_reward: 0.057, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.529870 episode 8200: avg_reward: 0.057, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.460146 episode 8300: avg_reward: 0.058, steps: 5 loss: 0.20, exp_val: -0.01\n",
      "0:00:35.492150 episode 8400: avg_reward: 0.056, steps: 5 loss: 0.14, exp_val: 0.07\n",
      "0:00:35.550090 episode 8500: avg_reward: 0.061, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.461788 episode 8600: avg_reward: 0.043, steps: 5 loss: 0.19, exp_val: 0.01\n",
      "0:00:35.567410 episode 8700: avg_reward: 0.069, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.443710 episode 8800: avg_reward: 0.066, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.482069 episode 8900: avg_reward: -0.003, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.870730 episode 9000: avg_reward: 0.046, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.410531 episode 9100: avg_reward: 0.036, steps: 5 loss: 0.18, exp_val: 0.22\n",
      "0:00:35.534431 episode 9200: avg_reward: 0.086, steps: 5 loss: 0.18, exp_val: 0.02\n",
      "0:00:35.479115 episode 9300: avg_reward: 0.095, steps: 5 loss: 0.18, exp_val: -0.12\n",
      "0:00:35.541273 episode 9400: avg_reward: 0.116, steps: 5 loss: 0.10, exp_val: 0.05\n",
      "0:00:35.521748 episode 9500: avg_reward: 0.145, steps: 5 loss: 0.16, exp_val: 0.14\n",
      "saving model..\n",
      "0:00:35.577636 episode 9600: avg_reward: 0.062, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.515377 episode 9700: avg_reward: 0.092, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.299278 episode 9800: avg_reward: 0.060, steps: 5 loss: 0.18, exp_val: 0.07\n",
      "0:00:35.372137 episode 9900: avg_reward: 0.084, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.458430 episode 10000: avg_reward: 0.045, steps: 5 loss: 0.18, exp_val: 0.07\n",
      "0:00:35.469556 episode 10100: avg_reward: 0.072, steps: 5 loss: 0.18, exp_val: -0.01\n",
      "0:00:35.594406 episode 10200: avg_reward: 0.066, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.576919 episode 10300: avg_reward: 0.065, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.432164 episode 10400: avg_reward: 0.044, steps: 5 loss: 0.20, exp_val: 0.15\n",
      "0:00:35.549774 episode 10500: avg_reward: 0.075, steps: 5 loss: 0.17, exp_val: 0.04\n",
      "0:00:35.450554 episode 10600: avg_reward: 0.058, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.554968 episode 10700: avg_reward: 0.097, steps: 5 loss: 0.23, exp_val: 0.18\n",
      "0:00:35.337457 episode 10800: avg_reward: 0.088, steps: 5 loss: 0.18, exp_val: 0.10\n",
      "0:00:35.419492 episode 10900: avg_reward: 0.123, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.450887 episode 11000: avg_reward: 0.021, steps: 5 loss: 0.11, exp_val: -0.03\n",
      "0:00:35.346981 episode 11100: avg_reward: 0.031, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.527399 episode 11200: avg_reward: 0.086, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.423777 episode 11300: avg_reward: 0.077, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.398444 episode 11400: avg_reward: 0.089, steps: 5 loss: 0.17, exp_val: -0.04\n",
      "0:00:35.347159 episode 11500: avg_reward: 0.061, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.370785 episode 11600: avg_reward: 0.068, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.402871 episode 11700: avg_reward: 0.060, steps: 5 loss: 0.13, exp_val: 0.11\n",
      "0:00:35.295855 episode 11800: avg_reward: 0.041, steps: 5 loss: 0.15, exp_val: -0.02\n",
      "0:00:35.313988 episode 11900: avg_reward: 0.078, steps: 5 loss: 0.22, exp_val: 0.13\n",
      "0:00:35.505495 episode 12000: avg_reward: 0.056, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.377041 episode 12100: avg_reward: 0.079, steps: 5 loss: 0.12, exp_val: -0.04\n",
      "0:00:35.421304 episode 12200: avg_reward: 0.057, steps: 5 loss: 0.19, exp_val: 0.02\n",
      "0:00:35.434864 episode 12300: avg_reward: 0.006, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.405279 episode 12400: avg_reward: 0.039, steps: 5 loss: 0.14, exp_val: -0.02\n",
      "0:00:35.392782 episode 12500: avg_reward: 0.070, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.423139 episode 12600: avg_reward: 0.088, steps: 5 loss: 0.19, exp_val: -0.10\n",
      "0:00:35.412613 episode 12700: avg_reward: 0.142, steps: 5 loss: 0.12, exp_val: 0.09\n",
      "0:00:35.485994 episode 12800: avg_reward: 0.135, steps: 5 loss: 0.22, exp_val: 0.02\n",
      "0:00:35.633586 episode 12900: avg_reward: 0.096, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.429736 episode 13000: avg_reward: 0.074, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.590926 episode 13100: avg_reward: 0.016, steps: 5 loss: 0.12, exp_val: 0.16\n",
      "0:00:35.620941 episode 13200: avg_reward: 0.058, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.560925 episode 13300: avg_reward: 0.117, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.528124 episode 13400: avg_reward: 0.047, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.542991 episode 13500: avg_reward: 0.113, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.569070 episode 13600: avg_reward: 0.055, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.639274 episode 13700: avg_reward: 0.082, steps: 5 loss: 0.18, exp_val: 0.07\n",
      "0:00:35.592221 episode 13800: avg_reward: 0.100, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.498237 episode 13900: avg_reward: 0.074, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.481131 episode 14000: avg_reward: 0.087, steps: 5 loss: 0.08, exp_val: 0.01\n",
      "0:00:35.507647 episode 14100: avg_reward: 0.127, steps: 5 loss: 0.19, exp_val: 0.14\n",
      "0:00:35.505283 episode 14200: avg_reward: 0.104, steps: 5 loss: 0.12, exp_val: 0.06\n",
      "0:00:35.494387 episode 14300: avg_reward: 0.144, steps: 5 loss: 0.14, exp_val: 0.19\n",
      "0:00:35.409075 episode 14400: avg_reward: 0.099, steps: 5 loss: 0.14, exp_val: -0.10\n",
      "0:00:35.491205 episode 14500: avg_reward: 0.085, steps: 5 loss: 0.17, exp_val: 0.18\n",
      "0:00:35.411772 episode 14600: avg_reward: 0.080, steps: 5 loss: 0.18, exp_val: 0.15\n",
      "0:00:35.388291 episode 14700: avg_reward: 0.040, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.438641 episode 14800: avg_reward: 0.053, steps: 5 loss: 0.22, exp_val: 0.02\n",
      "0:00:35.501368 episode 14900: avg_reward: 0.075, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.502671 episode 15000: avg_reward: 0.110, steps: 5 loss: 0.21, exp_val: 0.07\n",
      "0:00:35.396078 episode 15100: avg_reward: 0.080, steps: 5 loss: 0.18, exp_val: 0.02\n",
      "0:00:35.401188 episode 15200: avg_reward: 0.061, steps: 5 loss: 0.14, exp_val: -0.05\n",
      "0:00:35.471762 episode 15300: avg_reward: 0.057, steps: 5 loss: 0.14, exp_val: -0.04\n",
      "0:00:35.490843 episode 15400: avg_reward: 0.028, steps: 5 loss: 0.19, exp_val: 0.01\n",
      "0:00:35.402314 episode 15500: avg_reward: 0.085, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.494038 episode 15600: avg_reward: 0.050, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.387220 episode 15700: avg_reward: 0.048, steps: 5 loss: 0.20, exp_val: 0.10\n",
      "0:00:35.404283 episode 15800: avg_reward: 0.125, steps: 5 loss: 0.21, exp_val: 0.10\n",
      "0:00:35.429383 episode 15900: avg_reward: 0.096, steps: 5 loss: 0.13, exp_val: 0.11\n",
      "0:00:35.306458 episode 16000: avg_reward: 0.120, steps: 5 loss: 0.13, exp_val: 0.11\n",
      "0:00:35.391517 episode 16100: avg_reward: 0.115, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.410534 episode 16200: avg_reward: 0.067, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.461873 episode 16300: avg_reward: 0.053, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.441710 episode 16400: avg_reward: 0.052, steps: 5 loss: 0.21, exp_val: 0.13\n",
      "0:00:35.355393 episode 16500: avg_reward: 0.049, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.484525 episode 16600: avg_reward: 0.068, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.477986 episode 16700: avg_reward: 0.082, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.471325 episode 16800: avg_reward: 0.076, steps: 5 loss: 0.15, exp_val: -0.04\n",
      "0:00:35.460190 episode 16900: avg_reward: 0.021, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.404195 episode 17000: avg_reward: 0.032, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.528903 episode 17100: avg_reward: 0.073, steps: 5 loss: 0.15, exp_val: -0.13\n",
      "0:00:35.463996 episode 17200: avg_reward: 0.040, steps: 5 loss: 0.16, exp_val: 0.17\n",
      "0:00:35.476639 episode 17300: avg_reward: 0.003, steps: 5 loss: 0.15, exp_val: 0.11\n",
      "0:00:35.447557 episode 17400: avg_reward: 0.033, steps: 5 loss: 0.21, exp_val: 0.17\n",
      "0:00:35.487247 episode 17500: avg_reward: 0.104, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.444787 episode 17600: avg_reward: 0.035, steps: 5 loss: 0.10, exp_val: 0.09\n",
      "0:00:35.476946 episode 17700: avg_reward: 0.063, steps: 5 loss: 0.17, exp_val: 0.14\n",
      "0:00:35.437088 episode 17800: avg_reward: 0.113, steps: 5 loss: 0.16, exp_val: -0.05\n",
      "0:00:35.535895 episode 17900: avg_reward: 0.096, steps: 5 loss: 0.15, exp_val: -0.12\n",
      "0:00:35.471426 episode 18000: avg_reward: 0.066, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.431754 episode 18100: avg_reward: 0.062, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.454169 episode 18200: avg_reward: 0.076, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.353095 episode 18300: avg_reward: 0.043, steps: 5 loss: 0.20, exp_val: 0.06\n",
      "0:00:35.495578 episode 18400: avg_reward: 0.077, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.395914 episode 18500: avg_reward: 0.065, steps: 5 loss: 0.19, exp_val: 0.06\n",
      "0:00:35.465314 episode 18600: avg_reward: 0.062, steps: 5 loss: 0.15, exp_val: 0.15\n",
      "0:00:35.495499 episode 18700: avg_reward: 0.040, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.574981 episode 18800: avg_reward: 0.083, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.445172 episode 18900: avg_reward: 0.082, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.403277 episode 19000: avg_reward: 0.083, steps: 5 loss: 0.13, exp_val: -0.08\n",
      "0:00:35.426351 episode 19100: avg_reward: 0.090, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.472349 episode 19200: avg_reward: 0.083, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.399403 episode 19300: avg_reward: 0.065, steps: 5 loss: 0.20, exp_val: -0.10\n",
      "0:00:35.471963 episode 19400: avg_reward: 0.117, steps: 5 loss: 0.21, exp_val: -0.09\n",
      "0:00:35.399769 episode 19500: avg_reward: 0.067, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.464381 episode 19600: avg_reward: 0.029, steps: 5 loss: 0.19, exp_val: 0.04\n",
      "0:00:35.767768 episode 19700: avg_reward: 0.078, steps: 5 loss: 0.11, exp_val: 0.03\n",
      "0:00:35.486347 episode 19800: avg_reward: 0.069, steps: 5 loss: 0.18, exp_val: 0.15\n",
      "0:00:35.549214 episode 19900: avg_reward: 0.079, steps: 5 loss: 0.20, exp_val: 0.02\n",
      "0:00:35.596623 episode 20000: avg_reward: 0.050, steps: 5 loss: 0.19, exp_val: 0.04\n",
      "0:00:35.476856 episode 20100: avg_reward: 0.025, steps: 5 loss: 0.12, exp_val: 0.09\n",
      "0:00:35.550510 episode 20200: avg_reward: 0.104, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.590042 episode 20300: avg_reward: 0.077, steps: 5 loss: 0.11, exp_val: 0.11\n",
      "0:00:35.497147 episode 20400: avg_reward: 0.087, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.648821 episode 20500: avg_reward: 0.098, steps: 5 loss: 0.17, exp_val: -0.04\n",
      "0:00:35.499488 episode 20600: avg_reward: 0.091, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.542871 episode 20700: avg_reward: 0.126, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.568320 episode 20800: avg_reward: 0.077, steps: 5 loss: 0.21, exp_val: -0.03\n",
      "0:00:35.668453 episode 20900: avg_reward: 0.090, steps: 5 loss: 0.21, exp_val: 0.07\n",
      "0:00:35.513699 episode 21000: avg_reward: 0.053, steps: 5 loss: 0.16, exp_val: 0.23\n",
      "0:00:35.605373 episode 21100: avg_reward: 0.053, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.520310 episode 21200: avg_reward: 0.085, steps: 5 loss: 0.15, exp_val: -0.07\n",
      "0:00:35.657734 episode 21300: avg_reward: 0.044, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.608676 episode 21400: avg_reward: 0.126, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.501934 episode 21500: avg_reward: 0.100, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.507765 episode 21600: avg_reward: 0.061, steps: 5 loss: 0.16, exp_val: 0.17\n",
      "0:00:35.522066 episode 21700: avg_reward: 0.012, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.528527 episode 21800: avg_reward: -0.001, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.610476 episode 21900: avg_reward: 0.020, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.426810 episode 22000: avg_reward: 0.052, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.617734 episode 22100: avg_reward: 0.039, steps: 5 loss: 0.19, exp_val: -0.01\n",
      "0:00:35.598780 episode 22200: avg_reward: 0.070, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.498823 episode 22300: avg_reward: 0.118, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.609665 episode 22400: avg_reward: 0.065, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.583346 episode 22500: avg_reward: 0.028, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.584894 episode 22600: avg_reward: 0.066, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.504791 episode 22700: avg_reward: 0.055, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.590992 episode 22800: avg_reward: 0.035, steps: 5 loss: 0.11, exp_val: 0.14\n",
      "0:00:35.624657 episode 22900: avg_reward: 0.002, steps: 5 loss: 0.21, exp_val: 0.10\n",
      "0:00:35.668784 episode 23000: avg_reward: 0.029, steps: 5 loss: 0.09, exp_val: 0.17\n",
      "0:00:35.473988 episode 23100: avg_reward: 0.076, steps: 5 loss: 0.22, exp_val: 0.09\n",
      "0:00:35.511966 episode 23200: avg_reward: 0.068, steps: 5 loss: 0.17, exp_val: 0.07\n",
      "0:00:35.582317 episode 23300: avg_reward: 0.052, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.633743 episode 23400: avg_reward: 0.037, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.685384 episode 23500: avg_reward: 0.037, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.660862 episode 23600: avg_reward: 0.051, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.691641 episode 23700: avg_reward: 0.031, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.797770 episode 23800: avg_reward: 0.079, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.664719 episode 23900: avg_reward: 0.072, steps: 5 loss: 0.15, exp_val: -0.05\n",
      "0:00:35.578041 episode 24000: avg_reward: 0.091, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.686465 episode 24100: avg_reward: 0.058, steps: 5 loss: 0.16, exp_val: -0.09\n",
      "0:00:35.596313 episode 24200: avg_reward: 0.096, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.606067 episode 24300: avg_reward: 0.081, steps: 5 loss: 0.14, exp_val: -0.02\n",
      "0:00:35.620403 episode 24400: avg_reward: 0.066, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.645290 episode 24500: avg_reward: 0.053, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.589017 episode 24600: avg_reward: 0.059, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.577692 episode 24700: avg_reward: 0.023, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.539679 episode 24800: avg_reward: 0.072, steps: 5 loss: 0.13, exp_val: 0.11\n",
      "0:00:35.502030 episode 24900: avg_reward: 0.100, steps: 5 loss: 0.21, exp_val: -0.06\n",
      "0:00:35.554887 episode 25000: avg_reward: 0.059, steps: 5 loss: 0.17, exp_val: 0.15\n",
      "0:00:35.512806 episode 25100: avg_reward: 0.054, steps: 5 loss: 0.21, exp_val: 0.20\n",
      "0:00:35.551770 episode 25200: avg_reward: 0.065, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.599015 episode 25300: avg_reward: 0.012, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.567258 episode 25400: avg_reward: 0.059, steps: 5 loss: 0.22, exp_val: 0.05\n",
      "0:00:35.552651 episode 25500: avg_reward: 0.048, steps: 5 loss: 0.14, exp_val: -0.04\n",
      "0:00:35.652883 episode 25600: avg_reward: 0.087, steps: 5 loss: 0.21, exp_val: -0.03\n",
      "0:00:35.489345 episode 25700: avg_reward: 0.121, steps: 5 loss: 0.12, exp_val: -0.04\n",
      "0:00:35.616132 episode 25800: avg_reward: 0.122, steps: 5 loss: 0.19, exp_val: -0.02\n",
      "0:00:35.648954 episode 25900: avg_reward: 0.108, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.589233 episode 26000: avg_reward: 0.037, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.665980 episode 26100: avg_reward: 0.094, steps: 5 loss: 0.12, exp_val: -0.04\n",
      "0:00:35.642990 episode 26200: avg_reward: 0.109, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.722909 episode 26300: avg_reward: 0.094, steps: 5 loss: 0.17, exp_val: 0.17\n",
      "0:00:35.693884 episode 26400: avg_reward: 0.099, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.647506 episode 26500: avg_reward: 0.098, steps: 5 loss: 0.24, exp_val: -0.01\n",
      "0:00:35.652108 episode 26600: avg_reward: 0.034, steps: 5 loss: 0.20, exp_val: -0.07\n",
      "0:00:35.531098 episode 26700: avg_reward: 0.075, steps: 5 loss: 0.11, exp_val: -0.03\n",
      "0:00:35.589642 episode 26800: avg_reward: 0.061, steps: 5 loss: 0.26, exp_val: -0.11\n",
      "0:00:35.636544 episode 26900: avg_reward: 0.086, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.646609 episode 27000: avg_reward: -0.018, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.637186 episode 27100: avg_reward: 0.035, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.673110 episode 27200: avg_reward: 0.109, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.516487 episode 27300: avg_reward: 0.075, steps: 5 loss: 0.19, exp_val: 0.14\n",
      "0:00:35.502220 episode 27400: avg_reward: 0.072, steps: 5 loss: 0.23, exp_val: -0.04\n",
      "0:00:35.528867 episode 27500: avg_reward: 0.058, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.714073 episode 27600: avg_reward: 0.036, steps: 5 loss: 0.11, exp_val: -0.00\n",
      "0:00:35.584681 episode 27700: avg_reward: 0.086, steps: 5 loss: 0.14, exp_val: 0.09\n",
      "0:00:35.630129 episode 27800: avg_reward: 0.029, steps: 5 loss: 0.09, exp_val: 0.05\n",
      "0:00:35.518472 episode 27900: avg_reward: 0.031, steps: 5 loss: 0.20, exp_val: 0.05\n",
      "0:00:35.528735 episode 28000: avg_reward: 0.061, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.630730 episode 28100: avg_reward: 0.061, steps: 5 loss: 0.18, exp_val: 0.15\n",
      "0:00:35.586803 episode 28200: avg_reward: 0.080, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.522811 episode 28300: avg_reward: 0.058, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.623102 episode 28400: avg_reward: 0.060, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.562314 episode 28500: avg_reward: 0.072, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.531506 episode 28600: avg_reward: 0.028, steps: 5 loss: 0.15, exp_val: -0.04\n",
      "0:00:35.551087 episode 28700: avg_reward: 0.040, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.543040 episode 28800: avg_reward: 0.082, steps: 5 loss: 0.18, exp_val: -0.12\n",
      "0:00:35.663151 episode 28900: avg_reward: 0.114, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.569917 episode 29000: avg_reward: 0.088, steps: 5 loss: 0.09, exp_val: 0.01\n",
      "0:00:35.508340 episode 29100: avg_reward: 0.049, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.620396 episode 29200: avg_reward: 0.072, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.457716 episode 29300: avg_reward: 0.061, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.487537 episode 29400: avg_reward: 0.039, steps: 5 loss: 0.13, exp_val: 0.15\n",
      "0:00:35.489651 episode 29500: avg_reward: 0.121, steps: 5 loss: 0.25, exp_val: 0.15\n",
      "0:00:35.550866 episode 29600: avg_reward: 0.094, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.603012 episode 29700: avg_reward: 0.089, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.517055 episode 29800: avg_reward: 0.084, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.658429 episode 29900: avg_reward: 0.013, steps: 5 loss: 0.15, exp_val: -0.04\n",
      "0:00:35.610136 episode 30000: avg_reward: 0.030, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.706407 episode 30100: avg_reward: 0.050, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.594814 episode 30200: avg_reward: 0.088, steps: 5 loss: 0.14, exp_val: 0.07\n",
      "0:00:35.600828 episode 30300: avg_reward: 0.082, steps: 5 loss: 0.07, exp_val: -0.10\n",
      "0:00:35.679170 episode 30400: avg_reward: 0.135, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.565613 episode 30500: avg_reward: 0.054, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.757573 episode 30600: avg_reward: 0.110, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.671352 episode 30700: avg_reward: 0.072, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.586416 episode 30800: avg_reward: 0.084, steps: 5 loss: 0.11, exp_val: 0.14\n",
      "0:00:35.569004 episode 30900: avg_reward: 0.071, steps: 5 loss: 0.09, exp_val: 0.11\n",
      "0:00:35.674201 episode 31000: avg_reward: 0.061, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.507428 episode 31100: avg_reward: 0.052, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.620619 episode 31200: avg_reward: 0.093, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.622876 episode 31300: avg_reward: 0.131, steps: 5 loss: 0.10, exp_val: 0.08\n",
      "0:00:35.600796 episode 31400: avg_reward: 0.055, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.548820 episode 31500: avg_reward: 0.076, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.527626 episode 31600: avg_reward: 0.081, steps: 5 loss: 0.11, exp_val: -0.05\n",
      "0:00:35.631325 episode 31700: avg_reward: 0.063, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.622094 episode 31800: avg_reward: 0.043, steps: 5 loss: 0.21, exp_val: 0.04\n",
      "0:00:35.677206 episode 31900: avg_reward: 0.079, steps: 5 loss: 0.13, exp_val: 0.03\n",
      "0:00:35.619035 episode 32000: avg_reward: 0.053, steps: 5 loss: 0.21, exp_val: 0.10\n",
      "0:00:35.548567 episode 32100: avg_reward: 0.043, steps: 5 loss: 0.08, exp_val: -0.08\n",
      "0:00:35.596631 episode 32200: avg_reward: 0.045, steps: 5 loss: 0.22, exp_val: 0.12\n",
      "0:00:35.652622 episode 32300: avg_reward: 0.049, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.718953 episode 32400: avg_reward: 0.107, steps: 5 loss: 0.15, exp_val: -0.05\n",
      "0:00:35.515632 episode 32500: avg_reward: 0.067, steps: 5 loss: 0.17, exp_val: -0.06\n",
      "0:00:35.571321 episode 32600: avg_reward: 0.063, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.615191 episode 32700: avg_reward: 0.102, steps: 5 loss: 0.21, exp_val: 0.07\n",
      "0:00:35.605023 episode 32800: avg_reward: 0.027, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.511498 episode 32900: avg_reward: 0.039, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.604249 episode 33000: avg_reward: 0.101, steps: 5 loss: 0.19, exp_val: 0.04\n",
      "0:00:35.625878 episode 33100: avg_reward: 0.060, steps: 5 loss: 0.13, exp_val: 0.14\n",
      "0:00:35.676003 episode 33200: avg_reward: 0.082, steps: 5 loss: 0.17, exp_val: 0.18\n",
      "0:00:35.594809 episode 33300: avg_reward: 0.107, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.641093 episode 33400: avg_reward: 0.109, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.541329 episode 33500: avg_reward: 0.091, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.627077 episode 33600: avg_reward: 0.065, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.755694 episode 33700: avg_reward: 0.097, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.579115 episode 33800: avg_reward: 0.085, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.753821 episode 33900: avg_reward: 0.065, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.691865 episode 34000: avg_reward: 0.048, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.606520 episode 34100: avg_reward: 0.062, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.618621 episode 34200: avg_reward: 0.060, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.584511 episode 34300: avg_reward: 0.103, steps: 5 loss: 0.11, exp_val: 0.08\n",
      "0:00:35.584414 episode 34400: avg_reward: 0.106, steps: 5 loss: 0.11, exp_val: -0.08\n",
      "0:00:35.547996 episode 34500: avg_reward: 0.043, steps: 5 loss: 0.12, exp_val: 0.06\n",
      "0:00:35.563943 episode 34600: avg_reward: 0.012, steps: 5 loss: 0.18, exp_val: 0.25\n",
      "0:00:35.558423 episode 34700: avg_reward: 0.088, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.727182 episode 34800: avg_reward: 0.085, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.649663 episode 34900: avg_reward: 0.017, steps: 5 loss: 0.11, exp_val: 0.01\n",
      "0:00:35.572722 episode 35000: avg_reward: 0.079, steps: 5 loss: 0.13, exp_val: -0.05\n",
      "0:00:35.560615 episode 35100: avg_reward: 0.084, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.572299 episode 35200: avg_reward: 0.087, steps: 5 loss: 0.17, exp_val: 0.10\n",
      "0:00:35.603871 episode 35300: avg_reward: 0.081, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.632727 episode 35400: avg_reward: 0.066, steps: 5 loss: 0.13, exp_val: -0.08\n",
      "0:00:35.643765 episode 35500: avg_reward: 0.036, steps: 5 loss: 0.10, exp_val: -0.08\n",
      "0:00:35.874645 episode 35600: avg_reward: 0.083, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.656900 episode 35700: avg_reward: 0.102, steps: 5 loss: 0.17, exp_val: 0.14\n",
      "0:00:35.483832 episode 35800: avg_reward: 0.035, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.571803 episode 35900: avg_reward: 0.070, steps: 5 loss: 0.16, exp_val: -0.05\n",
      "0:00:35.674701 episode 36000: avg_reward: 0.116, steps: 5 loss: 0.15, exp_val: -0.07\n",
      "0:00:35.550978 episode 36100: avg_reward: 0.065, steps: 5 loss: 0.20, exp_val: 0.12\n",
      "0:00:35.523642 episode 36200: avg_reward: 0.048, steps: 5 loss: 0.18, exp_val: 0.10\n",
      "0:00:35.602130 episode 36300: avg_reward: 0.035, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.591704 episode 36400: avg_reward: 0.068, steps: 5 loss: 0.11, exp_val: 0.01\n",
      "0:00:35.710933 episode 36500: avg_reward: 0.066, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.685100 episode 36600: avg_reward: 0.049, steps: 5 loss: 0.13, exp_val: 0.11\n",
      "0:00:35.533714 episode 36700: avg_reward: 0.078, steps: 5 loss: 0.21, exp_val: 0.04\n",
      "0:00:35.650248 episode 36800: avg_reward: 0.093, steps: 5 loss: 0.19, exp_val: 0.04\n",
      "0:00:35.590118 episode 36900: avg_reward: 0.057, steps: 5 loss: 0.10, exp_val: -0.02\n",
      "0:00:35.550851 episode 37000: avg_reward: 0.069, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.698266 episode 37100: avg_reward: 0.056, steps: 5 loss: 0.20, exp_val: -0.14\n",
      "0:00:35.555418 episode 37200: avg_reward: 0.057, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.604279 episode 37300: avg_reward: 0.046, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.601207 episode 37400: avg_reward: 0.084, steps: 5 loss: 0.24, exp_val: -0.11\n",
      "0:00:35.540002 episode 37500: avg_reward: 0.073, steps: 5 loss: 0.18, exp_val: 0.17\n",
      "0:00:35.577129 episode 37600: avg_reward: 0.087, steps: 5 loss: 0.14, exp_val: -0.07\n",
      "0:00:35.631825 episode 37700: avg_reward: 0.065, steps: 5 loss: 0.19, exp_val: 0.02\n",
      "0:00:35.601344 episode 37800: avg_reward: 0.086, steps: 5 loss: 0.18, exp_val: 0.10\n",
      "0:00:35.488709 episode 37900: avg_reward: 0.040, steps: 5 loss: 0.18, exp_val: -0.09\n",
      "0:00:35.508222 episode 38000: avg_reward: -0.015, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.581973 episode 38100: avg_reward: 0.020, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.562591 episode 38200: avg_reward: 0.009, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.557703 episode 38300: avg_reward: 0.048, steps: 5 loss: 0.16, exp_val: -0.05\n",
      "0:00:35.558775 episode 38400: avg_reward: 0.031, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.528586 episode 38500: avg_reward: 0.095, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.651006 episode 38600: avg_reward: 0.042, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.555974 episode 38700: avg_reward: 0.070, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.597318 episode 38800: avg_reward: 0.111, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.587121 episode 38900: avg_reward: 0.068, steps: 5 loss: 0.16, exp_val: -0.10\n",
      "0:00:35.662451 episode 39000: avg_reward: 0.060, steps: 5 loss: 0.12, exp_val: -0.04\n",
      "0:00:35.620836 episode 39100: avg_reward: 0.074, steps: 5 loss: 0.21, exp_val: 0.07\n",
      "0:00:35.588597 episode 39200: avg_reward: 0.081, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.615667 episode 39300: avg_reward: 0.090, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.629941 episode 39400: avg_reward: 0.055, steps: 5 loss: 0.12, exp_val: -0.04\n",
      "0:00:35.594099 episode 39500: avg_reward: 0.062, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.621301 episode 39600: avg_reward: 0.120, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.532515 episode 39700: avg_reward: 0.080, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.574943 episode 39800: avg_reward: 0.034, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.588338 episode 39900: avg_reward: 0.030, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.502961 episode 40000: avg_reward: 0.060, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.667910 episode 40100: avg_reward: 0.041, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.587922 episode 40200: avg_reward: 0.038, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.643569 episode 40300: avg_reward: 0.065, steps: 5 loss: 0.18, exp_val: 0.02\n",
      "0:00:35.674567 episode 40400: avg_reward: 0.072, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.630162 episode 40500: avg_reward: 0.129, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.531781 episode 40600: avg_reward: 0.104, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.705062 episode 40700: avg_reward: 0.056, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.680550 episode 40800: avg_reward: 0.122, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.624114 episode 40900: avg_reward: 0.082, steps: 5 loss: 0.19, exp_val: -0.01\n",
      "0:00:35.624282 episode 41000: avg_reward: 0.094, steps: 5 loss: 0.12, exp_val: -0.04\n",
      "0:00:35.560415 episode 41100: avg_reward: 0.016, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.620960 episode 41200: avg_reward: 0.071, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.525035 episode 41300: avg_reward: 0.088, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.570405 episode 41400: avg_reward: 0.115, steps: 5 loss: 0.14, exp_val: 0.09\n",
      "0:00:35.588364 episode 41500: avg_reward: 0.038, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.725631 episode 41600: avg_reward: 0.090, steps: 5 loss: 0.18, exp_val: 0.07\n",
      "0:00:35.596426 episode 41700: avg_reward: 0.044, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.599027 episode 41800: avg_reward: 0.074, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.531812 episode 41900: avg_reward: 0.020, steps: 5 loss: 0.19, exp_val: 0.04\n",
      "0:00:35.633569 episode 42000: avg_reward: 0.088, steps: 5 loss: 0.10, exp_val: -0.05\n",
      "0:00:35.678001 episode 42100: avg_reward: 0.096, steps: 5 loss: 0.21, exp_val: 0.10\n",
      "0:00:35.642312 episode 42200: avg_reward: 0.062, steps: 5 loss: 0.16, exp_val: 0.17\n",
      "0:00:35.631697 episode 42300: avg_reward: 0.066, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.710189 episode 42400: avg_reward: 0.148, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.577025 episode 42500: avg_reward: 0.079, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.601773 episode 42600: avg_reward: 0.103, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.692615 episode 42700: avg_reward: 0.069, steps: 5 loss: 0.21, exp_val: -0.03\n",
      "0:00:35.645060 episode 42800: avg_reward: 0.070, steps: 5 loss: 0.19, exp_val: -0.01\n",
      "0:00:35.571025 episode 42900: avg_reward: 0.105, steps: 5 loss: 0.17, exp_val: -0.04\n",
      "0:00:35.591342 episode 43000: avg_reward: 0.034, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.591988 episode 43100: avg_reward: 0.045, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.799400 episode 43200: avg_reward: 0.053, steps: 5 loss: 0.21, exp_val: 0.10\n",
      "0:00:35.622252 episode 43300: avg_reward: 0.027, steps: 5 loss: 0.19, exp_val: 0.06\n",
      "0:00:35.540712 episode 43400: avg_reward: 0.006, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.651210 episode 43500: avg_reward: 0.034, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.535441 episode 43600: avg_reward: 0.069, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.625507 episode 43700: avg_reward: 0.081, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.616837 episode 43800: avg_reward: 0.134, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.601325 episode 43900: avg_reward: 0.071, steps: 5 loss: 0.18, exp_val: -0.07\n",
      "0:00:35.617985 episode 44000: avg_reward: 0.070, steps: 5 loss: 0.17, exp_val: -0.04\n",
      "0:00:35.745111 episode 44100: avg_reward: 0.105, steps: 5 loss: 0.23, exp_val: 0.12\n",
      "0:00:35.638224 episode 44200: avg_reward: 0.074, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.506129 episode 44300: avg_reward: 0.067, steps: 5 loss: 0.13, exp_val: 0.15\n",
      "0:00:35.570195 episode 44400: avg_reward: 0.085, steps: 5 loss: 0.14, exp_val: 0.07\n",
      "0:00:35.600986 episode 44500: avg_reward: 0.101, steps: 5 loss: 0.19, exp_val: 0.01\n",
      "0:00:35.563282 episode 44600: avg_reward: 0.067, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.595667 episode 44700: avg_reward: 0.090, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.549928 episode 44800: avg_reward: 0.024, steps: 5 loss: 0.21, exp_val: -0.03\n",
      "0:00:35.653555 episode 44900: avg_reward: 0.060, steps: 5 loss: 0.15, exp_val: -0.04\n",
      "0:00:35.510003 episode 45000: avg_reward: 0.113, steps: 5 loss: 0.15, exp_val: -0.07\n",
      "0:00:35.501164 episode 45100: avg_reward: 0.084, steps: 5 loss: 0.10, exp_val: -0.03\n",
      "0:00:35.599115 episode 45200: avg_reward: 0.067, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.506844 episode 45300: avg_reward: 0.026, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.646802 episode 45400: avg_reward: 0.049, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.516648 episode 45500: avg_reward: 0.092, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.553837 episode 45600: avg_reward: 0.045, steps: 5 loss: 0.09, exp_val: 0.03\n",
      "0:00:35.630579 episode 45700: avg_reward: 0.050, steps: 5 loss: 0.19, exp_val: 0.07\n",
      "0:00:35.737256 episode 45800: avg_reward: 0.039, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.538951 episode 45900: avg_reward: 0.047, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.546141 episode 46000: avg_reward: 0.038, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.558566 episode 46100: avg_reward: 0.079, steps: 5 loss: 0.21, exp_val: -0.09\n",
      "0:00:35.656849 episode 46200: avg_reward: 0.115, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.575412 episode 46300: avg_reward: 0.071, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.532936 episode 46400: avg_reward: 0.044, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.629276 episode 46500: avg_reward: 0.030, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.696467 episode 46600: avg_reward: 0.014, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.710260 episode 46700: avg_reward: 0.041, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.771430 episode 46800: avg_reward: 0.058, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:36.068743 episode 46900: avg_reward: 0.047, steps: 5 loss: 0.24, exp_val: 0.00\n",
      "0:00:35.659513 episode 47000: avg_reward: 0.057, steps: 5 loss: 0.18, exp_val: 0.10\n",
      "0:00:35.614630 episode 47100: avg_reward: 0.091, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.634098 episode 47200: avg_reward: 0.112, steps: 5 loss: 0.11, exp_val: -0.02\n",
      "0:00:35.666607 episode 47300: avg_reward: 0.120, steps: 5 loss: 0.22, exp_val: 0.17\n",
      "0:00:35.768687 episode 47400: avg_reward: 0.026, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.712758 episode 47500: avg_reward: 0.103, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.638878 episode 47600: avg_reward: 0.096, steps: 5 loss: 0.18, exp_val: 0.14\n",
      "0:00:35.681956 episode 47700: avg_reward: 0.034, steps: 5 loss: 0.19, exp_val: 0.02\n",
      "0:00:35.717370 episode 47800: avg_reward: 0.051, steps: 5 loss: 0.16, exp_val: 0.14\n",
      "0:00:35.733367 episode 47900: avg_reward: 0.071, steps: 5 loss: 0.18, exp_val: 0.15\n",
      "0:00:35.790984 episode 48000: avg_reward: 0.089, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.694194 episode 48100: avg_reward: 0.069, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.699654 episode 48200: avg_reward: 0.005, steps: 5 loss: 0.15, exp_val: -0.02\n",
      "0:00:35.791863 episode 48300: avg_reward: 0.028, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.655541 episode 48400: avg_reward: 0.025, steps: 5 loss: 0.20, exp_val: 0.02\n",
      "0:00:35.610150 episode 48500: avg_reward: 0.070, steps: 5 loss: 0.21, exp_val: -0.06\n",
      "0:00:35.725772 episode 48600: avg_reward: 0.068, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.663076 episode 48700: avg_reward: 0.091, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.636954 episode 48800: avg_reward: 0.100, steps: 5 loss: 0.19, exp_val: -0.02\n",
      "0:00:35.698996 episode 48900: avg_reward: 0.081, steps: 5 loss: 0.14, exp_val: 0.07\n",
      "0:00:35.652244 episode 49000: avg_reward: 0.067, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.722863 episode 49100: avg_reward: 0.077, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.805638 episode 49200: avg_reward: 0.101, steps: 5 loss: 0.14, exp_val: 0.07\n",
      "0:00:35.691414 episode 49300: avg_reward: 0.092, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.799440 episode 49400: avg_reward: 0.096, steps: 5 loss: 0.24, exp_val: 0.16\n",
      "0:00:35.720632 episode 49500: avg_reward: 0.055, steps: 5 loss: 0.15, exp_val: -0.04\n",
      "0:00:35.728378 episode 49600: avg_reward: 0.033, steps: 5 loss: 0.20, exp_val: 0.02\n",
      "0:00:35.638303 episode 49700: avg_reward: 0.064, steps: 5 loss: 0.20, exp_val: 0.02\n",
      "0:00:35.678085 episode 49800: avg_reward: 0.002, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.805000 episode 49900: avg_reward: 0.092, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.723333 episode 50000: avg_reward: 0.066, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.743426 episode 50100: avg_reward: 0.065, steps: 5 loss: 0.21, exp_val: -0.03\n",
      "0:00:35.743901 episode 50200: avg_reward: 0.068, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.739488 episode 50300: avg_reward: 0.076, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.721905 episode 50400: avg_reward: 0.031, steps: 5 loss: 0.18, exp_val: -0.05\n",
      "0:00:35.725867 episode 50500: avg_reward: 0.041, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.744860 episode 50600: avg_reward: 0.080, steps: 5 loss: 0.20, exp_val: -0.01\n",
      "0:00:35.675038 episode 50700: avg_reward: 0.104, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.791376 episode 50800: avg_reward: 0.075, steps: 5 loss: 0.23, exp_val: 0.08\n",
      "0:00:35.720501 episode 50900: avg_reward: 0.067, steps: 5 loss: 0.22, exp_val: 0.02\n",
      "0:00:35.740591 episode 51000: avg_reward: 0.070, steps: 5 loss: 0.13, exp_val: 0.14\n",
      "0:00:35.674450 episode 51100: avg_reward: 0.076, steps: 5 loss: 0.19, exp_val: -0.01\n",
      "0:00:35.690908 episode 51200: avg_reward: 0.055, steps: 5 loss: 0.09, exp_val: 0.06\n",
      "0:00:35.756785 episode 51300: avg_reward: 0.047, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.736173 episode 51400: avg_reward: 0.040, steps: 5 loss: 0.16, exp_val: -0.07\n",
      "0:00:35.708583 episode 51500: avg_reward: 0.080, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.598886 episode 51600: avg_reward: 0.014, steps: 5 loss: 0.16, exp_val: -0.05\n",
      "0:00:35.715661 episode 51700: avg_reward: 0.011, steps: 5 loss: 0.15, exp_val: 0.14\n",
      "0:00:35.609569 episode 51800: avg_reward: -0.007, steps: 5 loss: 0.17, exp_val: 0.15\n",
      "0:00:35.579550 episode 51900: avg_reward: -0.001, steps: 5 loss: 0.12, exp_val: 0.09\n",
      "0:00:35.650114 episode 52000: avg_reward: 0.028, steps: 5 loss: 0.10, exp_val: 0.11\n",
      "0:00:35.662689 episode 52100: avg_reward: 0.012, steps: 5 loss: 0.16, exp_val: -0.12\n",
      "0:00:35.640478 episode 52200: avg_reward: 0.016, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.590520 episode 52300: avg_reward: 0.023, steps: 5 loss: 0.19, exp_val: -0.01\n",
      "0:00:35.605652 episode 52400: avg_reward: 0.039, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.766083 episode 52500: avg_reward: 0.116, steps: 5 loss: 0.15, exp_val: -0.02\n",
      "0:00:35.681259 episode 52600: avg_reward: 0.062, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.674896 episode 52700: avg_reward: 0.058, steps: 5 loss: 0.12, exp_val: -0.04\n",
      "0:00:35.626291 episode 52800: avg_reward: 0.103, steps: 5 loss: 0.18, exp_val: -0.06\n",
      "0:00:35.634060 episode 52900: avg_reward: 0.094, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.784211 episode 53000: avg_reward: 0.129, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.656703 episode 53100: avg_reward: 0.098, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.772316 episode 53200: avg_reward: 0.037, steps: 5 loss: 0.20, exp_val: 0.12\n",
      "0:00:35.786986 episode 53300: avg_reward: 0.111, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.774013 episode 53400: avg_reward: 0.067, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.686964 episode 53500: avg_reward: 0.088, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.678156 episode 53600: avg_reward: 0.096, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.652529 episode 53700: avg_reward: 0.050, steps: 5 loss: 0.12, exp_val: 0.06\n",
      "0:00:35.700610 episode 53800: avg_reward: 0.055, steps: 5 loss: 0.10, exp_val: 0.16\n",
      "0:00:35.723450 episode 53900: avg_reward: 0.062, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.610126 episode 54000: avg_reward: 0.046, steps: 5 loss: 0.26, exp_val: 0.07\n",
      "0:00:35.725079 episode 54100: avg_reward: 0.104, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.696010 episode 54200: avg_reward: 0.081, steps: 5 loss: 0.26, exp_val: 0.07\n",
      "0:00:35.718069 episode 54300: avg_reward: 0.109, steps: 5 loss: 0.20, exp_val: -0.14\n",
      "0:00:35.698790 episode 54400: avg_reward: 0.099, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.685439 episode 54500: avg_reward: 0.109, steps: 5 loss: 0.11, exp_val: 0.01\n",
      "0:00:35.674679 episode 54600: avg_reward: 0.080, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.664163 episode 54700: avg_reward: 0.100, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.618203 episode 54800: avg_reward: 0.026, steps: 5 loss: 0.15, exp_val: -0.02\n",
      "0:00:35.690988 episode 54900: avg_reward: 0.110, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.883888 episode 55000: avg_reward: 0.127, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.756844 episode 55100: avg_reward: 0.055, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.719406 episode 55200: avg_reward: 0.013, steps: 5 loss: 0.19, exp_val: 0.04\n",
      "0:00:35.669659 episode 55300: avg_reward: -0.010, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.672680 episode 55400: avg_reward: 0.078, steps: 5 loss: 0.11, exp_val: 0.01\n",
      "0:00:35.717969 episode 55500: avg_reward: 0.083, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.635135 episode 55600: avg_reward: 0.045, steps: 5 loss: 0.10, exp_val: 0.12\n",
      "0:00:35.615032 episode 55700: avg_reward: 0.050, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.787374 episode 55800: avg_reward: 0.095, steps: 5 loss: 0.13, exp_val: 0.12\n",
      "0:00:35.675387 episode 55900: avg_reward: 0.064, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.846370 episode 56000: avg_reward: 0.094, steps: 5 loss: 0.14, exp_val: -0.02\n",
      "0:00:35.620307 episode 56100: avg_reward: 0.048, steps: 5 loss: 0.20, exp_val: 0.18\n",
      "0:00:35.707705 episode 56200: avg_reward: 0.068, steps: 5 loss: 0.19, exp_val: 0.07\n",
      "0:00:35.795805 episode 56300: avg_reward: 0.067, steps: 5 loss: 0.09, exp_val: 0.06\n",
      "0:00:35.725613 episode 56400: avg_reward: 0.111, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.719688 episode 56500: avg_reward: 0.109, steps: 5 loss: 0.16, exp_val: -0.05\n",
      "0:00:35.656853 episode 56600: avg_reward: 0.074, steps: 5 loss: 0.12, exp_val: 0.09\n",
      "0:00:35.615170 episode 56700: avg_reward: 0.074, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.721699 episode 56800: avg_reward: 0.087, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.672433 episode 56900: avg_reward: 0.081, steps: 5 loss: 0.17, exp_val: -0.07\n",
      "0:00:35.624185 episode 57000: avg_reward: 0.025, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.629040 episode 57100: avg_reward: 0.028, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.650522 episode 57200: avg_reward: 0.124, steps: 5 loss: 0.16, exp_val: 0.17\n",
      "0:00:35.722047 episode 57300: avg_reward: 0.095, steps: 5 loss: 0.14, exp_val: -0.07\n",
      "0:00:35.651596 episode 57400: avg_reward: 0.053, steps: 5 loss: 0.10, exp_val: 0.12\n",
      "0:00:35.710546 episode 57500: avg_reward: 0.038, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.646428 episode 57600: avg_reward: 0.047, steps: 5 loss: 0.18, exp_val: 0.10\n",
      "0:00:35.613163 episode 57700: avg_reward: 0.105, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.649241 episode 57800: avg_reward: 0.055, steps: 5 loss: 0.13, exp_val: 0.17\n",
      "0:00:35.624818 episode 57900: avg_reward: 0.074, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.736425 episode 58000: avg_reward: 0.059, steps: 5 loss: 0.18, exp_val: 0.15\n",
      "0:00:35.684408 episode 58100: avg_reward: 0.029, steps: 5 loss: 0.14, exp_val: 0.17\n",
      "0:00:35.708389 episode 58200: avg_reward: 0.047, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.644858 episode 58300: avg_reward: 0.036, steps: 5 loss: 0.11, exp_val: 0.08\n",
      "0:00:35.799333 episode 58400: avg_reward: 0.028, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.654406 episode 58500: avg_reward: 0.102, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.692412 episode 58600: avg_reward: 0.089, steps: 5 loss: 0.21, exp_val: 0.04\n",
      "0:00:35.646987 episode 58700: avg_reward: 0.152, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.616839 episode 58800: avg_reward: 0.099, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.658432 episode 58900: avg_reward: 0.124, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.656853 episode 59000: avg_reward: 0.118, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.602804 episode 59100: avg_reward: 0.106, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.711126 episode 59200: avg_reward: 0.064, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.676146 episode 59300: avg_reward: 0.094, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.613214 episode 59400: avg_reward: 0.078, steps: 5 loss: 0.15, exp_val: -0.05\n",
      "0:00:35.688329 episode 59500: avg_reward: 0.084, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.689265 episode 59600: avg_reward: 0.072, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.635812 episode 59700: avg_reward: 0.060, steps: 5 loss: 0.11, exp_val: 0.03\n",
      "0:00:35.658417 episode 59800: avg_reward: 0.071, steps: 5 loss: 0.14, exp_val: -0.05\n",
      "0:00:35.635682 episode 59900: avg_reward: 0.071, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.664756 episode 60000: avg_reward: 0.091, steps: 5 loss: 0.13, exp_val: -0.05\n",
      "0:00:35.691633 episode 60100: avg_reward: 0.084, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.573339 episode 60200: avg_reward: 0.105, steps: 5 loss: 0.21, exp_val: 0.01\n",
      "0:00:35.592621 episode 60300: avg_reward: 0.085, steps: 5 loss: 0.12, exp_val: -0.10\n",
      "0:00:35.608045 episode 60400: avg_reward: 0.096, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.587152 episode 60500: avg_reward: 0.100, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.597546 episode 60600: avg_reward: 0.081, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.638861 episode 60700: avg_reward: 0.037, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.598146 episode 60800: avg_reward: 0.044, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.657363 episode 60900: avg_reward: 0.071, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.658766 episode 61000: avg_reward: 0.084, steps: 5 loss: 0.10, exp_val: 0.01\n",
      "0:00:35.648202 episode 61100: avg_reward: 0.083, steps: 5 loss: 0.13, exp_val: 0.11\n",
      "0:00:35.729969 episode 61200: avg_reward: 0.042, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.640711 episode 61300: avg_reward: 0.060, steps: 5 loss: 0.13, exp_val: 0.14\n",
      "0:00:35.562008 episode 61400: avg_reward: -0.014, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.532553 episode 61500: avg_reward: -0.031, steps: 5 loss: 0.14, exp_val: -0.02\n",
      "0:00:35.626360 episode 61600: avg_reward: 0.064, steps: 5 loss: 0.20, exp_val: -0.20\n",
      "0:00:35.613767 episode 61700: avg_reward: 0.062, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.495352 episode 61800: avg_reward: 0.025, steps: 5 loss: 0.10, exp_val: 0.08\n",
      "0:00:35.512420 episode 61900: avg_reward: 0.026, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.514254 episode 62000: avg_reward: 0.036, steps: 5 loss: 0.20, exp_val: -0.09\n",
      "0:00:35.557317 episode 62100: avg_reward: 0.083, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.624228 episode 62200: avg_reward: 0.128, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.675898 episode 62300: avg_reward: 0.127, steps: 5 loss: 0.14, exp_val: -0.04\n",
      "0:00:35.650660 episode 62400: avg_reward: 0.114, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.576307 episode 62500: avg_reward: 0.083, steps: 5 loss: 0.20, exp_val: -0.04\n",
      "0:00:35.758378 episode 62600: avg_reward: 0.077, steps: 5 loss: 0.10, exp_val: 0.01\n",
      "0:00:35.611956 episode 62700: avg_reward: 0.076, steps: 5 loss: 0.19, exp_val: 0.06\n",
      "0:00:35.580232 episode 62800: avg_reward: 0.075, steps: 5 loss: 0.20, exp_val: 0.09\n",
      "0:00:35.628705 episode 62900: avg_reward: 0.125, steps: 5 loss: 0.14, exp_val: -0.05\n",
      "0:00:35.681427 episode 63000: avg_reward: 0.031, steps: 5 loss: 0.16, exp_val: -0.01\n",
      "0:00:35.582901 episode 63100: avg_reward: 0.043, steps: 5 loss: 0.14, exp_val: 0.07\n",
      "0:00:35.653390 episode 63200: avg_reward: 0.035, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.567816 episode 63300: avg_reward: 0.045, steps: 5 loss: 0.12, exp_val: 0.12\n",
      "0:00:35.777003 episode 63400: avg_reward: 0.051, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.715534 episode 63500: avg_reward: 0.064, steps: 5 loss: 0.10, exp_val: 0.06\n",
      "0:00:35.620321 episode 63600: avg_reward: 0.037, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.639563 episode 63700: avg_reward: 0.038, steps: 5 loss: 0.17, exp_val: -0.10\n",
      "0:00:35.655254 episode 63800: avg_reward: 0.045, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.640362 episode 63900: avg_reward: 0.073, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.587453 episode 64000: avg_reward: 0.096, steps: 5 loss: 0.19, exp_val: 0.01\n",
      "0:00:35.588155 episode 64100: avg_reward: 0.104, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.738313 episode 64200: avg_reward: 0.121, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.707159 episode 64300: avg_reward: 0.094, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.556203 episode 64400: avg_reward: 0.057, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.581592 episode 64500: avg_reward: 0.059, steps: 5 loss: 0.18, exp_val: -0.06\n",
      "0:00:35.587516 episode 64600: avg_reward: 0.062, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.642364 episode 64700: avg_reward: 0.118, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.608343 episode 64800: avg_reward: 0.045, steps: 5 loss: 0.13, exp_val: 0.06\n",
      "0:00:35.657164 episode 64900: avg_reward: 0.082, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.570993 episode 65000: avg_reward: 0.074, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.763305 episode 65100: avg_reward: 0.090, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.630974 episode 65200: avg_reward: 0.074, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.528276 episode 65300: avg_reward: 0.084, steps: 5 loss: 0.19, exp_val: -0.07\n",
      "0:00:35.574043 episode 65400: avg_reward: 0.065, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.674331 episode 65500: avg_reward: 0.081, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.660407 episode 65600: avg_reward: 0.081, steps: 5 loss: 0.11, exp_val: 0.08\n",
      "0:00:35.680821 episode 65700: avg_reward: 0.039, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.536194 episode 65800: avg_reward: 0.018, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.685261 episode 65900: avg_reward: 0.039, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.646592 episode 66000: avg_reward: 0.025, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.578689 episode 66100: avg_reward: 0.081, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.566878 episode 66200: avg_reward: -0.001, steps: 5 loss: 0.19, exp_val: -0.06\n",
      "0:00:35.472548 episode 66300: avg_reward: -0.015, steps: 5 loss: 0.15, exp_val: 0.15\n",
      "0:00:35.558841 episode 66400: avg_reward: -0.004, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.635067 episode 66500: avg_reward: 0.031, steps: 5 loss: 0.10, exp_val: 0.11\n",
      "0:00:35.552960 episode 66600: avg_reward: 0.009, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.653792 episode 66700: avg_reward: 0.081, steps: 5 loss: 0.17, exp_val: 0.20\n",
      "0:00:35.728708 episode 66800: avg_reward: 0.056, steps: 5 loss: 0.12, exp_val: 0.06\n",
      "0:00:35.643206 episode 66900: avg_reward: 0.065, steps: 5 loss: 0.10, exp_val: 0.09\n",
      "0:00:35.645269 episode 67000: avg_reward: 0.026, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.602011 episode 67100: avg_reward: 0.046, steps: 5 loss: 0.19, exp_val: 0.12\n",
      "0:00:35.493692 episode 67200: avg_reward: 0.026, steps: 5 loss: 0.19, exp_val: -0.01\n",
      "0:00:35.569586 episode 67300: avg_reward: 0.080, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.522574 episode 67400: avg_reward: 0.071, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.489669 episode 67500: avg_reward: 0.065, steps: 5 loss: 0.19, exp_val: -0.02\n",
      "0:00:35.640385 episode 67600: avg_reward: 0.034, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.618719 episode 67700: avg_reward: 0.073, steps: 5 loss: 0.19, exp_val: -0.06\n",
      "0:00:35.615983 episode 67800: avg_reward: 0.120, steps: 5 loss: 0.16, exp_val: 0.01\n",
      "0:00:35.571950 episode 67900: avg_reward: 0.053, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.641591 episode 68000: avg_reward: 0.061, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.558819 episode 68100: avg_reward: 0.041, steps: 5 loss: 0.14, exp_val: -0.05\n",
      "0:00:35.548832 episode 68200: avg_reward: 0.030, steps: 5 loss: 0.23, exp_val: 0.10\n",
      "0:00:35.522832 episode 68300: avg_reward: -0.019, steps: 5 loss: 0.11, exp_val: 0.17\n",
      "0:00:35.555100 episode 68400: avg_reward: 0.024, steps: 5 loss: 0.10, exp_val: 0.01\n",
      "0:00:35.718108 episode 68500: avg_reward: 0.086, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.534267 episode 68600: avg_reward: 0.097, steps: 5 loss: 0.17, exp_val: 0.15\n",
      "0:00:35.505333 episode 68700: avg_reward: 0.064, steps: 5 loss: 0.13, exp_val: 0.15\n",
      "0:00:35.527985 episode 68800: avg_reward: 0.097, steps: 5 loss: 0.15, exp_val: -0.02\n",
      "0:00:35.500338 episode 68900: avg_reward: 0.104, steps: 5 loss: 0.12, exp_val: 0.09\n",
      "0:00:35.527994 episode 69000: avg_reward: 0.078, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.649507 episode 69100: avg_reward: 0.100, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.558533 episode 69200: avg_reward: 0.106, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.597412 episode 69300: avg_reward: 0.064, steps: 5 loss: 0.16, exp_val: -0.10\n",
      "0:00:35.583302 episode 69400: avg_reward: 0.098, steps: 5 loss: 0.11, exp_val: 0.01\n",
      "0:00:35.489961 episode 69500: avg_reward: 0.047, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.624599 episode 69600: avg_reward: 0.052, steps: 5 loss: 0.17, exp_val: 0.14\n",
      "0:00:35.618835 episode 69700: avg_reward: 0.065, steps: 5 loss: 0.21, exp_val: 0.13\n",
      "0:00:35.646541 episode 69800: avg_reward: 0.080, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.625924 episode 69900: avg_reward: 0.083, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.557673 episode 70000: avg_reward: 0.081, steps: 5 loss: 0.12, exp_val: 0.09\n",
      "0:00:35.580250 episode 70100: avg_reward: 0.053, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.512619 episode 70200: avg_reward: 0.039, steps: 5 loss: 0.19, exp_val: 0.07\n",
      "0:00:35.556526 episode 70300: avg_reward: 0.060, steps: 5 loss: 0.20, exp_val: -0.09\n",
      "0:00:35.508449 episode 70400: avg_reward: 0.086, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.618125 episode 70500: avg_reward: 0.066, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.593526 episode 70600: avg_reward: 0.079, steps: 5 loss: 0.11, exp_val: -0.00\n",
      "0:00:35.557962 episode 70700: avg_reward: 0.151, steps: 6 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.523532 episode 70800: avg_reward: 0.115, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.657115 episode 70900: avg_reward: 0.056, steps: 5 loss: 0.13, exp_val: -0.12\n",
      "0:00:35.603653 episode 71000: avg_reward: 0.015, steps: 5 loss: 0.21, exp_val: 0.01\n",
      "0:00:35.620208 episode 71100: avg_reward: 0.028, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.623652 episode 71200: avg_reward: 0.038, steps: 5 loss: 0.21, exp_val: 0.02\n",
      "0:00:35.595826 episode 71300: avg_reward: 0.065, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.568790 episode 71400: avg_reward: 0.104, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.583026 episode 71500: avg_reward: 0.085, steps: 5 loss: 0.20, exp_val: -0.12\n",
      "0:00:35.511307 episode 71600: avg_reward: 0.094, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.543801 episode 71700: avg_reward: 0.100, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.549211 episode 71800: avg_reward: 0.040, steps: 5 loss: 0.16, exp_val: -0.05\n",
      "0:00:35.516042 episode 71900: avg_reward: 0.060, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.409412 episode 72000: avg_reward: 0.039, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.367032 episode 72100: avg_reward: 0.041, steps: 5 loss: 0.14, exp_val: -0.07\n",
      "0:00:35.568586 episode 72200: avg_reward: 0.018, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.498560 episode 72300: avg_reward: 0.091, steps: 5 loss: 0.12, exp_val: 0.12\n",
      "0:00:35.481459 episode 72400: avg_reward: 0.086, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.415370 episode 72500: avg_reward: 0.048, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.593940 episode 72600: avg_reward: 0.064, steps: 5 loss: 0.10, exp_val: 0.08\n",
      "0:00:35.680194 episode 72700: avg_reward: 0.120, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.424158 episode 72800: avg_reward: 0.063, steps: 5 loss: 0.18, exp_val: 0.01\n",
      "0:00:35.466686 episode 72900: avg_reward: 0.061, steps: 5 loss: 0.11, exp_val: 0.08\n",
      "0:00:35.498938 episode 73000: avg_reward: 0.057, steps: 5 loss: 0.19, exp_val: 0.23\n",
      "0:00:35.604087 episode 73100: avg_reward: 0.106, steps: 5 loss: 0.12, exp_val: 0.12\n",
      "0:00:35.757464 episode 73200: avg_reward: 0.145, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.579822 episode 73300: avg_reward: 0.091, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.531032 episode 73400: avg_reward: 0.055, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.498107 episode 73500: avg_reward: 0.073, steps: 5 loss: 0.18, exp_val: -0.06\n",
      "0:00:35.507753 episode 73600: avg_reward: 0.094, steps: 5 loss: 0.16, exp_val: 0.17\n",
      "0:00:35.487634 episode 73700: avg_reward: 0.058, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.528133 episode 73800: avg_reward: 0.095, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.597767 episode 73900: avg_reward: 0.107, steps: 5 loss: 0.20, exp_val: 0.23\n",
      "0:00:35.526215 episode 74000: avg_reward: 0.127, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.532180 episode 74100: avg_reward: 0.133, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.572020 episode 74200: avg_reward: 0.038, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.578699 episode 74300: avg_reward: 0.075, steps: 5 loss: 0.23, exp_val: 0.05\n",
      "0:00:35.614086 episode 74400: avg_reward: 0.089, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.598347 episode 74500: avg_reward: 0.056, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.501010 episode 74600: avg_reward: 0.063, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.628043 episode 74700: avg_reward: 0.011, steps: 5 loss: 0.16, exp_val: -0.04\n",
      "0:00:35.526439 episode 74800: avg_reward: 0.030, steps: 5 loss: 0.07, exp_val: 0.03\n",
      "0:00:35.522801 episode 74900: avg_reward: 0.079, steps: 5 loss: 0.14, exp_val: -0.04\n",
      "0:00:35.487599 episode 75000: avg_reward: 0.037, steps: 5 loss: 0.12, exp_val: 0.16\n",
      "0:00:35.596114 episode 75100: avg_reward: 0.074, steps: 5 loss: 0.18, exp_val: 0.15\n",
      "0:00:35.713902 episode 75200: avg_reward: 0.052, steps: 5 loss: 0.14, exp_val: 0.14\n",
      "0:00:35.647195 episode 75300: avg_reward: 0.096, steps: 5 loss: 0.15, exp_val: 0.25\n",
      "0:00:35.504623 episode 75400: avg_reward: 0.052, steps: 5 loss: 0.15, exp_val: 0.09\n",
      "0:00:35.494780 episode 75500: avg_reward: 0.146, steps: 5 loss: 0.20, exp_val: 0.09\n",
      "saving model..\n",
      "0:00:35.553708 episode 75600: avg_reward: 0.089, steps: 5 loss: 0.15, exp_val: 0.15\n",
      "0:00:35.519961 episode 75700: avg_reward: 0.102, steps: 5 loss: 0.20, exp_val: 0.09\n",
      "0:00:35.568261 episode 75800: avg_reward: 0.084, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.593590 episode 75900: avg_reward: 0.103, steps: 5 loss: 0.18, exp_val: 0.07\n",
      "0:00:35.576452 episode 76000: avg_reward: 0.043, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.503413 episode 76100: avg_reward: 0.081, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.489805 episode 76200: avg_reward: 0.039, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.579627 episode 76300: avg_reward: 0.029, steps: 5 loss: 0.16, exp_val: -0.09\n",
      "0:00:35.603403 episode 76400: avg_reward: 0.082, steps: 5 loss: 0.15, exp_val: 0.07\n",
      "0:00:35.613244 episode 76500: avg_reward: 0.048, steps: 5 loss: 0.08, exp_val: 0.01\n",
      "0:00:35.553246 episode 76600: avg_reward: 0.082, steps: 5 loss: 0.21, exp_val: 0.17\n",
      "0:00:35.630030 episode 76700: avg_reward: 0.078, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.667616 episode 76800: avg_reward: 0.097, steps: 5 loss: 0.18, exp_val: 0.18\n",
      "0:00:35.622987 episode 76900: avg_reward: 0.080, steps: 5 loss: 0.15, exp_val: -0.07\n",
      "0:00:35.532180 episode 77000: avg_reward: 0.113, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.499454 episode 77100: avg_reward: 0.114, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.586658 episode 77200: avg_reward: 0.101, steps: 5 loss: 0.09, exp_val: -0.07\n",
      "0:00:35.658358 episode 77300: avg_reward: 0.116, steps: 5 loss: 0.16, exp_val: -0.10\n",
      "0:00:35.800951 episode 77400: avg_reward: 0.096, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.588388 episode 77500: avg_reward: 0.131, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.637072 episode 77600: avg_reward: 0.109, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.654510 episode 77700: avg_reward: 0.097, steps: 5 loss: 0.18, exp_val: 0.07\n",
      "0:00:35.707020 episode 77800: avg_reward: 0.120, steps: 5 loss: 0.22, exp_val: 0.10\n",
      "0:00:35.567265 episode 77900: avg_reward: 0.129, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.529209 episode 78000: avg_reward: 0.106, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.622325 episode 78100: avg_reward: 0.058, steps: 5 loss: 0.14, exp_val: -0.04\n",
      "0:00:35.605306 episode 78200: avg_reward: 0.042, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.623475 episode 78300: avg_reward: 0.052, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.589563 episode 78400: avg_reward: 0.097, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.568885 episode 78500: avg_reward: 0.062, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.564629 episode 78600: avg_reward: 0.050, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.561041 episode 78700: avg_reward: 0.003, steps: 5 loss: 0.10, exp_val: 0.09\n",
      "0:00:35.577965 episode 78800: avg_reward: 0.036, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.531275 episode 78900: avg_reward: 0.028, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.589973 episode 79000: avg_reward: 0.075, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.484602 episode 79100: avg_reward: 0.055, steps: 5 loss: 0.21, exp_val: -0.09\n",
      "0:00:35.453383 episode 79200: avg_reward: 0.027, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.561051 episode 79300: avg_reward: 0.081, steps: 5 loss: 0.18, exp_val: 0.07\n",
      "0:00:35.651903 episode 79400: avg_reward: 0.065, steps: 5 loss: 0.19, exp_val: 0.14\n",
      "0:00:35.604229 episode 79500: avg_reward: 0.066, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.495449 episode 79600: avg_reward: 0.062, steps: 5 loss: 0.10, exp_val: -0.05\n",
      "0:00:35.476077 episode 79700: avg_reward: 0.014, steps: 5 loss: 0.20, exp_val: 0.09\n",
      "0:00:35.494063 episode 79800: avg_reward: 0.080, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.532993 episode 79900: avg_reward: 0.039, steps: 5 loss: 0.19, exp_val: -0.10\n",
      "0:00:35.534567 episode 80000: avg_reward: 0.089, steps: 5 loss: 0.17, exp_val: 0.02\n",
      "0:00:35.570898 episode 80100: avg_reward: 0.095, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.483277 episode 80200: avg_reward: 0.109, steps: 5 loss: 0.22, exp_val: 0.09\n",
      "0:00:35.725098 episode 80300: avg_reward: 0.076, steps: 5 loss: 0.12, exp_val: -0.04\n",
      "0:00:35.530951 episode 80400: avg_reward: 0.049, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.481167 episode 80500: avg_reward: 0.089, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.549575 episode 80600: avg_reward: 0.097, steps: 5 loss: 0.15, exp_val: -0.04\n",
      "0:00:35.529761 episode 80700: avg_reward: 0.033, steps: 5 loss: 0.11, exp_val: 0.01\n",
      "0:00:35.526261 episode 80800: avg_reward: 0.098, steps: 5 loss: 0.25, exp_val: -0.04\n",
      "0:00:35.585890 episode 80900: avg_reward: 0.073, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.463139 episode 81000: avg_reward: 0.040, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.543485 episode 81100: avg_reward: 0.064, steps: 5 loss: 0.15, exp_val: 0.15\n",
      "0:00:35.537520 episode 81200: avg_reward: 0.069, steps: 5 loss: 0.10, exp_val: -0.02\n",
      "0:00:35.518822 episode 81300: avg_reward: 0.040, steps: 5 loss: 0.21, exp_val: 0.04\n",
      "0:00:35.581749 episode 81400: avg_reward: 0.052, steps: 5 loss: 0.10, exp_val: -0.08\n",
      "0:00:35.594153 episode 81500: avg_reward: 0.038, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.575391 episode 81600: avg_reward: 0.015, steps: 5 loss: 0.13, exp_val: 0.09\n",
      "0:00:35.558179 episode 81700: avg_reward: 0.089, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.625732 episode 81800: avg_reward: 0.074, steps: 5 loss: 0.21, exp_val: 0.07\n",
      "0:00:35.614192 episode 81900: avg_reward: 0.078, steps: 5 loss: 0.21, exp_val: 0.04\n",
      "0:00:35.821871 episode 82000: avg_reward: 0.090, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.495757 episode 82100: avg_reward: 0.102, steps: 5 loss: 0.11, exp_val: 0.14\n",
      "0:00:35.563678 episode 82200: avg_reward: 0.093, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.602981 episode 82300: avg_reward: 0.073, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.543736 episode 82400: avg_reward: 0.074, steps: 5 loss: 0.10, exp_val: 0.05\n",
      "0:00:35.553097 episode 82500: avg_reward: 0.104, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.593113 episode 82600: avg_reward: 0.088, steps: 5 loss: 0.14, exp_val: 0.07\n",
      "0:00:35.601495 episode 82700: avg_reward: 0.094, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.570640 episode 82800: avg_reward: 0.070, steps: 5 loss: 0.19, exp_val: 0.06\n",
      "0:00:35.537344 episode 82900: avg_reward: 0.060, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.584703 episode 83000: avg_reward: 0.071, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.549262 episode 83100: avg_reward: 0.074, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.614619 episode 83200: avg_reward: 0.050, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.543664 episode 83300: avg_reward: 0.063, steps: 5 loss: 0.18, exp_val: 0.06\n",
      "0:00:35.607969 episode 83400: avg_reward: 0.072, steps: 5 loss: 0.19, exp_val: 0.02\n",
      "0:00:35.625396 episode 83500: avg_reward: 0.067, steps: 5 loss: 0.19, exp_val: 0.01\n",
      "0:00:35.688653 episode 83600: avg_reward: 0.143, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.680619 episode 83700: avg_reward: 0.056, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.438444 episode 83800: avg_reward: 0.034, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.480537 episode 83900: avg_reward: 0.032, steps: 5 loss: 0.17, exp_val: 0.10\n",
      "0:00:35.617667 episode 84000: avg_reward: 0.043, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.494893 episode 84100: avg_reward: 0.087, steps: 5 loss: 0.18, exp_val: 0.02\n",
      "0:00:35.592887 episode 84200: avg_reward: 0.055, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.483981 episode 84300: avg_reward: 0.009, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.528165 episode 84400: avg_reward: 0.092, steps: 5 loss: 0.17, exp_val: 0.18\n",
      "0:00:35.674199 episode 84500: avg_reward: 0.053, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.531552 episode 84600: avg_reward: 0.022, steps: 5 loss: 0.17, exp_val: 0.10\n",
      "0:00:35.610305 episode 84700: avg_reward: 0.078, steps: 5 loss: 0.20, exp_val: -0.04\n",
      "0:00:35.551885 episode 84800: avg_reward: 0.042, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.608594 episode 84900: avg_reward: 0.081, steps: 5 loss: 0.15, exp_val: -0.02\n",
      "0:00:35.639564 episode 85000: avg_reward: 0.083, steps: 5 loss: 0.18, exp_val: 0.10\n",
      "0:00:35.493868 episode 85100: avg_reward: 0.009, steps: 5 loss: 0.23, exp_val: 0.04\n",
      "0:00:35.449957 episode 85200: avg_reward: 0.021, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.555603 episode 85300: avg_reward: 0.006, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.523180 episode 85400: avg_reward: 0.035, steps: 5 loss: 0.16, exp_val: 0.15\n",
      "0:00:35.580654 episode 85500: avg_reward: 0.022, steps: 5 loss: 0.20, exp_val: -0.01\n",
      "0:00:35.507194 episode 85600: avg_reward: 0.034, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.489958 episode 85700: avg_reward: 0.041, steps: 5 loss: 0.18, exp_val: 0.14\n",
      "0:00:35.471589 episode 85800: avg_reward: 0.055, steps: 5 loss: 0.21, exp_val: 0.01\n",
      "0:00:35.462124 episode 85900: avg_reward: 0.104, steps: 5 loss: 0.22, exp_val: 0.05\n",
      "0:00:35.556809 episode 86000: avg_reward: 0.077, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.617353 episode 86100: avg_reward: 0.095, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.548220 episode 86200: avg_reward: 0.087, steps: 5 loss: 0.14, exp_val: 0.11\n",
      "0:00:35.487155 episode 86300: avg_reward: 0.048, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.541286 episode 86400: avg_reward: 0.026, steps: 5 loss: 0.20, exp_val: 0.06\n",
      "0:00:35.531141 episode 86500: avg_reward: 0.054, steps: 5 loss: 0.11, exp_val: -0.11\n",
      "0:00:35.561629 episode 86600: avg_reward: 0.061, steps: 5 loss: 0.17, exp_val: 0.06\n",
      "0:00:35.536393 episode 86700: avg_reward: 0.068, steps: 5 loss: 0.17, exp_val: 0.12\n",
      "0:00:35.487485 episode 86800: avg_reward: 0.070, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.518110 episode 86900: avg_reward: 0.099, steps: 5 loss: 0.12, exp_val: 0.06\n",
      "0:00:35.671501 episode 87000: avg_reward: 0.074, steps: 5 loss: 0.16, exp_val: -0.02\n",
      "0:00:35.508467 episode 87100: avg_reward: 0.101, steps: 5 loss: 0.18, exp_val: 0.10\n",
      "0:00:35.539086 episode 87200: avg_reward: 0.097, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.562547 episode 87300: avg_reward: 0.073, steps: 5 loss: 0.17, exp_val: -0.01\n",
      "0:00:35.574569 episode 87400: avg_reward: 0.103, steps: 5 loss: 0.16, exp_val: 0.09\n",
      "0:00:35.495110 episode 87500: avg_reward: 0.064, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.534937 episode 87600: avg_reward: 0.045, steps: 5 loss: 0.20, exp_val: -0.04\n",
      "0:00:35.547091 episode 87700: avg_reward: 0.084, steps: 5 loss: 0.12, exp_val: -0.00\n",
      "0:00:35.575388 episode 87800: avg_reward: 0.041, steps: 5 loss: 0.11, exp_val: 0.11\n",
      "0:00:35.845621 episode 87900: avg_reward: 0.123, steps: 5 loss: 0.19, exp_val: 0.02\n",
      "0:00:35.577783 episode 88000: avg_reward: 0.078, steps: 5 loss: 0.19, exp_val: 0.07\n",
      "0:00:35.576934 episode 88100: avg_reward: 0.050, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.480345 episode 88200: avg_reward: 0.045, steps: 5 loss: 0.17, exp_val: -0.04\n",
      "0:00:35.674766 episode 88300: avg_reward: 0.084, steps: 5 loss: 0.22, exp_val: 0.12\n",
      "0:00:35.503214 episode 88400: avg_reward: 0.014, steps: 5 loss: 0.14, exp_val: 0.06\n",
      "0:00:35.526092 episode 88500: avg_reward: 0.043, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.558983 episode 88600: avg_reward: 0.063, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.564623 episode 88700: avg_reward: 0.039, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.549687 episode 88800: avg_reward: 0.034, steps: 5 loss: 0.14, exp_val: -0.07\n",
      "0:00:35.519817 episode 88900: avg_reward: 0.046, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.554028 episode 89000: avg_reward: 0.062, steps: 5 loss: 0.12, exp_val: 0.06\n",
      "0:00:35.611838 episode 89100: avg_reward: 0.057, steps: 5 loss: 0.13, exp_val: 0.11\n",
      "0:00:35.532163 episode 89200: avg_reward: 0.076, steps: 5 loss: 0.17, exp_val: 0.02\n",
      "0:00:35.648294 episode 89300: avg_reward: 0.109, steps: 5 loss: 0.19, exp_val: 0.01\n",
      "0:00:35.636270 episode 89400: avg_reward: 0.059, steps: 5 loss: 0.14, exp_val: 0.17\n",
      "0:00:35.747534 episode 89500: avg_reward: 0.092, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.714896 episode 89600: avg_reward: 0.074, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.629112 episode 89700: avg_reward: 0.050, steps: 5 loss: 0.19, exp_val: 0.06\n",
      "0:00:35.723391 episode 89800: avg_reward: 0.067, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.614487 episode 89900: avg_reward: 0.051, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.699380 episode 90000: avg_reward: 0.082, steps: 5 loss: 0.19, exp_val: 0.23\n",
      "0:00:35.576323 episode 90100: avg_reward: 0.041, steps: 5 loss: 0.11, exp_val: 0.06\n",
      "0:00:35.560095 episode 90200: avg_reward: 0.086, steps: 5 loss: 0.16, exp_val: 0.17\n",
      "0:00:35.679406 episode 90300: avg_reward: 0.046, steps: 5 loss: 0.17, exp_val: -0.04\n",
      "0:00:35.733961 episode 90400: avg_reward: 0.031, steps: 5 loss: 0.21, exp_val: 0.01\n",
      "0:00:35.614700 episode 90500: avg_reward: 0.025, steps: 5 loss: 0.18, exp_val: 0.12\n",
      "0:00:35.576905 episode 90600: avg_reward: 0.065, steps: 5 loss: 0.13, exp_val: 0.06\n",
      "0:00:35.545845 episode 90700: avg_reward: 0.082, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.614842 episode 90800: avg_reward: 0.046, steps: 5 loss: 0.17, exp_val: 0.10\n",
      "0:00:35.563288 episode 90900: avg_reward: 0.063, steps: 5 loss: 0.16, exp_val: 0.04\n",
      "0:00:35.596924 episode 91000: avg_reward: 0.105, steps: 5 loss: 0.19, exp_val: -0.02\n",
      "0:00:35.589986 episode 91100: avg_reward: 0.061, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.622696 episode 91200: avg_reward: 0.048, steps: 5 loss: 0.17, exp_val: -0.10\n",
      "0:00:35.632513 episode 91300: avg_reward: 0.064, steps: 5 loss: 0.10, exp_val: 0.11\n",
      "0:00:35.586896 episode 91400: avg_reward: 0.021, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.487587 episode 91500: avg_reward: 0.054, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.596633 episode 91600: avg_reward: 0.070, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.641964 episode 91700: avg_reward: 0.061, steps: 5 loss: 0.20, exp_val: -0.10\n",
      "0:00:35.619032 episode 91800: avg_reward: 0.053, steps: 5 loss: 0.15, exp_val: 0.01\n",
      "0:00:35.527346 episode 91900: avg_reward: 0.030, steps: 5 loss: 0.15, exp_val: 0.04\n",
      "0:00:35.583101 episode 92000: avg_reward: 0.072, steps: 5 loss: 0.18, exp_val: 0.02\n",
      "0:00:35.550295 episode 92100: avg_reward: 0.122, steps: 5 loss: 0.12, exp_val: -0.07\n",
      "0:00:35.628796 episode 92200: avg_reward: 0.101, steps: 5 loss: 0.14, exp_val: 0.01\n",
      "0:00:35.633187 episode 92300: avg_reward: 0.080, steps: 5 loss: 0.18, exp_val: 0.15\n",
      "0:00:35.572710 episode 92400: avg_reward: 0.060, steps: 5 loss: 0.17, exp_val: -0.07\n",
      "0:00:35.517839 episode 92500: avg_reward: 0.036, steps: 5 loss: 0.10, exp_val: 0.11\n",
      "0:00:35.508552 episode 92600: avg_reward: 0.038, steps: 5 loss: 0.16, exp_val: 0.20\n",
      "0:00:35.527158 episode 92700: avg_reward: 0.042, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.743608 episode 92800: avg_reward: 0.099, steps: 5 loss: 0.19, exp_val: 0.01\n",
      "0:00:35.580181 episode 92900: avg_reward: 0.107, steps: 5 loss: 0.15, exp_val: 0.07\n",
      "0:00:35.645994 episode 93000: avg_reward: 0.018, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.640104 episode 93100: avg_reward: 0.086, steps: 5 loss: 0.16, exp_val: -0.01\n",
      "0:00:35.551003 episode 93200: avg_reward: 0.038, steps: 5 loss: 0.11, exp_val: 0.08\n",
      "0:00:35.591830 episode 93300: avg_reward: 0.010, steps: 5 loss: 0.17, exp_val: 0.04\n",
      "0:00:35.585203 episode 93400: avg_reward: 0.069, steps: 5 loss: 0.14, exp_val: -0.05\n",
      "0:00:35.518338 episode 93500: avg_reward: 0.077, steps: 5 loss: 0.11, exp_val: 0.04\n",
      "0:00:35.575555 episode 93600: avg_reward: 0.064, steps: 5 loss: 0.18, exp_val: -0.05\n",
      "0:00:35.605906 episode 93700: avg_reward: 0.057, steps: 5 loss: 0.12, exp_val: -0.10\n",
      "0:00:35.614087 episode 93800: avg_reward: 0.090, steps: 5 loss: 0.18, exp_val: -0.01\n",
      "0:00:35.564863 episode 93900: avg_reward: 0.113, steps: 5 loss: 0.20, exp_val: 0.18\n",
      "0:00:35.521157 episode 94000: avg_reward: 0.022, steps: 5 loss: 0.20, exp_val: -0.06\n",
      "0:00:35.611501 episode 94100: avg_reward: 0.053, steps: 5 loss: 0.17, exp_val: 0.09\n",
      "0:00:35.488153 episode 94200: avg_reward: 0.052, steps: 5 loss: 0.20, exp_val: -0.01\n",
      "0:00:35.500958 episode 94300: avg_reward: 0.054, steps: 5 loss: 0.14, exp_val: 0.12\n",
      "0:00:35.510559 episode 94400: avg_reward: 0.072, steps: 5 loss: 0.19, exp_val: -0.09\n",
      "0:00:35.567650 episode 94500: avg_reward: 0.049, steps: 5 loss: 0.17, exp_val: 0.17\n",
      "0:00:35.629243 episode 94600: avg_reward: 0.057, steps: 5 loss: 0.15, exp_val: -0.02\n",
      "0:00:35.621458 episode 94700: avg_reward: 0.067, steps: 5 loss: 0.14, exp_val: 0.03\n",
      "0:00:35.582818 episode 94800: avg_reward: 0.033, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.564188 episode 94900: avg_reward: -0.007, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.657829 episode 95000: avg_reward: -0.005, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.534253 episode 95100: avg_reward: 0.049, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.582635 episode 95200: avg_reward: 0.043, steps: 5 loss: 0.19, exp_val: -0.07\n",
      "0:00:35.516255 episode 95300: avg_reward: 0.026, steps: 5 loss: 0.09, exp_val: 0.03\n",
      "0:00:35.518654 episode 95400: avg_reward: 0.043, steps: 5 loss: 0.12, exp_val: 0.06\n",
      "0:00:35.696750 episode 95500: avg_reward: 0.064, steps: 5 loss: 0.19, exp_val: -0.07\n",
      "0:00:35.508825 episode 95600: avg_reward: 0.076, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.640649 episode 95700: avg_reward: 0.065, steps: 5 loss: 0.12, exp_val: 0.03\n",
      "0:00:35.597086 episode 95800: avg_reward: 0.094, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.580218 episode 95900: avg_reward: 0.093, steps: 5 loss: 0.14, exp_val: -0.00\n",
      "0:00:35.475819 episode 96000: avg_reward: 0.075, steps: 5 loss: 0.21, exp_val: 0.20\n",
      "0:00:35.574971 episode 96100: avg_reward: 0.089, steps: 5 loss: 0.18, exp_val: 0.09\n",
      "0:00:35.665654 episode 96200: avg_reward: 0.063, steps: 5 loss: 0.19, exp_val: 0.10\n",
      "0:00:35.637865 episode 96300: avg_reward: 0.049, steps: 5 loss: 0.16, exp_val: 0.14\n",
      "0:00:35.576550 episode 96400: avg_reward: 0.046, steps: 5 loss: 0.15, exp_val: 0.03\n",
      "0:00:35.594030 episode 96500: avg_reward: 0.044, steps: 5 loss: 0.14, exp_val: -0.07\n",
      "0:00:35.664524 episode 96600: avg_reward: 0.034, steps: 5 loss: 0.14, exp_val: 0.14\n",
      "0:00:35.647727 episode 96700: avg_reward: 0.119, steps: 5 loss: 0.13, exp_val: -0.05\n",
      "0:00:35.555141 episode 96800: avg_reward: 0.052, steps: 5 loss: 0.21, exp_val: 0.07\n",
      "0:00:35.579701 episode 96900: avg_reward: 0.124, steps: 5 loss: 0.13, exp_val: 0.11\n",
      "0:00:35.603469 episode 97000: avg_reward: 0.075, steps: 5 loss: 0.13, exp_val: 0.04\n",
      "0:00:35.732725 episode 97100: avg_reward: 0.084, steps: 5 loss: 0.22, exp_val: 0.07\n",
      "0:00:35.798500 episode 97200: avg_reward: 0.078, steps: 5 loss: 0.19, exp_val: -0.04\n",
      "0:00:35.534742 episode 97300: avg_reward: 0.075, steps: 5 loss: 0.20, exp_val: 0.02\n",
      "0:00:35.582460 episode 97400: avg_reward: 0.052, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.637586 episode 97500: avg_reward: 0.057, steps: 5 loss: 0.14, exp_val: 0.04\n",
      "0:00:35.603065 episode 97600: avg_reward: 0.018, steps: 5 loss: 0.14, exp_val: 0.15\n",
      "0:00:35.658206 episode 97700: avg_reward: 0.070, steps: 5 loss: 0.13, exp_val: 0.08\n",
      "0:00:35.683357 episode 97800: avg_reward: 0.124, steps: 5 loss: 0.16, exp_val: 0.09\n",
      "0:00:35.689292 episode 97900: avg_reward: 0.073, steps: 5 loss: 0.15, exp_val: -0.01\n",
      "0:00:35.740571 episode 98000: avg_reward: 0.070, steps: 5 loss: 0.15, exp_val: 0.06\n",
      "0:00:35.906622 episode 98100: avg_reward: 0.046, steps: 5 loss: 0.13, exp_val: -0.02\n",
      "0:00:35.586512 episode 98200: avg_reward: -0.007, steps: 5 loss: 0.13, exp_val: 0.01\n",
      "0:00:35.675495 episode 98300: avg_reward: 0.066, steps: 5 loss: 0.08, exp_val: 0.05\n",
      "0:00:35.685351 episode 98400: avg_reward: 0.041, steps: 5 loss: 0.16, exp_val: 0.07\n",
      "0:00:35.584099 episode 98500: avg_reward: 0.095, steps: 5 loss: 0.19, exp_val: -0.04\n",
      "0:00:35.563729 episode 98600: avg_reward: 0.058, steps: 5 loss: 0.14, exp_val: 0.07\n",
      "0:00:35.575011 episode 98700: avg_reward: 0.084, steps: 5 loss: 0.18, exp_val: -0.02\n",
      "0:00:35.778975 episode 98800: avg_reward: 0.025, steps: 5 loss: 0.14, exp_val: 0.09\n",
      "0:00:35.583617 episode 98900: avg_reward: 0.103, steps: 5 loss: 0.20, exp_val: -0.10\n",
      "0:00:35.490762 episode 99000: avg_reward: 0.078, steps: 5 loss: 0.17, exp_val: 0.03\n",
      "0:00:35.595429 episode 99100: avg_reward: 0.037, steps: 5 loss: 0.19, exp_val: -0.07\n",
      "0:00:35.557557 episode 99200: avg_reward: 0.035, steps: 5 loss: 0.21, exp_val: -0.03\n",
      "0:00:35.507039 episode 99300: avg_reward: 0.019, steps: 5 loss: 0.16, exp_val: -0.09\n",
      "0:00:35.572085 episode 99400: avg_reward: 0.003, steps: 5 loss: 0.15, exp_val: -0.09\n",
      "0:00:35.605649 episode 99500: avg_reward: 0.013, steps: 5 loss: 0.16, exp_val: 0.11\n",
      "0:00:35.620305 episode 99600: avg_reward: 0.048, steps: 5 loss: 0.19, exp_val: -0.06\n",
      "0:00:35.538844 episode 99700: avg_reward: 0.059, steps: 5 loss: 0.18, exp_val: 0.04\n",
      "0:00:35.588748 episode 99800: avg_reward: 0.111, steps: 5 loss: 0.15, exp_val: 0.12\n",
      "0:00:35.504508 episode 99900: avg_reward: 0.127, steps: 5 loss: 0.15, exp_val: 0.09\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(num_episode):\n",
    "    \n",
    "    cum_reward, steps = play_game(snake, epsilon = 0.1)\n",
    "    \n",
    "    for _ in range(4):\n",
    "        l, exp_val = optimize_model()\n",
    "    \n",
    "    avg_steps = float(steps)*0.01 + avg_steps*0.99\n",
    "    avg_reward = float(cum_reward)*0.01 + avg_reward*0.99\n",
    "    if i_episode % 100 == 0 and l is not None:\n",
    "        print('%s episode %d: avg_reward: %.3f, steps: %d loss: %.2f, exp_val: %.2f' % \n",
    "              (str(datetime.datetime.now() - start), i_episode, avg_reward, avg_steps, l.data[0], exp_val.data[0]))\n",
    "        \n",
    "        if best_reward < avg_reward and i_episode % 500 == 0:\n",
    "            print(\"saving model..\")\n",
    "            torch.save(model, \"best_model.torch\")\n",
    "            best_reward = avg_reward\n",
    "            \n",
    "        start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with greedy-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-04678a9ee3e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msnake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnakai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSnake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcum_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b9c3fa945ab9>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(snake, epsilon)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mcum_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f8e5ccb03e90>\u001b[0m in \u001b[0;36mtorch_step\u001b[0;34m(action)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'torch'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0maction_pure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mind2action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_pure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mended\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Sync/EDM/RL/snake/snakai.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Sync/EDM/RL/snake/snakai.py\u001b[0m in \u001b[0;36mon_render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display_surf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display_surf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_image_surf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display_surf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apple_surf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = torch.load(\"best_model.torch\")\n",
    "snake = snakai.Snake(render=True, game_size = game_size)\n",
    "while True:\n",
    "    cum_reward, steps = play_game(snake, epsilon = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
