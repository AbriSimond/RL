{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'right', 1: 'left', 2: 'up', 3: 'down'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import snakai\n",
    "import agents\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "action2ind = {'right' : 0,\n",
    "             'left' : 1,\n",
    "             'up' : 2,\n",
    "             'down' : 3}\n",
    "ind2action = {val: key for key, val in action2ind.items()}\n",
    "ind2action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=2, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=2, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.dense1 = nn.Linear(2048, 512)\n",
    "        self.head = nn.Linear(512, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = F.relu(self.dense1(x.view(x.size(0), -1)))\n",
    "        return (self.head(x))\n",
    "    \n",
    "model = DQN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_step(action):\n",
    "    if 'torch' in action.type():\n",
    "        action_pure = ind2action[action.numpy()[0][0]]\n",
    "        next_state, reward, ended = snake.step(action_pure)\n",
    "        next_state, reward, ended = torch.unsqueeze(torch.from_numpy(next_state),0).float(), FloatTensor([[reward]]), LongTensor([[ended]])\n",
    "        return next_state, reward, ended\n",
    "    else:\n",
    "        return snake.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_idx(a):\n",
    "    one = a == np.max(a)\n",
    "    col = np.argmax(one.max(axis=0))\n",
    "    row = np.argmax(one[:,col])\n",
    "    return row, col\n",
    "\n",
    "def simple_agent(state):\n",
    "    use_torch = 'torch' in state.type()\n",
    "    if use_torch:\n",
    "        state = state.numpy()\n",
    "        \n",
    "    y_player, x_player = max_idx(state[0,0,:,:])\n",
    "    y_apple, x_apple = max_idx(state[0,1,:,:])\n",
    "    action = \"down\"\n",
    "    if y_player < y_apple:\n",
    "        action = \"down\"\n",
    "    elif y_player > y_apple:\n",
    "        action = \"up\"\n",
    "    elif x_player < x_apple:\n",
    "        action = \"right\"\n",
    "    elif x_player > x_apple:\n",
    "        action = \"left\"\n",
    "    \n",
    "    action = action2ind[action]\n",
    "    if use_torch:\n",
    "        return LongTensor([[action]])\n",
    "    else: \n",
    "        return action\n",
    "    \n",
    "def random_agent(state):\n",
    "    return LongTensor([[random.randrange(4)]])\n",
    "\n",
    "def model_agent(state):\n",
    "    return model(Variable(state)).data.max(1)[1].view(1, 1)\n",
    "\n",
    "def epsilon_agent(state, th = 0.05):\n",
    "    if random.random() > th:\n",
    "        return model_agent(state)\n",
    "    else:\n",
    "        return random_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "GAMMA = 0.98\n",
    "optimizer = optim.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.001)\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "    \n",
    "    if len(memory) < batch_size:\n",
    "        return None, 0\n",
    "\n",
    "    # fetch and concat batch:\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    state_batch = Variable(torch.cat(batch.state))\n",
    "    action_batch = Variable(torch.cat(batch.action))\n",
    "    reward_batch = Variable(torch.cat(batch.reward))\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = model(state_batch).gather(1, action_batch)\n",
    "\n",
    "    ended_batch = torch.cat(batch.ended)\n",
    "    non_final_mask = ByteTensor(1 - ended_batch.numpy())\n",
    "    \n",
    "    non_final_next_states = Variable(torch.cat(\n",
    "    [state for end, state in zip(ended_batch.numpy().flatten(), batch.next_state) if end !=1])\n",
    "                                     ,volatile=True)\n",
    "    \n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = Variable(torch.zeros(batch_size).type(Tensor))\n",
    "    next_state_values[non_final_mask] = model(non_final_next_states).max(1)[0]\n",
    "\n",
    "    # Now, we don't want to mess up the loss with a volatile flag, so let's\n",
    "    # clear it. After this, we'll just end up with a Variable that has\n",
    "    # requires_grad=False\n",
    "    next_state_values.volatile = False\n",
    "    \n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch.view(batch_size).float()\n",
    "    #print(reward_batch)\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss, expected_state_action_values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(snake, epsilon = 0.05):\n",
    "    cum_reward = 0.0\n",
    "    snake.on_init()\n",
    "    state, reward, ended = snake.on_feedback()\n",
    "    state = torch.unsqueeze(torch.from_numpy(state),0).float()\n",
    "    for i in range(ep_length):\n",
    "        action = epsilon_agent(state, th = epsilon)\n",
    "        next_state, reward, ended = torch_step(action)\n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        if ended.numpy()[0][0] == 1:\n",
    "            return cum_reward, i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.now()\n",
    "game_size = (10,10)\n",
    "snake = snakai.Snake(render=False, game_size = game_size)\n",
    "\n",
    "ep_length = 10000\n",
    "num_episode = 100000\n",
    "avg_reward = -1.0\n",
    "avg_steps = 1.0\n",
    "best_reward = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:01:28.793285 episode 100: avg_reward: -0.506, steps: 3 loss: 0.01, exp_val: -0.72\n",
      "0:01:54.062175 episode 200: avg_reward: -0.375, steps: 13 loss: 0.00, exp_val: -0.54\n",
      "0:02:24.694718 episode 300: avg_reward: -0.421, steps: 20 loss: 0.01, exp_val: -0.30\n",
      "saving model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/serialization.py:159: UserWarning: Couldn't retrieve source code for container of type DQN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:01.917649 episode 400: avg_reward: -0.276, steps: 18 loss: 0.00, exp_val: -0.51\n",
      "0:04:18.795457 episode 500: avg_reward: -0.386, steps: 12 loss: 2.39, exp_val: 40.87\n",
      "0:05:13.341113 episode 600: avg_reward: -0.212, steps: 10 loss: 0.04, exp_val: 1.03\n",
      "saving model..\n",
      "0:05:25.721095 episode 700: avg_reward: -0.253, steps: 14 loss: 0.00, exp_val: -0.33\n",
      "0:05:28.150257 episode 800: avg_reward: -0.257, steps: 12 loss: 0.16, exp_val: 0.72\n",
      "0:05:17.507644 episode 900: avg_reward: -0.136, steps: 8 loss: 0.01, exp_val: -0.29\n",
      "saving model..\n",
      "0:05:36.004054 episode 1000: avg_reward: -0.299, steps: 18 loss: 0.02, exp_val: -0.17\n",
      "0:05:28.011849 episode 1100: avg_reward: -0.416, steps: 21 loss: 4.01, exp_val: 376.15\n",
      "0:05:38.976575 episode 1200: avg_reward: -0.395, steps: 10 loss: 0.06, exp_val: -0.00\n",
      "0:05:38.948302 episode 1300: avg_reward: -0.224, steps: 9 loss: 0.01, exp_val: -0.18\n",
      "0:05:35.665908 episode 1400: avg_reward: -0.177, steps: 9 loss: 0.01, exp_val: -0.53\n",
      "0:05:45.218160 episode 1500: avg_reward: -0.321, steps: 17 loss: 0.03, exp_val: 0.48\n",
      "0:05:37.233068 episode 1600: avg_reward: -0.423, steps: 26 loss: 0.00, exp_val: -0.36\n",
      "0:05:46.072210 episode 1700: avg_reward: -0.384, steps: 19 loss: 0.22, exp_val: 1.93\n",
      "0:06:02.148923 episode 1800: avg_reward: -0.215, steps: 11 loss: 0.01, exp_val: -0.27\n",
      "0:06:08.206839 episode 1900: avg_reward: -0.356, steps: 17 loss: 0.17, exp_val: 2.14\n",
      "0:06:00.488485 episode 2000: avg_reward: -0.435, steps: 23 loss: 0.01, exp_val: -0.47\n",
      "0:05:54.551138 episode 2100: avg_reward: -0.458, steps: 26 loss: 0.00, exp_val: -0.78\n",
      "0:06:01.953020 episode 2200: avg_reward: -0.445, steps: 17 loss: 0.03, exp_val: -0.08\n",
      "0:06:25.899830 episode 2300: avg_reward: -0.327, steps: 17 loss: 0.00, exp_val: -0.63\n",
      "0:06:05.767487 episode 2400: avg_reward: -0.420, steps: 20 loss: 0.08, exp_val: 2.60\n",
      "0:06:02.673904 episode 2500: avg_reward: -0.517, steps: 26 loss: 0.03, exp_val: -0.23\n",
      "0:06:29.803683 episode 2600: avg_reward: -0.552, steps: 23 loss: 0.09, exp_val: 1.17\n",
      "0:06:34.528016 episode 2700: avg_reward: -0.591, steps: 32 loss: 0.02, exp_val: 0.07\n",
      "0:06:27.716728 episode 2800: avg_reward: -0.468, steps: 28 loss: 0.01, exp_val: -0.57\n",
      "0:05:51.693238 episode 2900: avg_reward: -0.386, steps: 23 loss: 0.00, exp_val: -0.69\n",
      "0:05:02.455245 episode 3000: avg_reward: -0.253, steps: 19 loss: 0.00, exp_val: -0.64\n",
      "0:04:40.237880 episode 3100: avg_reward: -0.308, steps: 21 loss: 0.00, exp_val: -0.69\n",
      "0:04:34.353780 episode 3200: avg_reward: -0.298, steps: 22 loss: 0.00, exp_val: -0.64\n",
      "0:04:12.139179 episode 3300: avg_reward: -0.311, steps: 21 loss: 0.01, exp_val: -0.53\n",
      "0:04:07.452811 episode 3400: avg_reward: -0.226, steps: 18 loss: 0.00, exp_val: -0.49\n",
      "0:04:01.775633 episode 3500: avg_reward: -0.270, steps: 22 loss: 0.00, exp_val: -0.68\n",
      "0:03:54.510895 episode 3600: avg_reward: -0.218, steps: 21 loss: 0.01, exp_val: -0.60\n",
      "0:03:55.400836 episode 3700: avg_reward: -0.277, steps: 24 loss: 0.01, exp_val: -0.27\n",
      "0:03:59.534297 episode 3800: avg_reward: -0.317, steps: 24 loss: 0.01, exp_val: -0.43\n",
      "0:03:58.332176 episode 3900: avg_reward: -0.241, steps: 27 loss: 0.01, exp_val: -0.13\n",
      "0:03:55.113123 episode 4000: avg_reward: -0.273, steps: 15 loss: 0.01, exp_val: -0.25\n",
      "0:04:03.982443 episode 4100: avg_reward: -0.379, steps: 22 loss: 0.03, exp_val: 0.11\n",
      "0:03:51.625315 episode 4200: avg_reward: -0.221, steps: 18 loss: 0.00, exp_val: -0.76\n",
      "0:03:52.856777 episode 4300: avg_reward: -0.223, steps: 15 loss: 0.00, exp_val: -0.72\n",
      "0:03:52.351243 episode 4400: avg_reward: -0.221, steps: 18 loss: 0.01, exp_val: -0.39\n",
      "0:04:09.365156 episode 4500: avg_reward: -0.456, steps: 17 loss: 459.18, exp_val: 3981.34\n",
      "0:04:41.312217 episode 4600: avg_reward: -0.785, steps: 6 loss: 5971.88, exp_val: 36508.87\n",
      "0:04:33.441472 episode 4700: avg_reward: -0.918, steps: 2 loss: 6998.44, exp_val: 86511.38\n",
      "0:04:30.811294 episode 4800: avg_reward: -0.971, steps: 0 loss: 8964.93, exp_val: 70153.09\n",
      "0:04:29.326285 episode 4900: avg_reward: -0.991, steps: 0 loss: 12201.23, exp_val: 67435.61\n",
      "0:04:26.003408 episode 5000: avg_reward: -0.990, steps: 0 loss: 13749.81, exp_val: 82434.33\n",
      "0:04:24.342072 episode 5100: avg_reward: -0.988, steps: 0 loss: 8118.17, exp_val: 84369.85\n",
      "0:04:19.864560 episode 5200: avg_reward: -0.597, steps: 2 loss: 450.57, exp_val: 1656.27\n",
      "0:04:00.927933 episode 5300: avg_reward: -0.530, steps: 3 loss: 73.96, exp_val: 260.50\n",
      "0:03:51.330095 episode 5400: avg_reward: -0.420, steps: 3 loss: 389.50, exp_val: 1679.17\n",
      "0:03:48.215292 episode 5500: avg_reward: -0.241, steps: 3 loss: 24.55, exp_val: 85.74\n",
      "0:03:43.318150 episode 5600: avg_reward: -0.123, steps: 4 loss: 32.44, exp_val: 63.68\n",
      "0:03:45.701930 episode 5700: avg_reward: -0.173, steps: 4 loss: 149.58, exp_val: 359.04\n",
      "0:03:45.211677 episode 5800: avg_reward: -0.452, steps: 4 loss: 11.58, exp_val: 38.83\n",
      "0:03:50.274057 episode 5900: avg_reward: -0.525, steps: 4 loss: 457.71, exp_val: 1191.65\n",
      "0:03:57.726409 episode 6000: avg_reward: -0.715, steps: 5 loss: 23.26, exp_val: 81.56\n",
      "0:03:55.472372 episode 6100: avg_reward: -0.650, steps: 5 loss: 55.93, exp_val: 313.76\n",
      "0:03:49.665755 episode 6200: avg_reward: -0.408, steps: 5 loss: 8.24, exp_val: -4.32\n",
      "0:03:51.181874 episode 6300: avg_reward: -0.332, steps: 5 loss: 2.29, exp_val: 3.17\n",
      "0:03:51.715112 episode 6400: avg_reward: -0.303, steps: 5 loss: 9.26, exp_val: 48.57\n",
      "0:03:54.251303 episode 6500: avg_reward: -0.208, steps: 6 loss: 3.75, exp_val: -1.74\n",
      "0:04:01.842981 episode 6600: avg_reward: -0.126, steps: 5 loss: 2.77, exp_val: 6.64\n",
      "saving model..\n",
      "0:04:07.487924 episode 6700: avg_reward: -0.121, steps: 6 loss: 2.13, exp_val: 8.52\n",
      "0:04:07.952110 episode 6800: avg_reward: -0.176, steps: 5 loss: 2.65, exp_val: -6.85\n",
      "0:04:07.068086 episode 6900: avg_reward: -0.158, steps: 6 loss: 1.29, exp_val: -1.14\n",
      "0:04:08.631627 episode 7000: avg_reward: -0.190, steps: 6 loss: 1.44, exp_val: 4.76\n",
      "0:04:09.439907 episode 7100: avg_reward: -0.134, steps: 6 loss: 1.65, exp_val: 5.18\n",
      "0:04:07.113086 episode 7200: avg_reward: -0.207, steps: 6 loss: 4.23, exp_val: 22.15\n",
      "0:04:07.162207 episode 7300: avg_reward: -0.201, steps: 6 loss: 2.84, exp_val: 10.40\n",
      "0:04:07.198331 episode 7400: avg_reward: -0.260, steps: 6 loss: 2.88, exp_val: 20.56\n",
      "0:04:07.545027 episode 7500: avg_reward: -0.334, steps: 6 loss: 0.87, exp_val: 3.87\n",
      "0:04:07.358968 episode 7600: avg_reward: -0.319, steps: 6 loss: 2.22, exp_val: 10.60\n",
      "0:04:08.093953 episode 7700: avg_reward: -0.308, steps: 7 loss: 2.93, exp_val: 15.42\n",
      "0:04:10.492389 episode 7800: avg_reward: -0.256, steps: 7 loss: 2.08, exp_val: 10.57\n",
      "0:04:10.881830 episode 7900: avg_reward: -0.221, steps: 7 loss: 3.05, exp_val: 7.64\n",
      "0:04:11.634196 episode 8000: avg_reward: -0.264, steps: 8 loss: 3.10, exp_val: 19.48\n",
      "0:04:12.000714 episode 8100: avg_reward: -0.209, steps: 8 loss: 3.00, exp_val: 11.34\n",
      "0:04:14.107689 episode 8200: avg_reward: -0.155, steps: 8 loss: 2.25, exp_val: 24.19\n",
      "0:04:11.255459 episode 8300: avg_reward: -0.170, steps: 7 loss: 1.46, exp_val: 6.29\n",
      "0:04:11.943991 episode 8400: avg_reward: -0.180, steps: 8 loss: 1.22, exp_val: 2.97\n",
      "0:04:12.367302 episode 8500: avg_reward: -0.149, steps: 8 loss: 1.60, exp_val: 17.45\n",
      "0:04:08.541286 episode 8600: avg_reward: -0.300, steps: 10 loss: 1.64, exp_val: 13.34\n",
      "0:04:08.439036 episode 8700: avg_reward: -0.214, steps: 10 loss: 1.33, exp_val: 7.16\n",
      "0:04:08.606745 episode 8800: avg_reward: -0.156, steps: 9 loss: 1.25, exp_val: 7.99\n",
      "0:04:08.876800 episode 8900: avg_reward: -0.197, steps: 12 loss: 1.74, exp_val: 10.35\n",
      "0:04:10.577536 episode 9000: avg_reward: -0.152, steps: 10 loss: 1.54, exp_val: 10.39\n",
      "0:04:07.182377 episode 9100: avg_reward: -0.106, steps: 8 loss: 0.70, exp_val: 3.61\n",
      "0:04:08.506872 episode 9200: avg_reward: -0.142, steps: 8 loss: 1.50, exp_val: 3.79\n",
      "0:04:10.146019 episode 9300: avg_reward: -0.143, steps: 10 loss: 0.78, exp_val: 7.11\n",
      "0:04:10.665866 episode 9400: avg_reward: -0.125, steps: 11 loss: 2.25, exp_val: 12.61\n",
      "0:04:11.452840 episode 9500: avg_reward: -0.214, steps: 13 loss: 1.60, exp_val: 11.01\n",
      "0:04:13.615029 episode 9600: avg_reward: -0.328, steps: 15 loss: 0.83, exp_val: 4.73\n",
      "0:04:13.843869 episode 9700: avg_reward: -0.307, steps: 17 loss: 1.00, exp_val: 6.91\n",
      "0:04:14.721624 episode 9800: avg_reward: -0.313, steps: 17 loss: 2.66, exp_val: 27.70\n",
      "0:04:14.597254 episode 9900: avg_reward: -0.335, steps: 19 loss: 1.80, exp_val: 13.10\n",
      "0:04:15.334529 episode 10000: avg_reward: -0.312, steps: 19 loss: 1.22, exp_val: 11.01\n",
      "0:04:15.562084 episode 10100: avg_reward: -0.289, steps: 19 loss: 1.25, exp_val: 11.74\n",
      "0:04:18.856918 episode 10200: avg_reward: -0.391, steps: 25 loss: 1.38, exp_val: 14.28\n",
      "0:04:19.984056 episode 10300: avg_reward: -0.433, steps: 28 loss: 1.48, exp_val: 17.01\n",
      "0:04:18.390416 episode 10400: avg_reward: -0.436, steps: 27 loss: 0.99, exp_val: 12.88\n",
      "0:04:17.764108 episode 10500: avg_reward: -0.343, steps: 25 loss: 0.70, exp_val: 6.74\n",
      "0:04:17.190832 episode 10600: avg_reward: -0.402, steps: 27 loss: 0.09, exp_val: 3.57\n",
      "0:04:16.301954 episode 10700: avg_reward: -0.456, steps: 26 loss: 0.22, exp_val: 5.52\n",
      "0:04:14.618183 episode 10800: avg_reward: -0.537, steps: 25 loss: 0.15, exp_val: 4.92\n",
      "0:04:11.211392 episode 10900: avg_reward: -0.603, steps: 33 loss: 0.12, exp_val: 5.82\n",
      "0:04:12.244840 episode 11000: avg_reward: -0.608, steps: 36 loss: 0.13, exp_val: 5.25\n",
      "0:04:11.933711 episode 11100: avg_reward: -0.575, steps: 36 loss: 0.19, exp_val: 5.97\n",
      "0:04:11.726118 episode 11200: avg_reward: -0.637, steps: 37 loss: 0.02, exp_val: 1.96\n",
      "0:04:03.362006 episode 11300: avg_reward: -0.651, steps: 38 loss: 0.03, exp_val: 0.70\n",
      "0:04:06.233916 episode 11400: avg_reward: -0.787, steps: 47 loss: 0.05, exp_val: 1.45\n",
      "0:03:55.858548 episode 11500: avg_reward: -0.545, steps: 40 loss: 0.04, exp_val: 1.35\n",
      "0:03:58.118926 episode 11600: avg_reward: -0.703, steps: 45 loss: 0.01, exp_val: 0.85\n",
      "0:03:55.421033 episode 11700: avg_reward: -0.708, steps: 46 loss: 0.00, exp_val: 0.27\n",
      "0:03:56.304265 episode 11800: avg_reward: -0.554, steps: 41 loss: 0.02, exp_val: 0.60\n",
      "0:04:06.154855 episode 11900: avg_reward: -0.478, steps: 36 loss: 0.01, exp_val: 0.88\n",
      "0:04:00.976577 episode 12000: avg_reward: -0.418, steps: 37 loss: 0.02, exp_val: 0.59\n",
      "0:04:01.273691 episode 12100: avg_reward: -0.388, steps: 35 loss: 0.03, exp_val: 0.53\n",
      "0:03:58.008541 episode 12200: avg_reward: -0.265, steps: 30 loss: 0.01, exp_val: 0.65\n",
      "0:04:04.471764 episode 12300: avg_reward: -0.240, steps: 36 loss: 0.01, exp_val: 1.15\n",
      "0:03:59.110757 episode 12400: avg_reward: -0.231, steps: 37 loss: 0.05, exp_val: 0.62\n",
      "0:04:00.966435 episode 12500: avg_reward: -0.105, steps: 37 loss: 0.02, exp_val: 0.69\n",
      "0:04:07.519119 episode 12600: avg_reward: -0.205, steps: 34 loss: 0.08, exp_val: 1.67\n",
      "0:04:07.160340 episode 12700: avg_reward: -0.070, steps: 35 loss: 0.02, exp_val: 1.32\n",
      "0:04:04.601338 episode 12800: avg_reward: 0.091, steps: 36 loss: 0.02, exp_val: 1.54\n",
      "0:04:08.496227 episode 12900: avg_reward: 0.058, steps: 36 loss: 0.02, exp_val: 1.73\n",
      "saving model..\n",
      "0:04:18.756021 episode 13000: avg_reward: -0.036, steps: 38 loss: 0.04, exp_val: 1.63\n",
      "0:04:08.581744 episode 13100: avg_reward: -0.075, steps: 38 loss: 0.03, exp_val: 2.13\n",
      "0:04:10.308496 episode 13200: avg_reward: -0.066, steps: 41 loss: 0.01, exp_val: 2.31\n",
      "0:04:07.735401 episode 13300: avg_reward: 0.097, steps: 41 loss: 0.03, exp_val: 1.48\n",
      "0:04:06.975015 episode 13400: avg_reward: 0.064, steps: 39 loss: 0.01, exp_val: 1.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-28b533ca34c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mavg_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.01\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mavg_steps\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-e354c226607d>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# Optimize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i_episode in range(num_episode):\n",
    "    \n",
    "    cum_reward, steps = play_game(snake, epsilon = 0.05)\n",
    "    \n",
    "    for _ in range(5):\n",
    "        l, exp_val = optimize_model()\n",
    "    \n",
    "    avg_steps = float(steps)*0.01 + avg_steps*0.99\n",
    "    avg_reward = float(cum_reward)*0.01 + avg_reward*0.99\n",
    "    if i_episode % 100 == 0 and l is not None:\n",
    "        print('%s episode %d: avg_reward: %.3f, steps: %d loss: %.2f, exp_val: %.2f' % \n",
    "              (str(datetime.datetime.now() - start), i_episode, avg_reward, avg_steps, l.data[0], exp_val.data[0]))\n",
    "        \n",
    "        if best_reward < avg_reward and i_episode % 300 == 0:\n",
    "            print(\"saving model..\")\n",
    "            torch.save(model, \"best_model.torch\")\n",
    "            best_reward = avg_reward\n",
    "            \n",
    "        start = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with greedy-policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-70e0d457927a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msnake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnakai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSnake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mcum_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b9c3fa945ab9>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(snake, epsilon)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mcum_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-518835a86b81>\u001b[0m in \u001b[0;36mtorch_step\u001b[0;34m(action)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'torch'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0maction_pure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mind2action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msnake\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_pure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mended\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Sync/EDM/snake/snakai.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mfeedback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Sync/EDM/snake/snakai.py\u001b[0m in \u001b[0;36mon_render\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapple\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display_surf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apple_surf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display_surf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wall_surf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0mpygame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = torch.load(\"best_model.torch\")\n",
    "snake = snakai.Snake(render=True, game_size = game_size)\n",
    "while True:\n",
    "    cum_reward, steps = play_game(snake, epsilon = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
