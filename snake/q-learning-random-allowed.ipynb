{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/finn/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/finn/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install pygame -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snakai\n",
    "import agents\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n",
    "cpu = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:1\")\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how to play game and replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_size = (10,10)\n",
    "def tuple_to_torch(tup):\n",
    "    return torch.from_numpy(np.array(tup))\n",
    "\n",
    "action2ind = {'right' : 0,\n",
    "             'left' : 1,\n",
    "             'up' : 2,\n",
    "             'down' : 3}\n",
    "ind2action = {val: key for key, val in action2ind.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(snake, agent, epsilon = 0.05):\n",
    "    cum_reward = 0.0\n",
    "    snake.on_init()\n",
    "    state, reward, ended = snake.on_feedback()\n",
    "\n",
    "    for i in range(200):\n",
    "        action = agent(state, th = epsilon)\n",
    "        next_state, reward, ended = snake.step(action)\n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        # Keep all the games:\n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        if ended == 1:\n",
    "            return cum_reward, i\n",
    "    return cum_reward, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 64\n",
    "ksize = 4\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=ksize, stride=2, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 0)\n",
    "        #self.dense1 = nn.Linear(2592, 1024)\n",
    "        self.head = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.relu(self.dense1(x))\n",
    "        return 2*F.tanh(self.head(x))\n",
    "    \n",
    "model = DQN().to(device)\n",
    "batch_size = 32\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001) # , weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch():\n",
    "    if len(memory) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    # GET SAMPLE OF DATA\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = tuple_to_torch(batch.state).float().to(device)\n",
    "    next_state_batch = tuple_to_torch(batch.next_state).float().to(device)\n",
    "    action_batch = tuple_to_torch(list(action2ind[a] for a in batch.action)).to(device)\n",
    "    reward_batch = tuple_to_torch(batch.reward).float().to(device)\n",
    "\n",
    "\n",
    "    ## Calculate expected reward:\n",
    "    GAMMA = 0.99\n",
    "    with torch.set_grad_enabled(False):\n",
    "        not_ended_batch = 1 -torch.ByteTensor(batch.ended)\n",
    "        next_states_non_final = next_state_batch[not_ended_batch]\n",
    "        next_state_values = torch.zeros(batch_size).to(device)\n",
    "        reward_hat = model(next_states_non_final)\n",
    "        next_state_values[not_ended_batch] = reward_hat.max(1)[0]\n",
    "        expected_state_action_values = next_state_values*GAMMA + reward_batch\n",
    "\n",
    "\n",
    "    # Predict value function:\n",
    "    yhat = model(state_batch)\n",
    "    state_action_values = yhat.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import random_legal_move\n",
    "def deep_agent(state, th):\n",
    "    \n",
    "    if random.random() < th:\n",
    "        return random_legal_move(state)\n",
    "    \n",
    "    state = torch.unsqueeze(torch.from_numpy(state),0).float().to(device)\n",
    "    yhat = model(state)\n",
    "    action = [ind2action[a] for a in yhat.argmax(1).data.cpu().numpy()]\n",
    "    if len(action) > 1:\n",
    "        raise Exception\n",
    "    action = action[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "\n",
    "# Warmup memory:\n",
    "for _ in range(10):\n",
    "    play_game(snake, deep_agent, epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(n = 100, epsilon = 0.05):\n",
    "    rewards = np.zeros(n)\n",
    "    for ep in range(n):\n",
    "        rewards[ep],i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "        \n",
    "    return np.mean(rewards)\n",
    "\n",
    "def save_checkpoint():\n",
    "    filename = \"models/snake_legalexp_ep:%02d-reward:%.2f.pth\" %( ep, eval_reward)\n",
    "    torch.save(model.cpu().state_dict(), filename)\n",
    "    model.to(device)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-28 14:07:17.476435: ep: 0 \t reward: -0.200 \t loss: 0.0875 \t game len: 21.0 \t epsilon: 0.90\n",
      "2018-05-28 14:07:17.939320: ep: 0 \t Reward evaluation: -0.93\n",
      "2018-05-28 14:07:36.117576: ep: 100 \t reward: -0.642 \t loss: 0.0485 \t game len: 44.0 \t epsilon: 0.90\n",
      "2018-05-28 14:07:53.137246: ep: 200 \t reward: -0.792 \t loss: 0.0477 \t game len: 41.8 \t epsilon: 0.89\n",
      "2018-05-28 14:08:14.220083: ep: 300 \t reward: -0.629 \t loss: 0.0448 \t game len: 53.7 \t epsilon: 0.89\n",
      "2018-05-28 14:08:50.500148: ep: 400 \t reward: -0.678 \t loss: 0.0244 \t game len: 89.8 \t epsilon: 0.88\n",
      "2018-05-28 14:09:25.064425: ep: 500 \t reward: -0.515 \t loss: 0.0200 \t game len: 85.8 \t epsilon: 0.88\n",
      "2018-05-28 14:10:09.838024: ep: 600 \t reward: -0.273 \t loss: 0.0149 \t game len: 112.7 \t epsilon: 0.87\n",
      "2018-05-28 14:10:59.776904: ep: 700 \t reward: 0.005 \t loss: 0.0099 \t game len: 126.1 \t epsilon: 0.86\n",
      "2018-05-28 14:11:57.596790: ep: 800 \t reward: 0.306 \t loss: 0.0083 \t game len: 144.5 \t epsilon: 0.86\n",
      "2018-05-28 14:12:51.659080: ep: 900 \t reward: 0.418 \t loss: 0.0087 \t game len: 136.3 \t epsilon: 0.85\n",
      "2018-05-28 14:13:45.349640: ep: 1000 \t reward: 0.822 \t loss: 0.0094 \t game len: 137.5 \t epsilon: 0.85\n",
      "2018-05-28 14:14:40.801196: ep: 1100 \t reward: 0.955 \t loss: 0.0092 \t game len: 143.1 \t epsilon: 0.84\n",
      "2018-05-28 14:15:32.462248: ep: 1200 \t reward: 1.184 \t loss: 0.0111 \t game len: 134.7 \t epsilon: 0.84\n",
      "2018-05-28 14:16:25.816028: ep: 1300 \t reward: 1.223 \t loss: 0.0110 \t game len: 137.7 \t epsilon: 0.83\n",
      "2018-05-28 14:17:22.491011: ep: 1400 \t reward: 1.156 \t loss: 0.0096 \t game len: 145.3 \t epsilon: 0.83\n",
      "2018-05-28 14:18:14.926203: ep: 1500 \t reward: 1.204 \t loss: 0.0109 \t game len: 133.5 \t epsilon: 0.83\n",
      "2018-05-28 14:19:11.491046: ep: 1600 \t reward: 1.624 \t loss: 0.0115 \t game len: 144.1 \t epsilon: 0.82\n",
      "2018-05-28 14:20:06.144992: ep: 1700 \t reward: 1.296 \t loss: 0.0101 \t game len: 142.2 \t epsilon: 0.82\n",
      "2018-05-28 14:21:03.983581: ep: 1800 \t reward: 1.644 \t loss: 0.0107 \t game len: 140.9 \t epsilon: 0.81\n",
      "2018-05-28 14:21:59.207586: ep: 1900 \t reward: 1.352 \t loss: 0.0104 \t game len: 135.9 \t epsilon: 0.81\n",
      "2018-05-28 14:22:57.388269: ep: 2000 \t reward: 1.645 \t loss: 0.0116 \t game len: 147.9 \t epsilon: 0.80\n",
      "2018-05-28 14:23:02.216135: ep: 2000 \t Reward evaluation: 5.57\n",
      "2018-05-28 14:24:00.321173: ep: 2100 \t reward: 1.511 \t loss: 0.0168 \t game len: 148.0 \t epsilon: 0.80\n",
      "2018-05-28 14:24:54.900777: ep: 2200 \t reward: 1.973 \t loss: 0.0123 \t game len: 138.5 \t epsilon: 0.79\n",
      "2018-05-28 14:25:51.631661: ep: 2300 \t reward: 1.446 \t loss: 0.0113 \t game len: 140.4 \t epsilon: 0.79\n",
      "2018-05-28 14:26:49.180392: ep: 2400 \t reward: 1.803 \t loss: 0.0126 \t game len: 144.3 \t epsilon: 0.78\n",
      "2018-05-28 14:27:42.191554: ep: 2500 \t reward: 1.660 \t loss: 0.0121 \t game len: 132.3 \t epsilon: 0.78\n",
      "2018-05-28 14:28:38.868590: ep: 2600 \t reward: 1.755 \t loss: 0.0123 \t game len: 143.0 \t epsilon: 0.77\n",
      "2018-05-28 14:29:36.249047: ep: 2700 \t reward: 1.640 \t loss: 0.0115 \t game len: 142.4 \t epsilon: 0.77\n",
      "2018-05-28 14:30:30.038192: ep: 2800 \t reward: 2.262 \t loss: 0.0140 \t game len: 134.9 \t epsilon: 0.76\n",
      "2018-05-28 14:31:22.436142: ep: 2900 \t reward: 1.858 \t loss: 0.0135 \t game len: 134.9 \t epsilon: 0.76\n",
      "2018-05-28 14:32:17.074308: ep: 3000 \t reward: 1.528 \t loss: 0.0120 \t game len: 134.5 \t epsilon: 0.75\n",
      "2018-05-28 14:33:07.933714: ep: 3100 \t reward: 1.651 \t loss: 0.0126 \t game len: 127.2 \t epsilon: 0.74\n",
      "2018-05-28 14:34:03.154427: ep: 3200 \t reward: 1.593 \t loss: 0.0114 \t game len: 137.0 \t epsilon: 0.74\n",
      "2018-05-28 14:35:01.917934: ep: 3300 \t reward: 2.142 \t loss: 0.0135 \t game len: 139.6 \t epsilon: 0.73\n",
      "2018-05-28 14:36:01.926619: ep: 3400 \t reward: 1.825 \t loss: 0.0121 \t game len: 143.1 \t epsilon: 0.73\n",
      "2018-05-28 14:36:55.996672: ep: 3500 \t reward: 1.711 \t loss: 0.0116 \t game len: 134.3 \t epsilon: 0.72\n",
      "2018-05-28 14:37:50.741491: ep: 3600 \t reward: 2.033 \t loss: 0.0137 \t game len: 136.4 \t epsilon: 0.72\n",
      "2018-05-28 14:38:43.597498: ep: 3700 \t reward: 1.647 \t loss: 0.0127 \t game len: 131.7 \t epsilon: 0.72\n",
      "2018-05-28 14:39:38.105846: ep: 3800 \t reward: 2.123 \t loss: 0.0138 \t game len: 134.7 \t epsilon: 0.71\n",
      "2018-05-28 14:40:33.237573: ep: 3900 \t reward: 2.521 \t loss: 0.0139 \t game len: 138.1 \t epsilon: 0.71\n",
      "2018-05-28 14:41:25.856006: ep: 4000 \t reward: 2.101 \t loss: 0.0145 \t game len: 132.8 \t epsilon: 0.70\n",
      "2018-05-28 14:41:33.616871: ep: 4000 \t Reward evaluation: 6.68\n",
      "2018-05-28 14:42:30.401028: ep: 4100 \t reward: 2.298 \t loss: 0.0171 \t game len: 144.2 \t epsilon: 0.70\n",
      "2018-05-28 14:43:23.022537: ep: 4200 \t reward: 2.506 \t loss: 0.0138 \t game len: 132.6 \t epsilon: 0.69\n",
      "2018-05-28 14:44:14.093149: ep: 4300 \t reward: 2.093 \t loss: 0.0153 \t game len: 127.6 \t epsilon: 0.69\n",
      "2018-05-28 14:45:07.418142: ep: 4400 \t reward: 2.631 \t loss: 0.0154 \t game len: 128.3 \t epsilon: 0.68\n",
      "2018-05-28 14:46:02.888077: ep: 4500 \t reward: 2.372 \t loss: 0.0157 \t game len: 134.0 \t epsilon: 0.68\n",
      "2018-05-28 14:46:55.078743: ep: 4600 \t reward: 1.994 \t loss: 0.0144 \t game len: 129.3 \t epsilon: 0.67\n",
      "2018-05-28 14:47:47.603573: ep: 4700 \t reward: 2.077 \t loss: 0.0151 \t game len: 129.2 \t epsilon: 0.67\n",
      "2018-05-28 14:48:42.680198: ep: 4800 \t reward: 2.327 \t loss: 0.0142 \t game len: 136.4 \t epsilon: 0.66\n",
      "2018-05-28 14:49:33.782708: ep: 4900 \t reward: 2.370 \t loss: 0.0154 \t game len: 126.0 \t epsilon: 0.66\n",
      "2018-05-28 14:50:26.705806: ep: 5000 \t reward: 2.617 \t loss: 0.0159 \t game len: 129.7 \t epsilon: 0.65\n",
      "2018-05-28 14:51:22.965867: ep: 5100 \t reward: 2.657 \t loss: 0.0160 \t game len: 134.7 \t epsilon: 0.65\n",
      "2018-05-28 14:52:15.197315: ep: 5200 \t reward: 2.455 \t loss: 0.0152 \t game len: 128.7 \t epsilon: 0.64\n",
      "2018-05-28 14:53:10.909727: ep: 5300 \t reward: 2.474 \t loss: 0.0160 \t game len: 133.9 \t epsilon: 0.64\n",
      "2018-05-28 14:54:03.998353: ep: 5400 \t reward: 2.298 \t loss: 0.0153 \t game len: 129.3 \t epsilon: 0.63\n",
      "2018-05-28 14:54:58.685738: ep: 5500 \t reward: 2.793 \t loss: 0.0167 \t game len: 133.2 \t epsilon: 0.62\n",
      "2018-05-28 14:55:52.293782: ep: 5600 \t reward: 2.466 \t loss: 0.0154 \t game len: 132.6 \t epsilon: 0.62\n",
      "2018-05-28 14:56:45.853279: ep: 5700 \t reward: 2.653 \t loss: 0.0155 \t game len: 131.2 \t epsilon: 0.61\n",
      "2018-05-28 14:57:42.217328: ep: 5800 \t reward: 3.038 \t loss: 0.0169 \t game len: 136.1 \t epsilon: 0.61\n",
      "2018-05-28 14:58:37.510459: ep: 5900 \t reward: 2.394 \t loss: 0.0159 \t game len: 129.9 \t epsilon: 0.60\n",
      "2018-05-28 14:59:31.150764: ep: 6000 \t reward: 2.820 \t loss: 0.0167 \t game len: 126.7 \t epsilon: 0.60\n",
      "2018-05-28 14:59:41.831347: ep: 6000 \t Reward evaluation: 3.38\n",
      "2018-05-28 15:00:37.188487: ep: 6100 \t reward: 2.991 \t loss: 0.0161 \t game len: 132.6 \t epsilon: 0.59\n",
      "2018-05-28 15:01:29.416679: ep: 6200 \t reward: 2.894 \t loss: 0.0181 \t game len: 124.2 \t epsilon: 0.59\n",
      "2018-05-28 15:02:20.383814: ep: 6300 \t reward: 3.056 \t loss: 0.0179 \t game len: 126.3 \t epsilon: 0.58\n",
      "2018-05-28 15:03:13.748806: ep: 6400 \t reward: 2.436 \t loss: 0.0161 \t game len: 128.6 \t epsilon: 0.58\n",
      "2018-05-28 15:04:06.948804: ep: 6500 \t reward: 3.320 \t loss: 0.0177 \t game len: 132.0 \t epsilon: 0.57\n",
      "2018-05-28 15:04:59.948834: ep: 6600 \t reward: 3.390 \t loss: 0.0187 \t game len: 131.2 \t epsilon: 0.57\n",
      "2018-05-28 15:05:51.193097: ep: 6700 \t reward: 2.927 \t loss: 0.0172 \t game len: 119.9 \t epsilon: 0.56\n",
      "2018-05-28 15:06:40.832343: ep: 6800 \t reward: 3.139 \t loss: 0.0188 \t game len: 119.0 \t epsilon: 0.56\n",
      "2018-05-28 15:07:33.747929: ep: 6900 \t reward: 2.949 \t loss: 0.0170 \t game len: 125.8 \t epsilon: 0.55\n",
      "2018-05-28 15:08:21.823580: ep: 7000 \t reward: 2.833 \t loss: 0.0196 \t game len: 116.3 \t epsilon: 0.55\n",
      "2018-05-28 15:09:18.687664: ep: 7100 \t reward: 3.291 \t loss: 0.0173 \t game len: 137.0 \t epsilon: 0.54\n",
      "2018-05-28 15:10:09.591460: ep: 7200 \t reward: 2.992 \t loss: 0.0175 \t game len: 124.5 \t epsilon: 0.54\n",
      "2018-05-28 15:11:03.366096: ep: 7300 \t reward: 3.408 \t loss: 0.0186 \t game len: 130.4 \t epsilon: 0.54\n",
      "2018-05-28 15:11:57.563419: ep: 7400 \t reward: 3.504 \t loss: 0.0193 \t game len: 128.0 \t epsilon: 0.53\n",
      "2018-05-28 15:12:51.300638: ep: 7500 \t reward: 3.218 \t loss: 0.0182 \t game len: 126.2 \t epsilon: 0.53\n",
      "2018-05-28 15:13:44.284099: ep: 7600 \t reward: 3.398 \t loss: 0.0179 \t game len: 123.4 \t epsilon: 0.52\n",
      "2018-05-28 15:14:39.982022: ep: 7700 \t reward: 3.596 \t loss: 0.0189 \t game len: 130.7 \t epsilon: 0.52\n",
      "2018-05-28 15:15:38.069325: ep: 7800 \t reward: 3.928 \t loss: 0.0191 \t game len: 134.8 \t epsilon: 0.51\n",
      "2018-05-28 15:16:33.384001: ep: 7900 \t reward: 3.343 \t loss: 0.0189 \t game len: 124.8 \t epsilon: 0.51\n",
      "2018-05-28 15:17:29.025747: ep: 8000 \t reward: 3.644 \t loss: 0.0204 \t game len: 128.2 \t epsilon: 0.50\n",
      "2018-05-28 15:17:38.932777: ep: 8000 \t Reward evaluation: 6.36\n",
      "2018-05-28 15:18:37.293210: ep: 8100 \t reward: 3.481 \t loss: 0.0196 \t game len: 134.3 \t epsilon: 0.49\n",
      "2018-05-28 15:19:31.797896: ep: 8200 \t reward: 3.323 \t loss: 0.0188 \t game len: 125.7 \t epsilon: 0.49\n",
      "2018-05-28 15:20:27.229895: ep: 8300 \t reward: 3.445 \t loss: 0.0193 \t game len: 125.7 \t epsilon: 0.48\n",
      "2018-05-28 15:21:22.683336: ep: 8400 \t reward: 3.814 \t loss: 0.0187 \t game len: 128.1 \t epsilon: 0.48\n",
      "2018-05-28 15:22:17.974624: ep: 8500 \t reward: 3.867 \t loss: 0.0201 \t game len: 126.9 \t epsilon: 0.47\n",
      "2018-05-28 15:23:11.157673: ep: 8600 \t reward: 3.481 \t loss: 0.0203 \t game len: 121.1 \t epsilon: 0.47\n",
      "2018-05-28 15:24:06.758032: ep: 8700 \t reward: 3.378 \t loss: 0.0191 \t game len: 126.4 \t epsilon: 0.47\n",
      "2018-05-28 15:25:03.280581: ep: 8800 \t reward: 3.709 \t loss: 0.0200 \t game len: 129.7 \t epsilon: 0.46\n",
      "2018-05-28 15:25:58.424911: ep: 8900 \t reward: 3.957 \t loss: 0.0194 \t game len: 125.0 \t epsilon: 0.46\n",
      "2018-05-28 15:26:52.247515: ep: 9000 \t reward: 3.780 \t loss: 0.0209 \t game len: 120.6 \t epsilon: 0.45\n",
      "2018-05-28 15:27:48.793726: ep: 9100 \t reward: 4.294 \t loss: 0.0216 \t game len: 130.7 \t epsilon: 0.45\n",
      "2018-05-28 15:28:41.625280: ep: 9200 \t reward: 4.303 \t loss: 0.0224 \t game len: 119.8 \t epsilon: 0.44\n",
      "2018-05-28 15:29:36.173137: ep: 9300 \t reward: 3.818 \t loss: 0.0200 \t game len: 121.8 \t epsilon: 0.43\n",
      "2018-05-28 15:30:31.796709: ep: 9400 \t reward: 4.253 \t loss: 0.0205 \t game len: 124.8 \t epsilon: 0.43\n",
      "2018-05-28 15:31:27.969170: ep: 9500 \t reward: 4.720 \t loss: 0.0236 \t game len: 126.5 \t epsilon: 0.42\n",
      "2018-05-28 15:32:22.194390: ep: 9600 \t reward: 4.167 \t loss: 0.0211 \t game len: 123.2 \t epsilon: 0.42\n",
      "2018-05-28 15:33:14.427872: ep: 9700 \t reward: 4.092 \t loss: 0.0217 \t game len: 117.6 \t epsilon: 0.41\n",
      "2018-05-28 15:34:11.460348: ep: 9800 \t reward: 3.942 \t loss: 0.0209 \t game len: 126.5 \t epsilon: 0.41\n",
      "2018-05-28 15:35:07.166912: ep: 9900 \t reward: 3.949 \t loss: 0.0204 \t game len: 126.9 \t epsilon: 0.40\n",
      "2018-05-28 15:35:59.203450: ep: 10000 \t reward: 4.151 \t loss: 0.0229 \t game len: 116.8 \t epsilon: 0.40\n",
      "2018-05-28 15:36:07.715198: ep: 10000 \t Reward evaluation: 6.96\n",
      "2018-05-28 15:37:04.835125: ep: 10100 \t reward: 4.344 \t loss: 0.0242 \t game len: 128.8 \t epsilon: 0.40\n",
      "2018-05-28 15:37:56.657053: ep: 10200 \t reward: 4.308 \t loss: 0.0250 \t game len: 116.3 \t epsilon: 0.39\n",
      "2018-05-28 15:38:52.331943: ep: 10300 \t reward: 4.473 \t loss: 0.0225 \t game len: 127.0 \t epsilon: 0.39\n",
      "2018-05-28 15:39:41.578584: ep: 10400 \t reward: 3.838 \t loss: 0.0212 \t game len: 112.9 \t epsilon: 0.38\n",
      "2018-05-28 15:40:35.430553: ep: 10500 \t reward: 4.171 \t loss: 0.0232 \t game len: 120.8 \t epsilon: 0.38\n",
      "2018-05-28 15:41:27.375222: ep: 10600 \t reward: 4.434 \t loss: 0.0236 \t game len: 117.9 \t epsilon: 0.37\n",
      "2018-05-28 15:42:17.031687: ep: 10700 \t reward: 4.557 \t loss: 0.0248 \t game len: 110.7 \t epsilon: 0.36\n",
      "2018-05-28 15:43:11.192808: ep: 10800 \t reward: 3.947 \t loss: 0.0222 \t game len: 122.0 \t epsilon: 0.36\n",
      "2018-05-28 15:44:04.397870: ep: 10900 \t reward: 4.370 \t loss: 0.0217 \t game len: 119.0 \t epsilon: 0.35\n",
      "2018-05-28 15:44:55.023204: ep: 11000 \t reward: 4.352 \t loss: 0.0252 \t game len: 112.9 \t epsilon: 0.35\n",
      "2018-05-28 15:45:46.015858: ep: 11100 \t reward: 4.154 \t loss: 0.0238 \t game len: 113.6 \t epsilon: 0.34\n",
      "2018-05-28 15:46:42.586626: ep: 11200 \t reward: 4.547 \t loss: 0.0216 \t game len: 125.5 \t epsilon: 0.34\n",
      "2018-05-28 15:47:33.679498: ep: 11300 \t reward: 4.373 \t loss: 0.0238 \t game len: 115.8 \t epsilon: 0.33\n",
      "2018-05-28 15:48:24.552870: ep: 11400 \t reward: 4.425 \t loss: 0.0239 \t game len: 115.7 \t epsilon: 0.33\n",
      "2018-05-28 15:49:17.584632: ep: 11500 \t reward: 4.438 \t loss: 0.0220 \t game len: 118.3 \t epsilon: 0.32\n",
      "2018-05-28 15:50:11.779992: ep: 11600 \t reward: 4.387 \t loss: 0.0234 \t game len: 120.4 \t epsilon: 0.32\n",
      "2018-05-28 15:51:03.351391: ep: 11700 \t reward: 4.558 \t loss: 0.0236 \t game len: 116.7 \t epsilon: 0.31\n",
      "2018-05-28 15:51:58.024660: ep: 11800 \t reward: 5.181 \t loss: 0.0243 \t game len: 123.9 \t epsilon: 0.31\n",
      "2018-05-28 15:52:54.294347: ep: 11900 \t reward: 5.097 \t loss: 0.0239 \t game len: 125.2 \t epsilon: 0.31\n",
      "2018-05-28 15:53:50.185213: ep: 12000 \t reward: 4.959 \t loss: 0.0243 \t game len: 125.8 \t epsilon: 0.30\n",
      "2018-05-28 15:53:58.422775: ep: 12000 \t Reward evaluation: 6.72\n",
      "2018-05-28 15:54:54.532814: ep: 12100 \t reward: 5.457 \t loss: 0.0271 \t game len: 126.5 \t epsilon: 0.30\n",
      "2018-05-28 15:55:53.838702: ep: 12200 \t reward: 5.355 \t loss: 0.0232 \t game len: 130.6 \t epsilon: 0.29\n",
      "2018-05-28 15:56:42.558997: ep: 12300 \t reward: 5.259 \t loss: 0.0279 \t game len: 108.0 \t epsilon: 0.29\n",
      "2018-05-28 15:57:34.211284: ep: 12400 \t reward: 4.706 \t loss: 0.0246 \t game len: 115.9 \t epsilon: 0.28\n",
      "2018-05-28 15:58:29.832062: ep: 12500 \t reward: 5.898 \t loss: 0.0250 \t game len: 123.8 \t epsilon: 0.28\n",
      "2018-05-28 15:59:27.132718: ep: 12600 \t reward: 5.666 \t loss: 0.0266 \t game len: 127.9 \t epsilon: 0.27\n",
      "2018-05-28 16:00:21.574404: ep: 12700 \t reward: 5.422 \t loss: 0.0261 \t game len: 120.1 \t epsilon: 0.27\n",
      "2018-05-28 16:01:10.031065: ep: 12800 \t reward: 4.644 \t loss: 0.0247 \t game len: 106.8 \t epsilon: 0.26\n",
      "2018-05-28 16:02:06.777966: ep: 12900 \t reward: 5.988 \t loss: 0.0257 \t game len: 128.0 \t epsilon: 0.26\n",
      "2018-05-28 16:03:01.452605: ep: 13000 \t reward: 5.675 \t loss: 0.0263 \t game len: 122.0 \t epsilon: 0.25\n",
      "2018-05-28 16:03:54.008893: ep: 13100 \t reward: 5.062 \t loss: 0.0262 \t game len: 117.5 \t epsilon: 0.24\n",
      "2018-05-28 16:04:41.149795: ep: 13200 \t reward: 5.335 \t loss: 0.0268 \t game len: 107.5 \t epsilon: 0.24\n",
      "2018-05-28 16:05:33.365351: ep: 13300 \t reward: 5.156 \t loss: 0.0255 \t game len: 116.3 \t epsilon: 0.23\n",
      "2018-05-28 16:06:28.960923: ep: 13400 \t reward: 6.802 \t loss: 0.0286 \t game len: 123.4 \t epsilon: 0.23\n",
      "2018-05-28 16:07:24.430940: ep: 13500 \t reward: 5.694 \t loss: 0.0272 \t game len: 122.1 \t epsilon: 0.22\n",
      "2018-05-28 16:08:17.435813: ep: 13600 \t reward: 5.407 \t loss: 0.0255 \t game len: 118.6 \t epsilon: 0.22\n",
      "2018-05-28 16:09:14.923090: ep: 13700 \t reward: 6.506 \t loss: 0.0278 \t game len: 127.8 \t epsilon: 0.21\n",
      "2018-05-28 16:10:09.156023: ep: 13800 \t reward: 6.033 \t loss: 0.0287 \t game len: 120.5 \t epsilon: 0.21\n",
      "2018-05-28 16:11:07.588727: ep: 13900 \t reward: 6.199 \t loss: 0.0261 \t game len: 129.2 \t epsilon: 0.20\n",
      "2018-05-28 16:12:03.431377: ep: 14000 \t reward: 6.250 \t loss: 0.0275 \t game len: 122.1 \t epsilon: 0.20\n",
      "2018-05-28 16:12:12.039836: ep: 14000 \t Reward evaluation: 8.95\n",
      "2018-05-28 16:13:08.378717: ep: 14100 \t reward: 6.548 \t loss: 0.0307 \t game len: 124.5 \t epsilon: 0.19\n",
      "2018-05-28 16:14:05.065460: ep: 14200 \t reward: 6.877 \t loss: 0.0311 \t game len: 123.1 \t epsilon: 0.19\n",
      "2018-05-28 16:14:54.757289: ep: 14300 \t reward: 6.159 \t loss: 0.0300 \t game len: 108.9 \t epsilon: 0.18\n",
      "2018-05-28 16:15:50.119997: ep: 14400 \t reward: 6.579 \t loss: 0.0295 \t game len: 122.5 \t epsilon: 0.18\n",
      "2018-05-28 16:16:46.293098: ep: 14500 \t reward: 6.625 \t loss: 0.0288 \t game len: 122.9 \t epsilon: 0.17\n",
      "2018-05-28 16:17:42.007980: ep: 14600 \t reward: 6.323 \t loss: 0.0286 \t game len: 120.7 \t epsilon: 0.17\n",
      "2018-05-28 16:18:36.016625: ep: 14700 \t reward: 7.269 \t loss: 0.0323 \t game len: 118.2 \t epsilon: 0.17\n",
      "2018-05-28 16:19:27.091868: ep: 14800 \t reward: 6.505 \t loss: 0.0314 \t game len: 111.8 \t epsilon: 0.16\n",
      "2018-05-28 16:20:18.617734: ep: 14900 \t reward: 6.668 \t loss: 0.0323 \t game len: 112.7 \t epsilon: 0.16\n",
      "2018-05-28 16:21:17.098466: ep: 15000 \t reward: 7.016 \t loss: 0.0289 \t game len: 128.2 \t epsilon: 0.15\n",
      "2018-05-28 16:22:10.515686: ep: 15100 \t reward: 6.970 \t loss: 0.0311 \t game len: 116.9 \t epsilon: 0.15\n",
      "2018-05-28 16:23:01.594285: ep: 15200 \t reward: 6.756 \t loss: 0.0335 \t game len: 112.0 \t epsilon: 0.14\n",
      "2018-05-28 16:23:56.599240: ep: 15300 \t reward: 6.843 \t loss: 0.0310 \t game len: 121.4 \t epsilon: 0.14\n",
      "2018-05-28 16:24:52.635455: ep: 15400 \t reward: 7.286 \t loss: 0.0310 \t game len: 123.3 \t epsilon: 0.13\n",
      "2018-05-28 16:25:44.402852: ep: 15500 \t reward: 6.756 \t loss: 0.0343 \t game len: 114.0 \t epsilon: 0.12\n",
      "2018-05-28 16:26:37.431685: ep: 15600 \t reward: 7.081 \t loss: 0.0308 \t game len: 116.8 \t epsilon: 0.12\n",
      "2018-05-28 16:27:30.986027: ep: 15700 \t reward: 6.721 \t loss: 0.0320 \t game len: 116.3 \t epsilon: 0.11\n",
      "2018-05-28 16:28:23.701159: ep: 15800 \t reward: 7.595 \t loss: 0.0339 \t game len: 113.8 \t epsilon: 0.11\n",
      "2018-05-28 16:29:20.233751: ep: 15900 \t reward: 7.689 \t loss: 0.0327 \t game len: 122.5 \t epsilon: 0.10\n",
      "2018-05-28 16:30:11.393966: ep: 16000 \t reward: 6.951 \t loss: 0.0322 \t game len: 109.7 \t epsilon: 0.10\n",
      "2018-05-28 16:30:19.527886: ep: 16000 \t Reward evaluation: 6.41\n",
      "2018-05-28 16:31:15.626176: ep: 16100 \t reward: 7.848 \t loss: 0.0332 \t game len: 121.9 \t epsilon: 0.09\n",
      "2018-05-28 16:32:09.359174: ep: 16200 \t reward: 7.041 \t loss: 0.0316 \t game len: 116.7 \t epsilon: 0.09\n",
      "2018-05-28 16:33:04.405045: ep: 16300 \t reward: 7.670 \t loss: 0.0324 \t game len: 119.5 \t epsilon: 0.08\n",
      "2018-05-28 16:34:00.126102: ep: 16400 \t reward: 8.469 \t loss: 0.0359 \t game len: 121.4 \t epsilon: 0.08\n",
      "2018-05-28 16:34:57.606573: ep: 16500 \t reward: 8.204 \t loss: 0.0327 \t game len: 124.6 \t epsilon: 0.07\n",
      "2018-05-28 16:35:56.304496: ep: 16600 \t reward: 8.385 \t loss: 0.0328 \t game len: 128.8 \t epsilon: 0.07\n",
      "2018-05-28 16:36:53.985432: ep: 16700 \t reward: 8.270 \t loss: 0.0351 \t game len: 124.1 \t epsilon: 0.06\n",
      "2018-05-28 16:37:49.152343: ep: 16800 \t reward: 7.862 \t loss: 0.0328 \t game len: 119.3 \t epsilon: 0.06\n",
      "2018-05-28 16:38:46.232778: ep: 16900 \t reward: 8.125 \t loss: 0.0343 \t game len: 122.3 \t epsilon: 0.05\n",
      "2018-05-28 16:39:41.683619: ep: 17000 \t reward: 7.974 \t loss: 0.0323 \t game len: 119.1 \t epsilon: 0.05\n",
      "2018-05-28 16:40:36.821111: ep: 17100 \t reward: 7.653 \t loss: 0.0326 \t game len: 119.0 \t epsilon: 0.04\n",
      "2018-05-28 16:41:33.122336: ep: 17200 \t reward: 8.672 \t loss: 0.0334 \t game len: 122.3 \t epsilon: 0.04\n",
      "2018-05-28 16:42:24.189094: ep: 17300 \t reward: 8.188 \t loss: 0.0361 \t game len: 110.0 \t epsilon: 0.04\n",
      "2018-05-28 16:43:23.775877: ep: 17400 \t reward: 8.331 \t loss: 0.0336 \t game len: 131.0 \t epsilon: 0.03\n",
      "2018-05-28 16:44:19.884684: ep: 17500 \t reward: 8.713 \t loss: 0.0350 \t game len: 121.0 \t epsilon: 0.03\n",
      "2018-05-28 16:45:15.467713: ep: 17600 \t reward: 9.161 \t loss: 0.0361 \t game len: 118.8 \t epsilon: 0.02\n",
      "2018-05-28 16:46:11.417650: ep: 17700 \t reward: 8.911 \t loss: 0.0363 \t game len: 121.6 \t epsilon: 0.02\n",
      "2018-05-28 16:47:06.516452: ep: 17800 \t reward: 9.331 \t loss: 0.0368 \t game len: 118.9 \t epsilon: 0.01\n",
      "2018-05-28 16:48:02.372928: ep: 17900 \t reward: 7.943 \t loss: 0.0342 \t game len: 119.3 \t epsilon: 0.01\n",
      "2018-05-28 16:48:54.084263: ep: 18000 \t reward: 8.653 \t loss: 0.0353 \t game len: 112.0 \t epsilon: 0.01\n",
      "2018-05-28 16:49:02.621701: ep: 18000 \t Reward evaluation: 9.53\n",
      "2018-05-28 16:49:57.146835: ep: 18100 \t reward: 8.615 \t loss: 0.0354 \t game len: 118.7 \t epsilon: 0.01\n",
      "2018-05-28 16:50:51.573336: ep: 18200 \t reward: 9.097 \t loss: 0.0381 \t game len: 117.1 \t epsilon: 0.01\n",
      "2018-05-28 16:51:47.528638: ep: 18300 \t reward: 9.309 \t loss: 0.0354 \t game len: 120.2 \t epsilon: 0.01\n",
      "2018-05-28 16:52:44.565725: ep: 18400 \t reward: 9.244 \t loss: 0.0375 \t game len: 121.5 \t epsilon: 0.01\n",
      "2018-05-28 16:53:39.255350: ep: 18500 \t reward: 9.175 \t loss: 0.0361 \t game len: 118.3 \t epsilon: 0.01\n",
      "2018-05-28 16:54:36.071017: ep: 18600 \t reward: 8.930 \t loss: 0.0344 \t game len: 121.6 \t epsilon: 0.01\n",
      "2018-05-28 16:55:30.664154: ep: 18700 \t reward: 8.636 \t loss: 0.0359 \t game len: 117.8 \t epsilon: 0.01\n",
      "2018-05-28 16:56:28.326568: ep: 18800 \t reward: 9.884 \t loss: 0.0370 \t game len: 124.3 \t epsilon: 0.01\n",
      "2018-05-28 16:57:21.865814: ep: 18900 \t reward: 9.573 \t loss: 0.0401 \t game len: 116.0 \t epsilon: 0.01\n",
      "2018-05-28 16:58:20.776181: ep: 19000 \t reward: 9.072 \t loss: 0.0353 \t game len: 125.6 \t epsilon: 0.01\n",
      "2018-05-28 16:59:18.181049: ep: 19100 \t reward: 10.595 \t loss: 0.0390 \t game len: 124.0 \t epsilon: 0.01\n",
      "2018-05-28 17:00:11.100476: ep: 19200 \t reward: 8.943 \t loss: 0.0388 \t game len: 114.3 \t epsilon: 0.01\n",
      "2018-05-28 17:01:04.404970: ep: 19300 \t reward: 9.136 \t loss: 0.0360 \t game len: 116.2 \t epsilon: 0.01\n",
      "2018-05-28 17:01:57.213389: ep: 19400 \t reward: 10.219 \t loss: 0.0433 \t game len: 114.2 \t epsilon: 0.01\n",
      "2018-05-28 17:02:54.657455: ep: 19500 \t reward: 9.489 \t loss: 0.0365 \t game len: 122.2 \t epsilon: 0.01\n",
      "2018-05-28 17:03:51.679027: ep: 19600 \t reward: 9.276 \t loss: 0.0369 \t game len: 122.4 \t epsilon: 0.01\n",
      "2018-05-28 17:04:46.857872: ep: 19700 \t reward: 9.096 \t loss: 0.0373 \t game len: 118.3 \t epsilon: 0.01\n",
      "2018-05-28 17:05:40.052479: ep: 19800 \t reward: 8.611 \t loss: 0.0392 \t game len: 114.2 \t epsilon: 0.01\n",
      "2018-05-28 17:06:31.509160: ep: 19900 \t reward: 9.474 \t loss: 0.0402 \t game len: 110.7 \t epsilon: 0.01\n",
      "2018-05-28 17:07:23.323818: ep: 20000 \t reward: 8.836 \t loss: 0.0392 \t game len: 111.9 \t epsilon: 0.01\n",
      "2018-05-28 17:07:31.834498: ep: 20000 \t Reward evaluation: 9.86\n",
      "2018-05-28 17:08:27.120428: ep: 20100 \t reward: 7.378 \t loss: 0.0346 \t game len: 119.2 \t epsilon: 0.01\n",
      "2018-05-28 17:09:23.096212: ep: 20200 \t reward: 8.198 \t loss: 0.0327 \t game len: 119.0 \t epsilon: 0.01\n",
      "2018-05-28 17:10:15.896382: ep: 20300 \t reward: 9.232 \t loss: 0.0369 \t game len: 112.6 \t epsilon: 0.01\n",
      "2018-05-28 17:11:11.719844: ep: 20400 \t reward: 9.197 \t loss: 0.0385 \t game len: 120.2 \t epsilon: 0.01\n",
      "2018-05-28 17:12:10.944354: ep: 20500 \t reward: 9.944 \t loss: 0.0365 \t game len: 128.4 \t epsilon: 0.01\n",
      "2018-05-28 17:13:06.846800: ep: 20600 \t reward: 9.503 \t loss: 0.0371 \t game len: 120.9 \t epsilon: 0.01\n",
      "2018-05-28 17:13:58.762464: ep: 20700 \t reward: 8.188 \t loss: 0.0364 \t game len: 113.0 \t epsilon: 0.01\n",
      "2018-05-28 17:14:54.784486: ep: 20800 \t reward: 9.265 \t loss: 0.0360 \t game len: 121.4 \t epsilon: 0.01\n",
      "2018-05-28 17:15:45.996194: ep: 20900 \t reward: 8.720 \t loss: 0.0361 \t game len: 112.3 \t epsilon: 0.01\n",
      "2018-05-28 17:16:42.290888: ep: 21000 \t reward: 10.167 \t loss: 0.0372 \t game len: 120.2 \t epsilon: 0.01\n",
      "2018-05-28 17:17:37.310163: ep: 21100 \t reward: 8.232 \t loss: 0.0349 \t game len: 117.6 \t epsilon: 0.01\n",
      "2018-05-28 17:18:32.279982: ep: 21200 \t reward: 9.848 \t loss: 0.0392 \t game len: 117.8 \t epsilon: 0.01\n",
      "2018-05-28 17:19:25.149112: ep: 21300 \t reward: 8.474 \t loss: 0.0362 \t game len: 113.8 \t epsilon: 0.01\n",
      "2018-05-28 17:20:21.555198: ep: 21400 \t reward: 8.585 \t loss: 0.0349 \t game len: 121.8 \t epsilon: 0.01\n",
      "2018-05-28 17:21:13.885498: ep: 21500 \t reward: 9.001 \t loss: 0.0379 \t game len: 111.7 \t epsilon: 0.01\n",
      "2018-05-28 17:22:12.428389: ep: 21600 \t reward: 8.774 \t loss: 0.0338 \t game len: 125.0 \t epsilon: 0.01\n",
      "2018-05-28 17:23:07.129575: ep: 21700 \t reward: 9.980 \t loss: 0.0380 \t game len: 116.7 \t epsilon: 0.01\n",
      "2018-05-28 17:24:00.965501: ep: 21800 \t reward: 9.068 \t loss: 0.0402 \t game len: 116.0 \t epsilon: 0.01\n",
      "2018-05-28 17:24:55.756982: ep: 21900 \t reward: 8.920 \t loss: 0.0350 \t game len: 117.5 \t epsilon: 0.01\n",
      "2018-05-28 17:25:48.169504: ep: 22000 \t reward: 8.733 \t loss: 0.0376 \t game len: 112.1 \t epsilon: 0.01\n",
      "2018-05-28 17:25:58.757917: ep: 22000 \t Reward evaluation: 11.64\n",
      "2018-05-28 17:26:55.333277: ep: 22100 \t reward: 9.136 \t loss: 0.0373 \t game len: 121.2 \t epsilon: 0.01\n",
      "2018-05-28 17:27:51.392279: ep: 22200 \t reward: 9.662 \t loss: 0.0379 \t game len: 119.1 \t epsilon: 0.01\n",
      "2018-05-28 17:28:46.884907: ep: 22300 \t reward: 9.868 \t loss: 0.0388 \t game len: 117.7 \t epsilon: 0.01\n",
      "2018-05-28 17:29:40.576790: ep: 22400 \t reward: 9.620 \t loss: 0.0392 \t game len: 115.3 \t epsilon: 0.01\n",
      "2018-05-28 17:30:32.330194: ep: 22500 \t reward: 9.475 \t loss: 0.0388 \t game len: 112.7 \t epsilon: 0.01\n",
      "2018-05-28 17:31:28.879448: ep: 22600 \t reward: 9.229 \t loss: 0.0367 \t game len: 125.1 \t epsilon: 0.01\n",
      "2018-05-28 17:32:26.955544: ep: 22700 \t reward: 9.533 \t loss: 0.0353 \t game len: 125.9 \t epsilon: 0.01\n",
      "2018-05-28 17:33:22.636051: ep: 22800 \t reward: 9.387 \t loss: 0.0374 \t game len: 119.3 \t epsilon: 0.01\n",
      "2018-05-28 17:34:14.984564: ep: 22900 \t reward: 9.330 \t loss: 0.0395 \t game len: 111.0 \t epsilon: 0.01\n",
      "2018-05-28 17:35:07.137531: ep: 23000 \t reward: 9.950 \t loss: 0.0399 \t game len: 112.7 \t epsilon: 0.01\n",
      "2018-05-28 17:36:01.918979: ep: 23100 \t reward: 9.108 \t loss: 0.0381 \t game len: 118.0 \t epsilon: 0.01\n",
      "2018-05-28 17:36:56.528889: ep: 23200 \t reward: 9.606 \t loss: 0.0391 \t game len: 117.7 \t epsilon: 0.01\n",
      "2018-05-28 17:37:50.955577: ep: 23300 \t reward: 9.955 \t loss: 0.0391 \t game len: 118.2 \t epsilon: 0.01\n",
      "2018-05-28 17:38:50.283396: ep: 23400 \t reward: 8.914 \t loss: 0.0351 \t game len: 127.3 \t epsilon: 0.01\n",
      "2018-05-28 17:39:41.695381: ep: 23500 \t reward: 8.895 \t loss: 0.0373 \t game len: 111.0 \t epsilon: 0.01\n",
      "2018-05-28 17:40:34.263352: ep: 23600 \t reward: 9.142 \t loss: 0.0399 \t game len: 112.7 \t epsilon: 0.01\n",
      "2018-05-28 17:41:28.195018: ep: 23700 \t reward: 8.396 \t loss: 0.0354 \t game len: 117.4 \t epsilon: 0.01\n",
      "2018-05-28 17:42:23.440500: ep: 23800 \t reward: 9.305 \t loss: 0.0376 \t game len: 118.6 \t epsilon: 0.01\n",
      "2018-05-28 17:43:20.382906: ep: 23900 \t reward: 9.021 \t loss: 0.0376 \t game len: 121.8 \t epsilon: 0.01\n",
      "2018-05-28 17:44:10.758658: ep: 24000 \t reward: 8.899 \t loss: 0.0390 \t game len: 107.7 \t epsilon: 0.01\n",
      "2018-05-28 17:44:18.460738: ep: 24000 \t Reward evaluation: 7.40\n",
      "2018-05-28 17:45:16.354463: ep: 24100 \t reward: 9.003 \t loss: 0.0351 \t game len: 123.5 \t epsilon: 0.01\n",
      "2018-05-28 17:46:04.876806: ep: 24200 \t reward: 8.147 \t loss: 0.0390 \t game len: 103.2 \t epsilon: 0.01\n",
      "2018-05-28 17:46:58.908163: ep: 24300 \t reward: 9.404 \t loss: 0.0397 \t game len: 115.7 \t epsilon: 0.01\n",
      "2018-05-28 17:47:54.480823: ep: 24400 \t reward: 9.374 \t loss: 0.0365 \t game len: 119.6 \t epsilon: 0.01\n",
      "2018-05-28 17:48:50.103369: ep: 24500 \t reward: 8.875 \t loss: 0.0368 \t game len: 119.0 \t epsilon: 0.01\n",
      "2018-05-28 17:49:43.146251: ep: 24600 \t reward: 8.974 \t loss: 0.0365 \t game len: 115.3 \t epsilon: 0.01\n",
      "2018-05-28 17:50:37.388710: ep: 24700 \t reward: 8.962 \t loss: 0.0356 \t game len: 116.4 \t epsilon: 0.01\n",
      "2018-05-28 17:51:27.250975: ep: 24800 \t reward: 8.638 \t loss: 0.0397 \t game len: 107.5 \t epsilon: 0.01\n",
      "2018-05-28 17:52:20.266541: ep: 24900 \t reward: 9.287 \t loss: 0.0396 \t game len: 113.5 \t epsilon: 0.01\n",
      "2018-05-28 17:53:12.943602: ep: 25000 \t reward: 8.958 \t loss: 0.0381 \t game len: 113.7 \t epsilon: 0.01\n",
      "2018-05-28 17:54:01.314522: ep: 25100 \t reward: 7.451 \t loss: 0.0362 \t game len: 103.9 \t epsilon: 0.01\n",
      "2018-05-28 17:54:56.571212: ep: 25200 \t reward: 9.061 \t loss: 0.0362 \t game len: 117.7 \t epsilon: 0.01\n",
      "2018-05-28 17:55:52.181017: ep: 25300 \t reward: 9.335 \t loss: 0.0387 \t game len: 118.5 \t epsilon: 0.01\n",
      "2018-05-28 17:56:45.902869: ep: 25400 \t reward: 9.015 \t loss: 0.0354 \t game len: 116.0 \t epsilon: 0.01\n",
      "2018-05-28 17:57:35.513375: ep: 25500 \t reward: 9.003 \t loss: 0.0394 \t game len: 107.3 \t epsilon: 0.01\n",
      "2018-05-28 17:58:24.375950: ep: 25600 \t reward: 8.786 \t loss: 0.0391 \t game len: 105.8 \t epsilon: 0.01\n",
      "2018-05-28 17:59:13.714863: ep: 25700 \t reward: 9.620 \t loss: 0.0435 \t game len: 107.3 \t epsilon: 0.01\n",
      "2018-05-28 18:00:07.107710: ep: 25800 \t reward: 8.233 \t loss: 0.0371 \t game len: 114.5 \t epsilon: 0.01\n",
      "2018-05-28 18:00:56.948196: ep: 25900 \t reward: 8.568 \t loss: 0.0374 \t game len: 106.5 \t epsilon: 0.01\n",
      "2018-05-28 18:01:51.844081: ep: 26000 \t reward: 8.907 \t loss: 0.0370 \t game len: 118.9 \t epsilon: 0.01\n",
      "2018-05-28 18:01:59.692490: ep: 26000 \t Reward evaluation: 8.83\n"
     ]
    }
   ],
   "source": [
    "REPORT_INTERVAL = 100\n",
    "EVAL_INTERVAL = 2000\n",
    "R = []\n",
    "L = []\n",
    "play_length = []\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "decay = 0.1/2000\n",
    "start_ep = 0\n",
    "\n",
    "for ep in range(start_ep,1000000):\n",
    "    \n",
    "    # Play one game:\n",
    "    epsilon = max(EPS_START - decay*(ep), EPS_END)\n",
    "    r, i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "    R.append(r)\n",
    "    play_length.append(i)\n",
    "    \n",
    "    # Train:\n",
    "    for _ in range(i):\n",
    "        l = train_batch()\n",
    "        L.append(float(l))\n",
    "    \n",
    "    if ep % REPORT_INTERVAL == 0:\n",
    "        print(\"%s: ep: %s \\t reward: %.3f \\t loss: %.4f \\t game len: %.1f \\t epsilon: %.2f\" % \n",
    "              (str(datetime.datetime.now()), ep, np.mean(R), np.mean(L), np.mean(play_length), epsilon))\n",
    "        R = []\n",
    "        L = []\n",
    "        play_length = []\n",
    "    \n",
    "    if ep % EVAL_INTERVAL == 0:\n",
    "        eval_reward = evaluate_agent()\n",
    "        save_checkpoint()\n",
    "        print(\"%s: ep: %s \\t Reward evaluation: %.2f\" % (str(datetime.datetime.now()), ep, eval_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time now:\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate agent with 5% epsilon greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agent with greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "snake.on_init()\n",
    "state, reward, done = snake.on_feedback()\n",
    "\n",
    "for _ in range(10):\n",
    "    print(play_game(snake, deep_agent, epsilon = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
