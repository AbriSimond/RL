{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/finn/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/finn/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install pygame -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snakai\n",
    "import agents\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n",
    "cpu = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:1\")\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how to play game and replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_size = (10,10)\n",
    "def tuple_to_torch(tup):\n",
    "    return torch.from_numpy(np.array(tup))\n",
    "\n",
    "action2ind = {'right' : 0,\n",
    "             'left' : 1,\n",
    "             'up' : 2,\n",
    "             'down' : 3}\n",
    "ind2action = {val: key for key, val in action2ind.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(snake, agent, epsilon = 0.05):\n",
    "    cum_reward = 0.0\n",
    "    snake.on_init()\n",
    "    state, reward, ended = snake.on_feedback()\n",
    "\n",
    "    for i in range(200):\n",
    "        action = agent(state, th = epsilon)\n",
    "        next_state, reward, ended = snake.step(action)\n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        # Keep all the games:\n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        if ended == 1:\n",
    "            return cum_reward, i\n",
    "    return cum_reward, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 64\n",
    "ksize = 4\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=ksize, stride=2, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 0)\n",
    "        #self.dense1 = nn.Linear(2592, 1024)\n",
    "        self.head = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.relu(self.dense1(x))\n",
    "        return 2*F.tanh(self.head(x))\n",
    "    \n",
    "model = DQN().to(device)\n",
    "batch_size = 32\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001) # , weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-02 *\n",
       "       [ 8.9083,  8.5205,  8.2099,  9.3867,  9.0999,  8.3634,  9.2137,\n",
       "         7.7036,  7.5422,  7.8198,  7.8792,  8.5759,  9.0187,  8.1598,\n",
       "         7.6688,  7.9860,  9.4241,  8.0003,  8.7647,  8.3466,  9.0797,\n",
       "         9.2634,  7.9579,  8.5084,  7.3906,  8.2424,  8.4048,  8.8836,\n",
       "         7.7570], device='cuda:1')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_hat.max(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # GET SAMPLE OF DATA\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = tuple_to_torch(batch.state).float().to(device)\n",
    "    next_state_batch = tuple_to_torch(batch.next_state).float().to(device)\n",
    "    action_batch = tuple_to_torch(list(action2ind[a] for a in batch.action)).to(device)\n",
    "    reward_batch = tuple_to_torch(batch.reward).float().to(device)\n",
    "\n",
    "\n",
    "    ## Calculate expected reward:\n",
    "    GAMMA = 0.99\n",
    "    with torch.set_grad_enabled(False):\n",
    "        not_ended_batch = 1 -torch.ByteTensor(batch.ended)\n",
    "        next_states_non_final = next_state_batch[not_ended_batch]\n",
    "        next_state_values = torch.zeros(batch_size).to(device)\n",
    "        reward_hat = model(next_states_non_final)\n",
    "        next_state_values[not_ended_batch] = reward_hat.max(1)[0]\n",
    "        expected_state_action_values = next_state_values*GAMMA + reward_batch\n",
    "\n",
    "\n",
    "    # Predict value function:\n",
    "    yhat = model(state_batch)\n",
    "    state_action_values = yhat.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch():\n",
    "    if len(memory) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    # GET SAMPLE OF DATA\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = tuple_to_torch(batch.state).float().to(device)\n",
    "    next_state_batch = tuple_to_torch(batch.next_state).float().to(device)\n",
    "    action_batch = tuple_to_torch(list(action2ind[a] for a in batch.action)).to(device)\n",
    "    reward_batch = tuple_to_torch(batch.reward).float().to(device)\n",
    "\n",
    "\n",
    "    ## Calculate expected reward:\n",
    "    GAMMA = 0.99\n",
    "    with torch.set_grad_enabled(False):\n",
    "        not_ended_batch = 1 -torch.ByteTensor(batch.ended)\n",
    "        next_states_non_final = next_state_batch[not_ended_batch]\n",
    "        next_state_values = torch.zeros(batch_size).to(device)\n",
    "        reward_hat = model(next_states_non_final)\n",
    "        next_state_values[not_ended_batch] = reward_hat.max(1)[0]\n",
    "        expected_state_action_values = next_state_values*GAMMA + reward_batch\n",
    "\n",
    "\n",
    "    # Predict value function:\n",
    "    yhat = model(state_batch)\n",
    "    state_action_values = yhat.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import random_legal_move\n",
    "def deep_agent(state, th):\n",
    "    \n",
    "    if random.random() < th:\n",
    "        return random_legal_move(state)\n",
    "    \n",
    "    state = torch.unsqueeze(torch.from_numpy(state),0).float().to(device)\n",
    "    yhat = model(state)\n",
    "    action = [ind2action[a] for a in yhat.argmax(1).data.cpu().numpy()]\n",
    "    if len(action) > 1:\n",
    "        raise Exception\n",
    "    action = action[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "\n",
    "# Warmup memory:\n",
    "for _ in range(10):\n",
    "    play_game(snake, deep_agent, epsilon = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(n = 100, epsilon = 0.05):\n",
    "    rewards = np.zeros(n)\n",
    "    for ep in range(n):\n",
    "        rewards[ep],i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "        \n",
    "    return np.mean(rewards)\n",
    "\n",
    "def save_checkpoint():\n",
    "    filename = \"models/snake_legalexp_ep:%02d-reward:%.2f.pth\" %( ep, eval_reward)\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-28 13:38:58.261089: ep: 0 \t reward: -1.430 \t loss: 0.0585 \t game len: 43.0 \t epsilon: 0.90\n",
      "2018-05-28 13:38:58.565142: ep: 0 \t Reward evaluation: -0.94\n",
      "2018-05-28 13:39:11.462230: ep: 100 \t reward: -0.867 \t loss: 0.0536 \t game len: 32.1 \t epsilon: 0.90\n",
      "2018-05-28 13:39:38.315400: ep: 200 \t reward: -0.593 \t loss: 0.0432 \t game len: 63.2 \t epsilon: 0.89\n",
      "2018-05-28 13:40:06.998426: ep: 300 \t reward: -0.779 \t loss: 0.0284 \t game len: 73.7 \t epsilon: 0.89\n",
      "2018-05-28 13:40:36.635899: ep: 400 \t reward: -0.470 \t loss: 0.0232 \t game len: 78.2 \t epsilon: 0.88\n",
      "2018-05-28 13:41:08.635133: ep: 500 \t reward: -0.601 \t loss: 0.0229 \t game len: 81.2 \t epsilon: 0.88\n",
      "2018-05-28 13:41:39.795866: ep: 600 \t reward: -0.181 \t loss: 0.0245 \t game len: 79.6 \t epsilon: 0.87\n",
      "2018-05-28 13:42:16.344601: ep: 700 \t reward: -0.371 \t loss: 0.0195 \t game len: 93.5 \t epsilon: 0.86\n",
      "2018-05-28 13:43:03.788676: ep: 800 \t reward: -0.338 \t loss: 0.0135 \t game len: 123.2 \t epsilon: 0.86\n",
      "2018-05-28 13:43:52.668336: ep: 900 \t reward: -0.110 \t loss: 0.0115 \t game len: 123.7 \t epsilon: 0.85\n",
      "2018-05-28 13:44:48.866019: ep: 1000 \t reward: 0.006 \t loss: 0.0085 \t game len: 141.0 \t epsilon: 0.85\n",
      "2018-05-28 13:45:46.330188: ep: 1100 \t reward: 0.964 \t loss: 0.0099 \t game len: 149.2 \t epsilon: 0.84\n",
      "2018-05-28 13:46:41.910045: ep: 1200 \t reward: 0.824 \t loss: 0.0085 \t game len: 145.1 \t epsilon: 0.84\n",
      "2018-05-28 13:47:36.145525: ep: 1300 \t reward: 1.044 \t loss: 0.0106 \t game len: 136.3 \t epsilon: 0.83\n",
      "2018-05-28 13:48:32.014608: ep: 1400 \t reward: 0.912 \t loss: 0.0091 \t game len: 144.3 \t epsilon: 0.83\n",
      "2018-05-28 13:49:27.306233: ep: 1500 \t reward: 0.808 \t loss: 0.0092 \t game len: 137.7 \t epsilon: 0.83\n",
      "2018-05-28 13:50:21.892211: ep: 1600 \t reward: 1.049 \t loss: 0.0103 \t game len: 137.9 \t epsilon: 0.82\n",
      "2018-05-28 13:51:17.233368: ep: 1700 \t reward: 1.066 \t loss: 0.0107 \t game len: 138.2 \t epsilon: 0.82\n",
      "2018-05-28 13:52:11.334804: ep: 1800 \t reward: 0.773 \t loss: 0.0095 \t game len: 138.2 \t epsilon: 0.81\n",
      "2018-05-28 13:53:09.753471: ep: 1900 \t reward: 1.471 \t loss: 0.0107 \t game len: 146.1 \t epsilon: 0.81\n",
      "2018-05-28 13:54:02.429398: ep: 2000 \t reward: 1.156 \t loss: 0.0112 \t game len: 130.2 \t epsilon: 0.80\n",
      "2018-05-28 13:54:10.028654: ep: 2000 \t Reward evaluation: 4.80\n",
      "2018-05-28 13:55:05.255449: ep: 2100 \t reward: 1.340 \t loss: 0.0154 \t game len: 138.0 \t epsilon: 0.80\n"
     ]
    }
   ],
   "source": [
    "REPORT_INTERVAL = 100\n",
    "EVAL_INTERVAL = 2000\n",
    "R = []\n",
    "L = []\n",
    "play_length = []\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "decay = 0.1/2000\n",
    "start_ep = 0\n",
    "\n",
    "for ep in range(start_ep,1000000):\n",
    "    \n",
    "    # Play one game:\n",
    "    epsilon = max(EPS_START - decay*(ep), EPS_END)\n",
    "    r, i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "    R.append(r)\n",
    "    play_length.append(i)\n",
    "    \n",
    "    # Train:\n",
    "    for _ in range(i):\n",
    "        l = train_batch()\n",
    "        L.append(float(l))\n",
    "    \n",
    "    if ep % REPORT_INTERVAL == 0:\n",
    "        print(\"%s: ep: %s \\t reward: %.3f \\t loss: %.4f \\t game len: %.1f \\t epsilon: %.2f\" % \n",
    "              (str(datetime.datetime.now()), ep, np.mean(R), np.mean(L), np.mean(play_length), epsilon))\n",
    "        R = []\n",
    "        L = []\n",
    "        play_length = []\n",
    "    \n",
    "    if ep % EVAL_INTERVAL == 0:\n",
    "        eval_reward = evaluate_agent()\n",
    "        save_checkpoint()\n",
    "        print(\"%s: ep: %s \\t Reward evaluation: %.2f\" % (str(datetime.datetime.now()), ep, eval_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time now:\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate agent with 5% epsilon greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agent with greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "snake.on_init()\n",
    "state, reward, done = snake.on_feedback()\n",
    "\n",
    "for _ in range(10):\n",
    "    print(play_game(snake, deep_agent, epsilon = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
