{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snakai\n",
    "import agents\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how to play game and replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_size = (10,10)\n",
    "def tuple_to_torch(tup):\n",
    "    return torch.from_numpy(np.array(tup))\n",
    "\n",
    "action2ind = {'right' : 0,\n",
    "             'left' : 1,\n",
    "             'up' : 2,\n",
    "             'down' : 3}\n",
    "ind2action = {val: key for key, val in action2ind.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(snake, agent, epsilon = 0.05):\n",
    "    cum_reward = 0.0\n",
    "    snake.on_init()\n",
    "    state, reward, ended = snake.on_feedback()\n",
    "\n",
    "    for i in range(200):\n",
    "        action = agent(state, th = epsilon)\n",
    "        next_state, reward, ended = snake.step(action)\n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        # Keep all the games:\n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        if ended == 1:\n",
    "            return cum_reward, i\n",
    "    return cum_reward, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 64\n",
    "ksize = 4\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=ksize, stride=2, padding = 2)\n",
    "        self.bn1 = nn.BatchNorm2d(ch)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 0)\n",
    "        #self.dense1 = nn.Linear(2592, 1024)\n",
    "        self.head = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.relu(self.dense1(x))\n",
    "        return 2*F.tanh(self.head(x))\n",
    "    \n",
    "model = DQN()\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitation_state_dict = torch.load(\"imitation_learning.pth\")\n",
    "#model.load_state_dict(imitation_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(model.head.parameters(), lr = 0.001) # , weight_decay = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001) # , weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch():\n",
    "    if len(memory) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    # GET SAMPLE OF DATA\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = tuple_to_torch(batch.state).float()\n",
    "    next_state_batch = tuple_to_torch(batch.next_state).float()\n",
    "    action_batch = tuple_to_torch(list(action2ind[a] for a in batch.action))\n",
    "    reward_batch = tuple_to_torch(batch.reward).float()\n",
    "\n",
    "\n",
    "    ## Calculate expected reward:\n",
    "    GAMMA = 0.99\n",
    "    with torch.set_grad_enabled(False):\n",
    "        not_ended_batch = 1 -torch.ByteTensor(batch.ended)\n",
    "        next_states_non_final = next_state_batch[not_ended_batch]\n",
    "        next_state_values = torch.zeros(batch_size)\n",
    "        reward_hat = model(next_states_non_final)\n",
    "        next_state_values[not_ended_batch] = reward_hat.max(1)[0]\n",
    "        expected_state_action_values = next_state_values*GAMMA + reward_batch\n",
    "\n",
    "\n",
    "    # Predict value function:\n",
    "    yhat = model(state_batch)\n",
    "    state_action_values = yhat.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_agent(state, th):\n",
    "    \n",
    "    if random.random() < th:\n",
    "        return random.sample(list(ind2action.values()), 1)[0]\n",
    "    \n",
    "    state = torch.unsqueeze(torch.from_numpy(state),0).float()\n",
    "    yhat = model(state)\n",
    "    action = [ind2action[a] for a in yhat.argmax(1).data.numpy()]\n",
    "    if len(action) > 1:\n",
    "        raise Exception\n",
    "    action = action[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "\n",
    "# Warmup memory:\n",
    "for _ in range(10):\n",
    "    play_game(snake, deep_agent, epsilon = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-27 11:51:29.220465: episodes: 0, reward: -1.050, loss: 0.0135, game length: 5.0\n",
      "2018-05-27 11:51:53.874540: episodes: 100, reward: -0.964, loss: 0.0113, game length: 6.5\n",
      "2018-05-27 11:52:19.607622: episodes: 200, reward: -0.909, loss: 0.0112, game length: 6.0\n",
      "2018-05-27 11:52:45.438028: episodes: 300, reward: -0.881, loss: 0.0097, game length: 5.3\n",
      "2018-05-27 11:53:11.695321: episodes: 400, reward: -0.868, loss: 0.0105, game length: 6.0\n"
     ]
    }
   ],
   "source": [
    "REPORT_INTERVAL = 100\n",
    "R = []\n",
    "L = []\n",
    "play_length = []\n",
    "for ep in range(10000):\n",
    "    \n",
    "    # Play one game:\n",
    "    r, i = play_game(snake, deep_agent, epsilon = 0.5)\n",
    "    R.append(r)\n",
    "    play_length.append(i)\n",
    "    \n",
    "    # Train:\n",
    "    for _ in range(10):\n",
    "        l = train_batch()\n",
    "        L.append(float(l))\n",
    "    \n",
    "    if ep % REPORT_INTERVAL == 0:\n",
    "        print(\"%s: ep: %s \\t reward: %.3f \\t loss: %.4f \\t game len: %.1f\" % \n",
    "              (str(datetime.datetime.now()), ep, np.mean(R), np.mean(L), np.mean(play_length)))\n",
    "        R = []\n",
    "        L = []\n",
    "        play_length = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.parameters():\n",
    "    print(layer.require_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=True, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.1)\n",
    "snake.on_init()\n",
    "state, reward, done = snake.on_feedback()\n",
    "\n",
    "for _ in range(10):\n",
    "    print(play_game(snake, deep_agent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
