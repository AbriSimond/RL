{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip3 install pygame -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snakai\n",
    "import agents\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how to play game and replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_size = (10,10)\n",
    "def tuple_to_torch(tup):\n",
    "    return torch.from_numpy(np.array(tup))\n",
    "\n",
    "action2ind = {'right' : 0,\n",
    "             'left' : 1,\n",
    "             'up' : 2,\n",
    "             'down' : 3}\n",
    "ind2action = {val: key for key, val in action2ind.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(snake, agent, epsilon = 0.05):\n",
    "    cum_reward = 0.0\n",
    "    snake.on_init()\n",
    "    state, reward, ended = snake.on_feedback()\n",
    "\n",
    "    for i in range(200):\n",
    "        action = agent(state, th = epsilon)\n",
    "        next_state, reward, ended = snake.step(action)\n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        # Keep all the games:\n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        if ended == 1:\n",
    "            return cum_reward, i\n",
    "    return cum_reward, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 64\n",
    "ksize = 4\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=ksize, stride=2, padding = 2)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 0)\n",
    "        #self.dense1 = nn.Linear(2592, 1024)\n",
    "        self.head = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.relu(self.dense1(x))\n",
    "        return 2*F.tanh(self.head(x))\n",
    "    \n",
    "model = DQN()\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imitation_state_dict = torch.load(\"imitation_learning.pth\")\n",
    "#model.load_state_dict(imitation_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(model.head.parameters(), lr = 0.001) # , weight_decay = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001) # , weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch():\n",
    "    if len(memory) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    # GET SAMPLE OF DATA\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = tuple_to_torch(batch.state).float()\n",
    "    next_state_batch = tuple_to_torch(batch.next_state).float()\n",
    "    action_batch = tuple_to_torch(list(action2ind[a] for a in batch.action))\n",
    "    reward_batch = tuple_to_torch(batch.reward).float()\n",
    "\n",
    "\n",
    "    ## Calculate expected reward:\n",
    "    GAMMA = 0.99\n",
    "    with torch.set_grad_enabled(False):\n",
    "        not_ended_batch = 1 -torch.ByteTensor(batch.ended)\n",
    "        next_states_non_final = next_state_batch[not_ended_batch]\n",
    "        next_state_values = torch.zeros(batch_size)\n",
    "        reward_hat = model(next_states_non_final)\n",
    "        next_state_values[not_ended_batch] = reward_hat.max(1)[0]\n",
    "        expected_state_action_values = next_state_values*GAMMA + reward_batch\n",
    "\n",
    "\n",
    "    # Predict value function:\n",
    "    yhat = model(state_batch)\n",
    "    state_action_values = yhat.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_agent(state, th):\n",
    "    \n",
    "    if random.random() < th:\n",
    "        return random.sample(list(ind2action.values()), 1)[0]\n",
    "    \n",
    "    state = torch.unsqueeze(torch.from_numpy(state),0).float()\n",
    "    yhat = model(state)\n",
    "    action = [ind2action[a] for a in yhat.argmax(1).data.numpy()]\n",
    "    if len(action) > 1:\n",
    "        raise Exception\n",
    "    action = action[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "\n",
    "# Warmup memory:\n",
    "for _ in range(10):\n",
    "    play_game(snake, deep_agent, epsilon = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(n = 100, epsilon = 0.05):\n",
    "    rewards = np.zeros(n)\n",
    "    for ep in range(n):\n",
    "        rewards[ep],i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "        \n",
    "    return np.mean(rewards)\n",
    "\n",
    "def save_checkpoint():\n",
    "    filename = \"models/snake_ep:%02d-reward:%.2f.pth\" %( ep, eval_reward)\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-27 11:04:58.410492: ep: 0 \t reward: -1.010 \t loss: 0.0403 \t game len: 1.0 \t epsilon: 0.90\n",
      "2018-05-27 11:04:58.791300: ep: 0 \t Reward evaluation: -0.93\n",
      "2018-05-27 11:05:29.820608: ep: 100 \t reward: -0.950 \t loss: 0.0096 \t game len: 2.0 \t epsilon: 0.90\n",
      "2018-05-27 11:06:01.321170: ep: 200 \t reward: -0.962 \t loss: 0.0063 \t game len: 2.3 \t epsilon: 0.89\n",
      "2018-05-27 11:06:32.523294: ep: 300 \t reward: -0.924 \t loss: 0.0091 \t game len: 2.4 \t epsilon: 0.89\n",
      "2018-05-27 11:07:03.796442: ep: 400 \t reward: -0.943 \t loss: 0.0115 \t game len: 2.4 \t epsilon: 0.88\n",
      "2018-05-27 11:07:35.210666: ep: 500 \t reward: -0.982 \t loss: 0.0132 \t game len: 2.3 \t epsilon: 0.88\n",
      "2018-05-27 11:08:06.640280: ep: 600 \t reward: -0.972 \t loss: 0.0128 \t game len: 2.2 \t epsilon: 0.87\n",
      "2018-05-27 11:08:37.941304: ep: 700 \t reward: -0.954 \t loss: 0.0125 \t game len: 2.5 \t epsilon: 0.86\n",
      "2018-05-27 11:09:09.125471: ep: 800 \t reward: -0.931 \t loss: 0.0125 \t game len: 2.2 \t epsilon: 0.86\n",
      "2018-05-27 11:09:40.291433: ep: 900 \t reward: -0.900 \t loss: 0.0128 \t game len: 2.1 \t epsilon: 0.85\n",
      "2018-05-27 11:10:11.363161: ep: 1000 \t reward: -0.890 \t loss: 0.0107 \t game len: 2.1 \t epsilon: 0.85\n",
      "2018-05-27 11:10:42.442621: ep: 1100 \t reward: -0.931 \t loss: 0.0144 \t game len: 3.2 \t epsilon: 0.84\n",
      "2018-05-27 11:11:13.636448: ep: 1200 \t reward: -0.994 \t loss: 0.0117 \t game len: 2.4 \t epsilon: 0.84\n",
      "2018-05-27 11:11:44.762993: ep: 1300 \t reward: -0.956 \t loss: 0.0102 \t game len: 2.6 \t epsilon: 0.83\n",
      "2018-05-27 11:12:16.014025: ep: 1400 \t reward: -0.943 \t loss: 0.0107 \t game len: 3.4 \t epsilon: 0.83\n",
      "2018-05-27 11:12:47.211146: ep: 1500 \t reward: -0.940 \t loss: 0.0104 \t game len: 3.1 \t epsilon: 0.83\n",
      "2018-05-27 11:13:18.314046: ep: 1600 \t reward: -0.996 \t loss: 0.0094 \t game len: 2.7 \t epsilon: 0.82\n",
      "2018-05-27 11:13:49.462738: ep: 1700 \t reward: -0.936 \t loss: 0.0093 \t game len: 2.7 \t epsilon: 0.82\n",
      "2018-05-27 11:14:20.573454: ep: 1800 \t reward: -0.925 \t loss: 0.0095 \t game len: 2.6 \t epsilon: 0.81\n",
      "2018-05-27 11:14:51.835165: ep: 1900 \t reward: -0.991 \t loss: 0.0093 \t game len: 3.2 \t epsilon: 0.81\n",
      "2018-05-27 11:15:23.147954: ep: 2000 \t reward: -0.878 \t loss: 0.0089 \t game len: 2.9 \t epsilon: 0.80\n",
      "2018-05-27 11:15:27.615324: ep: 2000 \t Reward evaluation: -0.88\n",
      "2018-05-27 11:16:00.688034: ep: 2100 \t reward: -0.907 \t loss: 0.0090 \t game len: 2.8 \t epsilon: 0.80\n",
      "2018-05-27 11:16:33.787581: ep: 2200 \t reward: -0.936 \t loss: 0.0062 \t game len: 2.6 \t epsilon: 0.79\n",
      "2018-05-27 11:17:06.719906: ep: 2300 \t reward: -0.943 \t loss: 0.0061 \t game len: 2.3 \t epsilon: 0.79\n",
      "2018-05-27 11:17:39.561223: ep: 2400 \t reward: -0.892 \t loss: 0.0065 \t game len: 2.4 \t epsilon: 0.78\n",
      "2018-05-27 11:18:12.118976: ep: 2500 \t reward: -0.925 \t loss: 0.0080 \t game len: 3.6 \t epsilon: 0.78\n",
      "2018-05-27 11:18:44.540843: ep: 2600 \t reward: -0.962 \t loss: 0.0078 \t game len: 3.2 \t epsilon: 0.77\n",
      "2018-05-27 11:19:16.835380: ep: 2700 \t reward: -0.934 \t loss: 0.0080 \t game len: 2.5 \t epsilon: 0.77\n",
      "2018-05-27 11:19:49.011157: ep: 2800 \t reward: -0.961 \t loss: 0.0078 \t game len: 3.1 \t epsilon: 0.76\n",
      "2018-05-27 11:20:20.736151: ep: 2900 \t reward: -0.970 \t loss: 0.0077 \t game len: 3.0 \t epsilon: 0.76\n",
      "2018-05-27 11:20:52.653992: ep: 3000 \t reward: -0.913 \t loss: 0.0070 \t game len: 3.4 \t epsilon: 0.75\n",
      "2018-05-27 11:21:24.417382: ep: 3100 \t reward: -0.970 \t loss: 0.0074 \t game len: 3.1 \t epsilon: 0.74\n",
      "2018-05-27 11:21:56.122180: ep: 3200 \t reward: -0.910 \t loss: 0.0080 \t game len: 3.1 \t epsilon: 0.74\n",
      "2018-05-27 11:22:27.754999: ep: 3300 \t reward: -0.922 \t loss: 0.0087 \t game len: 3.4 \t epsilon: 0.73\n",
      "2018-05-27 11:22:59.297739: ep: 3400 \t reward: -0.888 \t loss: 0.0081 \t game len: 2.9 \t epsilon: 0.73\n",
      "2018-05-27 11:23:30.909215: ep: 3500 \t reward: -0.912 \t loss: 0.0096 \t game len: 3.3 \t epsilon: 0.72\n",
      "2018-05-27 11:24:02.476823: ep: 3600 \t reward: -0.818 \t loss: 0.0089 \t game len: 3.0 \t epsilon: 0.72\n",
      "2018-05-27 11:24:34.047043: ep: 3700 \t reward: -0.892 \t loss: 0.0091 \t game len: 3.3 \t epsilon: 0.72\n",
      "2018-05-27 11:25:05.702795: ep: 3800 \t reward: -0.896 \t loss: 0.0094 \t game len: 3.8 \t epsilon: 0.71\n",
      "2018-05-27 11:25:37.245207: ep: 3900 \t reward: -0.875 \t loss: 0.0106 \t game len: 3.7 \t epsilon: 0.71\n",
      "2018-05-27 11:26:08.927530: ep: 4000 \t reward: -0.846 \t loss: 0.0115 \t game len: 3.8 \t epsilon: 0.70\n",
      "2018-05-27 11:26:12.450828: ep: 4000 \t Reward evaluation: 0.15\n",
      "2018-05-27 11:26:45.519044: ep: 4100 \t reward: -0.891 \t loss: 0.0173 \t game len: 3.2 \t epsilon: 0.70\n",
      "2018-05-27 11:27:18.496906: ep: 4200 \t reward: -0.918 \t loss: 0.0125 \t game len: 2.9 \t epsilon: 0.69\n",
      "2018-05-27 11:27:51.294652: ep: 4300 \t reward: -0.939 \t loss: 0.0116 \t game len: 4.0 \t epsilon: 0.69\n",
      "2018-05-27 11:28:24.080072: ep: 4400 \t reward: -0.820 \t loss: 0.0117 \t game len: 3.2 \t epsilon: 0.68\n",
      "2018-05-27 11:28:56.638240: ep: 4500 \t reward: -0.873 \t loss: 0.0106 \t game len: 3.4 \t epsilon: 0.68\n",
      "2018-05-27 11:29:29.208238: ep: 4600 \t reward: -0.880 \t loss: 0.0104 \t game len: 3.1 \t epsilon: 0.67\n",
      "2018-05-27 11:30:01.500975: ep: 4700 \t reward: -0.895 \t loss: 0.0110 \t game len: 3.6 \t epsilon: 0.67\n",
      "2018-05-27 11:30:33.651758: ep: 4800 \t reward: -0.813 \t loss: 0.0115 \t game len: 3.5 \t epsilon: 0.66\n",
      "2018-05-27 11:31:05.785605: ep: 4900 \t reward: -0.829 \t loss: 0.0141 \t game len: 4.1 \t epsilon: 0.66\n",
      "2018-05-27 11:31:37.737157: ep: 5000 \t reward: -0.879 \t loss: 0.0134 \t game len: 4.1 \t epsilon: 0.65\n",
      "2018-05-27 11:32:09.608470: ep: 5100 \t reward: -0.868 \t loss: 0.0123 \t game len: 4.0 \t epsilon: 0.65\n",
      "2018-05-27 11:32:41.572878: ep: 5200 \t reward: -0.820 \t loss: 0.0134 \t game len: 4.3 \t epsilon: 0.64\n",
      "2018-05-27 11:33:13.487391: ep: 5300 \t reward: -0.829 \t loss: 0.0137 \t game len: 4.0 \t epsilon: 0.64\n",
      "2018-05-27 11:33:45.309759: ep: 5400 \t reward: -0.874 \t loss: 0.0154 \t game len: 3.6 \t epsilon: 0.63\n",
      "2018-05-27 11:34:17.164403: ep: 5500 \t reward: -0.856 \t loss: 0.0143 \t game len: 3.8 \t epsilon: 0.62\n",
      "2018-05-27 11:34:49.195123: ep: 5600 \t reward: -0.823 \t loss: 0.0142 \t game len: 4.5 \t epsilon: 0.62\n",
      "2018-05-27 11:35:21.324758: ep: 5700 \t reward: -0.762 \t loss: 0.0157 \t game len: 4.4 \t epsilon: 0.61\n",
      "2018-05-27 11:35:53.416485: ep: 5800 \t reward: -0.874 \t loss: 0.0150 \t game len: 3.6 \t epsilon: 0.61\n",
      "2018-05-27 11:36:25.562913: ep: 5900 \t reward: -0.777 \t loss: 0.0146 \t game len: 3.9 \t epsilon: 0.60\n",
      "2018-05-27 11:36:57.731066: ep: 6000 \t reward: -0.840 \t loss: 0.0161 \t game len: 4.2 \t epsilon: 0.60\n",
      "2018-05-27 11:37:00.807364: ep: 6000 \t Reward evaluation: 1.14\n",
      "2018-05-27 11:37:34.154864: ep: 6100 \t reward: -0.820 \t loss: 0.0293 \t game len: 4.2 \t epsilon: 0.59\n",
      "2018-05-27 11:38:07.533925: ep: 6200 \t reward: -0.780 \t loss: 0.0238 \t game len: 5.3 \t epsilon: 0.59\n",
      "2018-05-27 11:38:40.669962: ep: 6300 \t reward: -0.792 \t loss: 0.0215 \t game len: 4.5 \t epsilon: 0.58\n",
      "2018-05-27 11:39:13.758458: ep: 6400 \t reward: -0.838 \t loss: 0.0204 \t game len: 4.0 \t epsilon: 0.58\n",
      "2018-05-27 11:39:46.624284: ep: 6500 \t reward: -0.769 \t loss: 0.0206 \t game len: 4.1 \t epsilon: 0.57\n",
      "2018-05-27 11:40:19.370478: ep: 6600 \t reward: -0.790 \t loss: 0.0209 \t game len: 4.2 \t epsilon: 0.57\n",
      "2018-05-27 11:40:52.092117: ep: 6700 \t reward: -0.813 \t loss: 0.0206 \t game len: 4.5 \t epsilon: 0.56\n",
      "2018-05-27 11:41:24.650632: ep: 6800 \t reward: -0.776 \t loss: 0.0199 \t game len: 3.8 \t epsilon: 0.56\n",
      "2018-05-27 11:41:57.132659: ep: 6900 \t reward: -0.816 \t loss: 0.0183 \t game len: 4.9 \t epsilon: 0.55\n",
      "2018-05-27 11:42:29.610580: ep: 7000 \t reward: -0.815 \t loss: 0.0176 \t game len: 3.7 \t epsilon: 0.55\n",
      "2018-05-27 11:43:02.064333: ep: 7100 \t reward: -0.746 \t loss: 0.0170 \t game len: 4.8 \t epsilon: 0.54\n",
      "2018-05-27 11:43:34.614907: ep: 7200 \t reward: -0.840 \t loss: 0.0179 \t game len: 5.2 \t epsilon: 0.54\n",
      "2018-05-27 11:44:07.279740: ep: 7300 \t reward: -0.770 \t loss: 0.0168 \t game len: 5.3 \t epsilon: 0.54\n",
      "2018-05-27 11:44:39.756739: ep: 7400 \t reward: -0.824 \t loss: 0.0165 \t game len: 5.6 \t epsilon: 0.53\n",
      "2018-05-27 11:45:12.372082: ep: 7500 \t reward: -0.746 \t loss: 0.0156 \t game len: 4.9 \t epsilon: 0.53\n",
      "2018-05-27 11:45:45.042972: ep: 7600 \t reward: -0.751 \t loss: 0.0162 \t game len: 4.3 \t epsilon: 0.52\n",
      "2018-05-27 11:46:17.631982: ep: 7700 \t reward: -0.756 \t loss: 0.0169 \t game len: 4.9 \t epsilon: 0.52\n",
      "2018-05-27 11:46:50.245447: ep: 7800 \t reward: -0.809 \t loss: 0.0173 \t game len: 5.1 \t epsilon: 0.51\n",
      "2018-05-27 11:47:22.764353: ep: 7900 \t reward: -0.788 \t loss: 0.0168 \t game len: 5.0 \t epsilon: 0.51\n",
      "2018-05-27 11:47:55.410565: ep: 8000 \t reward: -0.721 \t loss: 0.0176 \t game len: 5.5 \t epsilon: 0.50\n",
      "2018-05-27 11:47:58.265669: ep: 8000 \t Reward evaluation: 0.48\n",
      "2018-05-27 11:48:31.804158: ep: 8100 \t reward: -0.808 \t loss: 0.0253 \t game len: 5.0 \t epsilon: 0.49\n",
      "2018-05-27 11:49:05.390105: ep: 8200 \t reward: -0.740 \t loss: 0.0191 \t game len: 5.3 \t epsilon: 0.49\n",
      "2018-05-27 11:49:38.868111: ep: 8300 \t reward: -0.707 \t loss: 0.0191 \t game len: 6.0 \t epsilon: 0.48\n",
      "2018-05-27 11:50:12.212452: ep: 8400 \t reward: -0.825 \t loss: 0.0186 \t game len: 4.7 \t epsilon: 0.48\n",
      "2018-05-27 11:50:45.298647: ep: 8500 \t reward: -0.754 \t loss: 0.0186 \t game len: 5.7 \t epsilon: 0.47\n",
      "2018-05-27 11:51:18.523688: ep: 8600 \t reward: -0.698 \t loss: 0.0183 \t game len: 6.2 \t epsilon: 0.47\n",
      "2018-05-27 11:51:51.457581: ep: 8700 \t reward: -0.786 \t loss: 0.0191 \t game len: 5.8 \t epsilon: 0.47\n",
      "2018-05-27 11:52:24.269060: ep: 8800 \t reward: -0.750 \t loss: 0.0185 \t game len: 6.3 \t epsilon: 0.46\n",
      "2018-05-27 11:52:57.123407: ep: 8900 \t reward: -0.740 \t loss: 0.0187 \t game len: 5.3 \t epsilon: 0.46\n",
      "2018-05-27 11:53:29.970927: ep: 9000 \t reward: -0.873 \t loss: 0.0172 \t game len: 4.4 \t epsilon: 0.45\n",
      "2018-05-27 11:54:02.916517: ep: 9100 \t reward: -0.557 \t loss: 0.0169 \t game len: 6.2 \t epsilon: 0.45\n",
      "2018-05-27 11:54:35.803383: ep: 9200 \t reward: -0.662 \t loss: 0.0191 \t game len: 5.6 \t epsilon: 0.44\n",
      "2018-05-27 11:55:08.797544: ep: 9300 \t reward: -0.657 \t loss: 0.0203 \t game len: 6.2 \t epsilon: 0.43\n",
      "2018-05-27 11:55:41.710782: ep: 9400 \t reward: -0.669 \t loss: 0.0196 \t game len: 5.2 \t epsilon: 0.43\n",
      "2018-05-27 11:56:14.786882: ep: 9500 \t reward: -0.492 \t loss: 0.0226 \t game len: 6.7 \t epsilon: 0.42\n",
      "2018-05-27 11:56:47.559810: ep: 9600 \t reward: -0.660 \t loss: 0.0216 \t game len: 5.4 \t epsilon: 0.42\n",
      "2018-05-27 11:57:20.593215: ep: 9700 \t reward: -0.542 \t loss: 0.0238 \t game len: 7.7 \t epsilon: 0.41\n",
      "2018-05-27 11:57:53.612500: ep: 9800 \t reward: -0.542 \t loss: 0.0245 \t game len: 6.7 \t epsilon: 0.41\n",
      "2018-05-27 11:58:26.611283: ep: 9900 \t reward: -0.619 \t loss: 0.0235 \t game len: 6.3 \t epsilon: 0.40\n",
      "2018-05-27 11:58:59.663766: ep: 10000 \t reward: -0.620 \t loss: 0.0247 \t game len: 6.5 \t epsilon: 0.40\n",
      "2018-05-27 11:59:03.760869: ep: 10000 \t Reward evaluation: 2.12\n",
      "2018-05-27 11:59:37.532513: ep: 10100 \t reward: -0.606 \t loss: 0.0371 \t game len: 6.0 \t epsilon: 0.40\n",
      "2018-05-27 12:00:11.321132: ep: 10200 \t reward: -0.711 \t loss: 0.0290 \t game len: 6.5 \t epsilon: 0.39\n",
      "2018-05-27 12:00:45.132022: ep: 10300 \t reward: -0.638 \t loss: 0.0254 \t game len: 8.2 \t epsilon: 0.39\n",
      "2018-05-27 12:01:17.892858: ep: 10400 \t reward: -0.653 \t loss: 0.0251 \t game len: 6.7 \t epsilon: 0.38\n",
      "2018-05-27 12:01:51.407571: ep: 10500 \t reward: -0.653 \t loss: 0.0224 \t game len: 6.7 \t epsilon: 0.38\n",
      "2018-05-27 12:02:24.762117: ep: 10600 \t reward: -0.620 \t loss: 0.0230 \t game len: 7.5 \t epsilon: 0.37\n",
      "2018-05-27 12:02:57.953727: ep: 10700 \t reward: -0.627 \t loss: 0.0223 \t game len: 6.1 \t epsilon: 0.36\n",
      "2018-05-27 12:03:31.320679: ep: 10800 \t reward: -0.525 \t loss: 0.0211 \t game len: 9.1 \t epsilon: 0.36\n",
      "2018-05-27 12:04:04.636471: ep: 10900 \t reward: -0.610 \t loss: 0.0235 \t game len: 8.4 \t epsilon: 0.35\n",
      "2018-05-27 12:04:37.956718: ep: 11000 \t reward: -0.488 \t loss: 0.0219 \t game len: 7.4 \t epsilon: 0.35\n",
      "2018-05-27 12:05:11.304661: ep: 11100 \t reward: -0.498 \t loss: 0.0244 \t game len: 7.3 \t epsilon: 0.34\n",
      "2018-05-27 12:05:44.730507: ep: 11200 \t reward: -0.399 \t loss: 0.0244 \t game len: 8.5 \t epsilon: 0.34\n",
      "2018-05-27 12:06:18.091471: ep: 11300 \t reward: -0.517 \t loss: 0.0233 \t game len: 8.2 \t epsilon: 0.33\n",
      "2018-05-27 12:06:51.476046: ep: 11400 \t reward: -0.461 \t loss: 0.0240 \t game len: 8.7 \t epsilon: 0.33\n",
      "2018-05-27 12:07:24.919325: ep: 11500 \t reward: -0.533 \t loss: 0.0257 \t game len: 8.8 \t epsilon: 0.32\n",
      "2018-05-27 12:07:58.235558: ep: 11600 \t reward: -0.481 \t loss: 0.0258 \t game len: 7.7 \t epsilon: 0.32\n",
      "2018-05-27 12:08:31.830415: ep: 11700 \t reward: -0.268 \t loss: 0.0270 \t game len: 10.6 \t epsilon: 0.31\n",
      "2018-05-27 12:09:05.462861: ep: 11800 \t reward: -0.301 \t loss: 0.0273 \t game len: 10.8 \t epsilon: 0.31\n",
      "2018-05-27 12:09:38.998305: ep: 11900 \t reward: -0.431 \t loss: 0.0259 \t game len: 8.8 \t epsilon: 0.31\n",
      "2018-05-27 12:10:12.670400: ep: 12000 \t reward: -0.562 \t loss: 0.0265 \t game len: 9.8 \t epsilon: 0.30\n",
      "2018-05-27 12:10:15.985525: ep: 12000 \t Reward evaluation: 1.84\n",
      "2018-05-27 12:10:50.003483: ep: 12100 \t reward: -0.511 \t loss: 0.0358 \t game len: 8.7 \t epsilon: 0.30\n",
      "2018-05-27 12:11:23.778737: ep: 12200 \t reward: -0.561 \t loss: 0.0280 \t game len: 7.7 \t epsilon: 0.29\n",
      "2018-05-27 12:11:57.875169: ep: 12300 \t reward: -0.273 \t loss: 0.0275 \t game len: 10.2 \t epsilon: 0.29\n",
      "2018-05-27 12:12:31.704221: ep: 12400 \t reward: -0.466 \t loss: 0.0260 \t game len: 10.2 \t epsilon: 0.28\n",
      "2018-05-27 12:13:05.430917: ep: 12500 \t reward: -0.428 \t loss: 0.0259 \t game len: 10.4 \t epsilon: 0.28\n",
      "2018-05-27 12:13:39.038385: ep: 12600 \t reward: -0.402 \t loss: 0.0269 \t game len: 8.8 \t epsilon: 0.27\n",
      "2018-05-27 12:14:12.599835: ep: 12700 \t reward: -0.308 \t loss: 0.0262 \t game len: 9.5 \t epsilon: 0.27\n",
      "2018-05-27 12:14:46.054492: ep: 12800 \t reward: -0.346 \t loss: 0.0261 \t game len: 10.4 \t epsilon: 0.26\n",
      "2018-05-27 12:15:19.860836: ep: 12900 \t reward: -0.362 \t loss: 0.0269 \t game len: 11.0 \t epsilon: 0.26\n",
      "2018-05-27 12:15:53.633076: ep: 13000 \t reward: -0.305 \t loss: 0.0269 \t game len: 10.3 \t epsilon: 0.25\n",
      "2018-05-27 12:16:27.505306: ep: 13100 \t reward: -0.526 \t loss: 0.0247 \t game len: 11.2 \t epsilon: 0.24\n",
      "2018-05-27 12:17:01.471267: ep: 13200 \t reward: -0.457 \t loss: 0.0247 \t game len: 11.4 \t epsilon: 0.24\n",
      "2018-05-27 12:17:35.292198: ep: 13300 \t reward: -0.313 \t loss: 0.0251 \t game len: 11.1 \t epsilon: 0.23\n",
      "2018-05-27 12:18:09.026635: ep: 13400 \t reward: -0.293 \t loss: 0.0231 \t game len: 10.1 \t epsilon: 0.23\n",
      "2018-05-27 12:18:42.835542: ep: 13500 \t reward: -0.161 \t loss: 0.0240 \t game len: 11.0 \t epsilon: 0.22\n",
      "2018-05-27 12:19:16.798213: ep: 13600 \t reward: -0.033 \t loss: 0.0268 \t game len: 13.4 \t epsilon: 0.22\n",
      "2018-05-27 12:19:50.766586: ep: 13700 \t reward: -0.244 \t loss: 0.0298 \t game len: 12.2 \t epsilon: 0.21\n",
      "2018-05-27 12:20:24.712448: ep: 13800 \t reward: -0.144 \t loss: 0.0296 \t game len: 11.4 \t epsilon: 0.21\n",
      "2018-05-27 12:20:58.853016: ep: 13900 \t reward: 0.062 \t loss: 0.0297 \t game len: 14.9 \t epsilon: 0.20\n",
      "2018-05-27 12:21:32.972309: ep: 14000 \t reward: -0.146 \t loss: 0.0306 \t game len: 12.6 \t epsilon: 0.20\n",
      "2018-05-27 12:21:36.115715: ep: 14000 \t Reward evaluation: 1.76\n",
      "2018-05-27 12:22:10.559251: ep: 14100 \t reward: -0.175 \t loss: 0.0350 \t game len: 12.5 \t epsilon: 0.19\n",
      "2018-05-27 12:22:45.157324: ep: 14200 \t reward: -0.130 \t loss: 0.0317 \t game len: 16.0 \t epsilon: 0.19\n",
      "2018-05-27 12:23:19.653222: ep: 14300 \t reward: 0.161 \t loss: 0.0300 \t game len: 16.2 \t epsilon: 0.18\n",
      "2018-05-27 12:23:53.876412: ep: 14400 \t reward: -0.075 \t loss: 0.0291 \t game len: 13.6 \t epsilon: 0.18\n",
      "2018-05-27 12:24:28.340613: ep: 14500 \t reward: 0.063 \t loss: 0.0284 \t game len: 16.9 \t epsilon: 0.17\n",
      "2018-05-27 12:25:02.581380: ep: 14600 \t reward: 0.014 \t loss: 0.0292 \t game len: 13.7 \t epsilon: 0.17\n",
      "2018-05-27 12:25:36.852532: ep: 14700 \t reward: -0.013 \t loss: 0.0278 \t game len: 14.4 \t epsilon: 0.17\n",
      "2018-05-27 12:26:11.293208: ep: 14800 \t reward: 0.058 \t loss: 0.0284 \t game len: 16.4 \t epsilon: 0.16\n",
      "2018-05-27 12:26:45.771616: ep: 14900 \t reward: 0.367 \t loss: 0.0313 \t game len: 15.8 \t epsilon: 0.16\n",
      "2018-05-27 12:27:20.620648: ep: 15000 \t reward: 0.187 \t loss: 0.0298 \t game len: 20.6 \t epsilon: 0.15\n",
      "2018-05-27 12:27:55.636862: ep: 15100 \t reward: 0.361 \t loss: 0.0296 \t game len: 20.5 \t epsilon: 0.15\n",
      "2018-05-27 12:28:30.029499: ep: 15200 \t reward: 0.366 \t loss: 0.0283 \t game len: 18.9 \t epsilon: 0.14\n",
      "2018-05-27 12:29:05.114737: ep: 15300 \t reward: 0.450 \t loss: 0.0301 \t game len: 22.6 \t epsilon: 0.14\n",
      "2018-05-27 12:29:40.153685: ep: 15400 \t reward: 0.481 \t loss: 0.0303 \t game len: 22.6 \t epsilon: 0.13\n",
      "2018-05-27 12:30:14.968069: ep: 15500 \t reward: 0.247 \t loss: 0.0293 \t game len: 20.7 \t epsilon: 0.12\n",
      "2018-05-27 12:30:50.211916: ep: 15600 \t reward: 0.773 \t loss: 0.0277 \t game len: 23.6 \t epsilon: 0.12\n",
      "2018-05-27 12:31:25.636782: ep: 15700 \t reward: 1.005 \t loss: 0.0311 \t game len: 26.7 \t epsilon: 0.11\n",
      "2018-05-27 12:32:01.023434: ep: 15800 \t reward: 0.674 \t loss: 0.0331 \t game len: 24.5 \t epsilon: 0.11\n",
      "2018-05-27 12:32:36.288588: ep: 15900 \t reward: 0.330 \t loss: 0.0315 \t game len: 23.6 \t epsilon: 0.10\n",
      "2018-05-27 12:33:11.693904: ep: 16000 \t reward: 0.607 \t loss: 0.0273 \t game len: 26.1 \t epsilon: 0.10\n",
      "2018-05-27 12:33:14.866298: ep: 16000 \t Reward evaluation: 2.23\n",
      "2018-05-27 12:33:50.567017: ep: 16100 \t reward: 0.926 \t loss: 0.0360 \t game len: 27.6 \t epsilon: 0.09\n",
      "2018-05-27 12:34:25.974671: ep: 16200 \t reward: 0.753 \t loss: 0.0310 \t game len: 25.7 \t epsilon: 0.09\n",
      "2018-05-27 12:35:01.790541: ep: 16300 \t reward: 1.208 \t loss: 0.0320 \t game len: 29.6 \t epsilon: 0.08\n",
      "2018-05-27 12:35:37.659037: ep: 16400 \t reward: 1.281 \t loss: 0.0352 \t game len: 30.4 \t epsilon: 0.08\n",
      "2018-05-27 12:36:14.546281: ep: 16500 \t reward: 2.047 \t loss: 0.0335 \t game len: 41.7 \t epsilon: 0.07\n",
      "2018-05-27 12:36:51.353645: ep: 16600 \t reward: 1.760 \t loss: 0.0339 \t game len: 40.1 \t epsilon: 0.07\n",
      "2018-05-27 12:37:27.758398: ep: 16700 \t reward: 1.668 \t loss: 0.0321 \t game len: 38.2 \t epsilon: 0.06\n",
      "2018-05-27 12:38:04.580463: ep: 16800 \t reward: 1.848 \t loss: 0.0322 \t game len: 42.4 \t epsilon: 0.06\n",
      "2018-05-27 12:38:41.319136: ep: 16900 \t reward: 1.631 \t loss: 0.0315 \t game len: 39.9 \t epsilon: 0.05\n",
      "2018-05-27 12:39:18.025285: ep: 17000 \t reward: 2.145 \t loss: 0.0317 \t game len: 41.0 \t epsilon: 0.05\n",
      "2018-05-27 12:39:55.767554: ep: 17100 \t reward: 2.363 \t loss: 0.0329 \t game len: 51.5 \t epsilon: 0.05\n",
      "2018-05-27 12:40:33.841434: ep: 17200 \t reward: 1.902 \t loss: 0.0317 \t game len: 57.2 \t epsilon: 0.05\n",
      "2018-05-27 12:41:11.193544: ep: 17300 \t reward: 1.953 \t loss: 0.0262 \t game len: 46.0 \t epsilon: 0.05\n",
      "2018-05-27 12:41:48.431138: ep: 17400 \t reward: 2.154 \t loss: 0.0337 \t game len: 52.2 \t epsilon: 0.05\n",
      "2018-05-27 12:42:25.237319: ep: 17500 \t reward: 1.799 \t loss: 0.0319 \t game len: 43.3 \t epsilon: 0.05\n",
      "2018-05-27 12:43:02.595944: ep: 17600 \t reward: 2.031 \t loss: 0.0295 \t game len: 48.4 \t epsilon: 0.05\n",
      "2018-05-27 12:43:39.695140: ep: 17700 \t reward: 1.957 \t loss: 0.0343 \t game len: 47.7 \t epsilon: 0.05\n",
      "2018-05-27 12:44:17.247904: ep: 17800 \t reward: 2.676 \t loss: 0.0341 \t game len: 51.5 \t epsilon: 0.05\n",
      "2018-05-27 12:44:55.275939: ep: 17900 \t reward: 1.951 \t loss: 0.0321 \t game len: 54.4 \t epsilon: 0.05\n",
      "2018-05-27 12:45:32.965361: ep: 18000 \t reward: 2.413 \t loss: 0.0283 \t game len: 53.6 \t epsilon: 0.05\n",
      "2018-05-27 12:45:37.225169: ep: 18000 \t Reward evaluation: 2.87\n",
      "2018-05-27 12:46:14.465434: ep: 18100 \t reward: 1.629 \t loss: 0.0342 \t game len: 47.1 \t epsilon: 0.05\n",
      "2018-05-27 12:46:52.657958: ep: 18200 \t reward: 3.185 \t loss: 0.0311 \t game len: 56.2 \t epsilon: 0.05\n",
      "2018-05-27 12:47:29.972606: ep: 18300 \t reward: 2.505 \t loss: 0.0343 \t game len: 46.4 \t epsilon: 0.05\n",
      "2018-05-27 12:48:08.148051: ep: 18400 \t reward: 3.181 \t loss: 0.0388 \t game len: 56.6 \t epsilon: 0.05\n",
      "2018-05-27 12:48:46.036783: ep: 18500 \t reward: 2.444 \t loss: 0.0330 \t game len: 52.5 \t epsilon: 0.05\n",
      "2018-05-27 12:49:23.652373: ep: 18600 \t reward: 2.666 \t loss: 0.0325 \t game len: 51.5 \t epsilon: 0.05\n",
      "2018-05-27 12:50:01.501344: ep: 18700 \t reward: 3.125 \t loss: 0.0310 \t game len: 53.1 \t epsilon: 0.05\n",
      "2018-05-27 12:50:39.025912: ep: 18800 \t reward: 3.036 \t loss: 0.0365 \t game len: 51.9 \t epsilon: 0.05\n",
      "2018-05-27 12:51:16.296715: ep: 18900 \t reward: 3.112 \t loss: 0.0359 \t game len: 48.3 \t epsilon: 0.05\n",
      "2018-05-27 12:51:54.260085: ep: 19000 \t reward: 2.979 \t loss: 0.0391 \t game len: 55.6 \t epsilon: 0.05\n",
      "2018-05-27 12:52:31.852882: ep: 19100 \t reward: 2.631 \t loss: 0.0320 \t game len: 50.0 \t epsilon: 0.05\n",
      "2018-05-27 12:53:09.601750: ep: 19200 \t reward: 2.212 \t loss: 0.0348 \t game len: 51.4 \t epsilon: 0.05\n",
      "2018-05-27 12:53:46.525860: ep: 19300 \t reward: 1.601 \t loss: 0.0270 \t game len: 41.9 \t epsilon: 0.05\n",
      "2018-05-27 12:54:23.716979: ep: 19400 \t reward: 2.653 \t loss: 0.0340 \t game len: 47.8 \t epsilon: 0.05\n",
      "2018-05-27 12:55:01.321796: ep: 19500 \t reward: 1.832 \t loss: 0.0315 \t game len: 51.0 \t epsilon: 0.05\n",
      "2018-05-27 12:55:38.269250: ep: 19600 \t reward: 2.388 \t loss: 0.0309 \t game len: 47.0 \t epsilon: 0.05\n",
      "2018-05-27 12:56:15.525713: ep: 19700 \t reward: 1.657 \t loss: 0.0299 \t game len: 46.4 \t epsilon: 0.05\n",
      "2018-05-27 12:56:53.338440: ep: 19800 \t reward: 3.297 \t loss: 0.0333 \t game len: 53.1 \t epsilon: 0.05\n",
      "2018-05-27 12:57:31.272339: ep: 19900 \t reward: 2.548 \t loss: 0.0339 \t game len: 56.2 \t epsilon: 0.05\n",
      "2018-05-27 12:58:08.403137: ep: 20000 \t reward: 2.441 \t loss: 0.0302 \t game len: 45.8 \t epsilon: 0.05\n",
      "2018-05-27 12:58:13.037400: ep: 20000 \t Reward evaluation: 3.89\n",
      "2018-05-27 12:58:50.164353: ep: 20100 \t reward: 2.856 \t loss: 0.0408 \t game len: 44.6 \t epsilon: 0.05\n",
      "2018-05-27 12:59:27.818451: ep: 20200 \t reward: 2.580 \t loss: 0.0372 \t game len: 51.0 \t epsilon: 0.05\n",
      "2018-05-27 13:00:05.723827: ep: 20300 \t reward: 3.365 \t loss: 0.0354 \t game len: 54.4 \t epsilon: 0.05\n",
      "2018-05-27 13:00:43.125301: ep: 20400 \t reward: 1.554 \t loss: 0.0358 \t game len: 49.5 \t epsilon: 0.05\n",
      "2018-05-27 13:01:20.545807: ep: 20500 \t reward: 1.974 \t loss: 0.0243 \t game len: 52.0 \t epsilon: 0.05\n",
      "2018-05-27 13:01:57.824666: ep: 20600 \t reward: 2.234 \t loss: 0.0338 \t game len: 48.2 \t epsilon: 0.05\n",
      "2018-05-27 13:02:35.546936: ep: 20700 \t reward: 2.265 \t loss: 0.0291 \t game len: 53.3 \t epsilon: 0.05\n",
      "2018-05-27 13:03:13.563995: ep: 20800 \t reward: 2.671 \t loss: 0.0355 \t game len: 55.0 \t epsilon: 0.05\n",
      "2018-05-27 13:03:51.400126: ep: 20900 \t reward: 2.733 \t loss: 0.0297 \t game len: 52.9 \t epsilon: 0.05\n",
      "2018-05-27 13:04:28.426717: ep: 21000 \t reward: 2.467 \t loss: 0.0368 \t game len: 44.1 \t epsilon: 0.05\n",
      "2018-05-27 13:05:05.976050: ep: 21100 \t reward: 2.937 \t loss: 0.0375 \t game len: 51.7 \t epsilon: 0.05\n",
      "2018-05-27 13:05:43.498845: ep: 21200 \t reward: 2.910 \t loss: 0.0329 \t game len: 49.3 \t epsilon: 0.05\n",
      "2018-05-27 13:06:21.069074: ep: 21300 \t reward: 2.348 \t loss: 0.0350 \t game len: 49.0 \t epsilon: 0.05\n",
      "2018-05-27 13:06:58.260683: ep: 21400 \t reward: 2.418 \t loss: 0.0326 \t game len: 46.0 \t epsilon: 0.05\n",
      "2018-05-27 13:07:35.293397: ep: 21500 \t reward: 2.494 \t loss: 0.0357 \t game len: 43.5 \t epsilon: 0.05\n",
      "2018-05-27 13:08:12.682343: ep: 21600 \t reward: 3.268 \t loss: 0.0400 \t game len: 48.9 \t epsilon: 0.05\n",
      "2018-05-27 13:08:49.442303: ep: 21700 \t reward: 2.462 \t loss: 0.0416 \t game len: 43.7 \t epsilon: 0.05\n",
      "2018-05-27 13:09:26.293873: ep: 21800 \t reward: 2.737 \t loss: 0.0360 \t game len: 47.5 \t epsilon: 0.05\n",
      "2018-05-27 13:10:04.059127: ep: 21900 \t reward: 2.432 \t loss: 0.0339 \t game len: 54.7 \t epsilon: 0.05\n",
      "2018-05-27 13:10:41.652384: ep: 22000 \t reward: 2.685 \t loss: 0.0282 \t game len: 50.6 \t epsilon: 0.05\n",
      "2018-05-27 13:10:45.606139: ep: 22000 \t Reward evaluation: 3.04\n",
      "2018-05-27 13:11:23.154007: ep: 22100 \t reward: 2.447 \t loss: 0.0367 \t game len: 51.2 \t epsilon: 0.05\n",
      "2018-05-27 13:12:01.229232: ep: 22200 \t reward: 2.740 \t loss: 0.0313 \t game len: 57.2 \t epsilon: 0.05\n",
      "2018-05-27 13:12:38.940546: ep: 22300 \t reward: 2.259 \t loss: 0.0327 \t game len: 51.7 \t epsilon: 0.05\n",
      "2018-05-27 13:13:16.782121: ep: 22400 \t reward: 2.906 \t loss: 0.0290 \t game len: 54.8 \t epsilon: 0.05\n",
      "2018-05-27 13:13:54.232408: ep: 22500 \t reward: 2.845 \t loss: 0.0326 \t game len: 48.7 \t epsilon: 0.05\n",
      "2018-05-27 13:14:32.750943: ep: 22600 \t reward: 3.934 \t loss: 0.0348 \t game len: 64.1 \t epsilon: 0.05\n",
      "2018-05-27 13:15:10.486102: ep: 22700 \t reward: 2.763 \t loss: 0.0345 \t game len: 55.0 \t epsilon: 0.05\n",
      "2018-05-27 13:15:48.040968: ep: 22800 \t reward: 2.846 \t loss: 0.0343 \t game len: 50.6 \t epsilon: 0.05\n",
      "2018-05-27 13:16:26.472920: ep: 22900 \t reward: 3.678 \t loss: 0.0357 \t game len: 61.4 \t epsilon: 0.05\n",
      "2018-05-27 13:17:03.726773: ep: 23000 \t reward: 2.159 \t loss: 0.0360 \t game len: 47.7 \t epsilon: 0.05\n",
      "2018-05-27 13:17:40.634433: ep: 23100 \t reward: 2.614 \t loss: 0.0345 \t game len: 45.6 \t epsilon: 0.05\n",
      "2018-05-27 13:18:18.258748: ep: 23200 \t reward: 2.857 \t loss: 0.0357 \t game len: 51.6 \t epsilon: 0.05\n",
      "2018-05-27 13:18:56.121144: ep: 23300 \t reward: 3.261 \t loss: 0.0337 \t game len: 55.6 \t epsilon: 0.05\n",
      "2018-05-27 13:19:34.216444: ep: 23400 \t reward: 3.124 \t loss: 0.0383 \t game len: 58.2 \t epsilon: 0.05\n",
      "2018-05-27 13:20:11.978410: ep: 23500 \t reward: 3.085 \t loss: 0.0350 \t game len: 54.1 \t epsilon: 0.05\n",
      "2018-05-27 13:20:49.412939: ep: 23600 \t reward: 2.593 \t loss: 0.0324 \t game len: 51.8 \t epsilon: 0.05\n",
      "2018-05-27 13:21:27.278651: ep: 23700 \t reward: 2.626 \t loss: 0.0335 \t game len: 54.5 \t epsilon: 0.05\n",
      "2018-05-27 13:22:05.701972: ep: 23800 \t reward: 3.327 \t loss: 0.0335 \t game len: 60.1 \t epsilon: 0.05\n",
      "2018-05-27 13:22:43.180080: ep: 23900 \t reward: 2.320 \t loss: 0.0329 \t game len: 52.8 \t epsilon: 0.05\n",
      "2018-05-27 13:23:20.417129: ep: 24000 \t reward: 2.552 \t loss: 0.0308 \t game len: 47.7 \t epsilon: 0.05\n",
      "2018-05-27 13:23:24.194069: ep: 24000 \t Reward evaluation: 3.41\n",
      "2018-05-27 13:24:01.573723: ep: 24100 \t reward: 2.066 \t loss: 0.0371 \t game len: 48.9 \t epsilon: 0.05\n",
      "2018-05-27 13:24:38.705252: ep: 24200 \t reward: 2.211 \t loss: 0.0312 \t game len: 46.6 \t epsilon: 0.05\n",
      "2018-05-27 13:25:16.336608: ep: 24300 \t reward: 2.784 \t loss: 0.0310 \t game len: 52.9 \t epsilon: 0.05\n",
      "2018-05-27 13:25:53.792413: ep: 24400 \t reward: 2.945 \t loss: 0.0339 \t game len: 48.8 \t epsilon: 0.05\n",
      "2018-05-27 13:26:31.260783: ep: 24500 \t reward: 3.521 \t loss: 0.0420 \t game len: 49.9 \t epsilon: 0.05\n",
      "2018-05-27 13:27:08.384522: ep: 24600 \t reward: 2.731 \t loss: 0.0383 \t game len: 47.0 \t epsilon: 0.05\n",
      "2018-05-27 13:27:46.007767: ep: 24700 \t reward: 2.832 \t loss: 0.0369 \t game len: 52.1 \t epsilon: 0.05\n",
      "2018-05-27 13:28:23.297670: ep: 24800 \t reward: 2.524 \t loss: 0.0301 \t game len: 47.5 \t epsilon: 0.05\n",
      "2018-05-27 13:29:00.013786: ep: 24900 \t reward: 2.903 \t loss: 0.0381 \t game len: 42.0 \t epsilon: 0.05\n",
      "2018-05-27 13:29:37.245981: ep: 25000 \t reward: 2.984 \t loss: 0.0400 \t game len: 49.0 \t epsilon: 0.05\n",
      "2018-05-27 13:30:14.553412: ep: 25100 \t reward: 2.571 \t loss: 0.0368 \t game len: 47.9 \t epsilon: 0.05\n",
      "2018-05-27 13:30:51.808668: ep: 25200 \t reward: 2.525 \t loss: 0.0360 \t game len: 47.5 \t epsilon: 0.05\n",
      "2018-05-27 13:31:29.055961: ep: 25300 \t reward: 2.240 \t loss: 0.0303 \t game len: 49.6 \t epsilon: 0.05\n",
      "2018-05-27 13:32:06.171235: ep: 25400 \t reward: 2.745 \t loss: 0.0365 \t game len: 46.6 \t epsilon: 0.05\n",
      "2018-05-27 13:32:43.028800: ep: 25500 \t reward: 3.107 \t loss: 0.0380 \t game len: 44.7 \t epsilon: 0.05\n",
      "2018-05-27 13:33:20.441842: ep: 25600 \t reward: 3.162 \t loss: 0.0405 \t game len: 49.4 \t epsilon: 0.05\n",
      "2018-05-27 13:33:58.171511: ep: 25700 \t reward: 3.297 \t loss: 0.0381 \t game len: 54.0 \t epsilon: 0.05\n",
      "2018-05-27 13:34:35.548352: ep: 25800 \t reward: 2.723 \t loss: 0.0335 \t game len: 48.8 \t epsilon: 0.05\n",
      "2018-05-27 13:35:12.669238: ep: 25900 \t reward: 2.870 \t loss: 0.0362 \t game len: 46.2 \t epsilon: 0.05\n",
      "2018-05-27 13:35:49.743264: ep: 26000 \t reward: 2.986 \t loss: 0.0375 \t game len: 45.8 \t epsilon: 0.05\n",
      "2018-05-27 13:35:53.269735: ep: 26000 \t Reward evaluation: 1.88\n",
      "2018-05-27 13:36:30.337581: ep: 26100 \t reward: 2.400 \t loss: 0.0328 \t game len: 48.8 \t epsilon: 0.05\n",
      "2018-05-27 13:37:07.480229: ep: 26200 \t reward: 2.725 \t loss: 0.0343 \t game len: 45.6 \t epsilon: 0.05\n",
      "2018-05-27 13:37:44.513625: ep: 26300 \t reward: 3.033 \t loss: 0.0386 \t game len: 44.1 \t epsilon: 0.05\n",
      "2018-05-27 13:38:22.053754: ep: 26400 \t reward: 3.758 \t loss: 0.0406 \t game len: 50.4 \t epsilon: 0.05\n",
      "2018-05-27 13:38:59.616446: ep: 26500 \t reward: 2.742 \t loss: 0.0378 \t game len: 52.0 \t epsilon: 0.05\n",
      "2018-05-27 13:39:37.276060: ep: 26600 \t reward: 3.353 \t loss: 0.0348 \t game len: 51.5 \t epsilon: 0.05\n",
      "2018-05-27 13:40:15.219456: ep: 26700 \t reward: 3.128 \t loss: 0.0395 \t game len: 55.8 \t epsilon: 0.05\n",
      "2018-05-27 13:40:52.295970: ep: 26800 \t reward: 2.270 \t loss: 0.0362 \t game len: 43.6 \t epsilon: 0.05\n",
      "2018-05-27 13:41:30.269393: ep: 26900 \t reward: 3.167 \t loss: 0.0347 \t game len: 58.0 \t epsilon: 0.05\n",
      "2018-05-27 13:42:07.306469: ep: 27000 \t reward: 2.654 \t loss: 0.0337 \t game len: 44.6 \t epsilon: 0.05\n",
      "2018-05-27 13:42:44.526481: ep: 27100 \t reward: 3.042 \t loss: 0.0362 \t game len: 48.3 \t epsilon: 0.05\n",
      "2018-05-27 13:43:22.175110: ep: 27200 \t reward: 2.970 \t loss: 0.0347 \t game len: 52.5 \t epsilon: 0.05\n",
      "2018-05-27 13:44:00.297461: ep: 27300 \t reward: 2.844 \t loss: 0.0368 \t game len: 56.9 \t epsilon: 0.05\n",
      "2018-05-27 13:44:37.880476: ep: 27400 \t reward: 3.630 \t loss: 0.0360 \t game len: 52.1 \t epsilon: 0.05\n",
      "2018-05-27 13:45:15.141069: ep: 27500 \t reward: 2.851 \t loss: 0.0397 \t game len: 48.2 \t epsilon: 0.05\n",
      "2018-05-27 13:45:52.548405: ep: 27600 \t reward: 3.387 \t loss: 0.0357 \t game len: 49.0 \t epsilon: 0.05\n",
      "2018-05-27 13:46:29.736620: ep: 27700 \t reward: 3.289 \t loss: 0.0387 \t game len: 46.8 \t epsilon: 0.05\n",
      "2018-05-27 13:47:06.596389: ep: 27800 \t reward: 2.882 \t loss: 0.0405 \t game len: 43.0 \t epsilon: 0.05\n",
      "2018-05-27 13:47:43.836536: ep: 27900 \t reward: 3.552 \t loss: 0.0395 \t game len: 47.8 \t epsilon: 0.05\n",
      "2018-05-27 13:48:21.867646: ep: 28000 \t reward: 3.626 \t loss: 0.0405 \t game len: 58.5 \t epsilon: 0.05\n",
      "2018-05-27 13:48:25.517894: ep: 28000 \t Reward evaluation: 3.78\n",
      "2018-05-27 13:49:02.920361: ep: 28100 \t reward: 3.112 \t loss: 0.0376 \t game len: 51.4 \t epsilon: 0.05\n",
      "2018-05-27 13:49:40.195274: ep: 28200 \t reward: 3.465 \t loss: 0.0379 \t game len: 51.4 \t epsilon: 0.05\n",
      "2018-05-27 13:50:17.959566: ep: 28300 \t reward: 3.599 \t loss: 0.0404 \t game len: 54.2 \t epsilon: 0.05\n",
      "2018-05-27 13:50:56.060421: ep: 28400 \t reward: 2.434 \t loss: 0.0327 \t game len: 57.5 \t epsilon: 0.05\n",
      "2018-05-27 13:51:33.091527: ep: 28500 \t reward: 2.580 \t loss: 0.0300 \t game len: 45.0 \t epsilon: 0.05\n",
      "2018-05-27 13:52:10.115301: ep: 28600 \t reward: 3.412 \t loss: 0.0402 \t game len: 42.5 \t epsilon: 0.05\n",
      "2018-05-27 13:52:47.149156: ep: 28700 \t reward: 2.956 \t loss: 0.0412 \t game len: 44.7 \t epsilon: 0.05\n",
      "2018-05-27 13:53:25.089307: ep: 28800 \t reward: 3.591 \t loss: 0.0379 \t game len: 54.0 \t epsilon: 0.05\n",
      "2018-05-27 13:54:02.397396: ep: 28900 \t reward: 2.987 \t loss: 0.0413 \t game len: 47.8 \t epsilon: 0.05\n",
      "2018-05-27 13:54:39.728602: ep: 29000 \t reward: 2.727 \t loss: 0.0349 \t game len: 49.5 \t epsilon: 0.05\n",
      "2018-05-27 13:55:17.551735: ep: 29100 \t reward: 3.150 \t loss: 0.0347 \t game len: 53.6 \t epsilon: 0.05\n",
      "2018-05-27 13:55:55.251395: ep: 29200 \t reward: 2.505 \t loss: 0.0336 \t game len: 52.4 \t epsilon: 0.05\n",
      "2018-05-27 13:56:32.813284: ep: 29300 \t reward: 3.422 \t loss: 0.0337 \t game len: 51.7 \t epsilon: 0.05\n",
      "2018-05-27 13:57:10.506874: ep: 29400 \t reward: 3.059 \t loss: 0.0371 \t game len: 53.7 \t epsilon: 0.05\n",
      "2018-05-27 13:57:48.226377: ep: 29500 \t reward: 3.683 \t loss: 0.0387 \t game len: 52.8 \t epsilon: 0.05\n",
      "2018-05-27 13:58:25.197121: ep: 29600 \t reward: 3.028 \t loss: 0.0389 \t game len: 43.6 \t epsilon: 0.05\n",
      "2018-05-27 13:59:03.900774: ep: 29700 \t reward: 4.952 \t loss: 0.0397 \t game len: 63.2 \t epsilon: 0.05\n",
      "2018-05-27 13:59:40.992975: ep: 29800 \t reward: 2.691 \t loss: 0.0489 \t game len: 43.9 \t epsilon: 0.05\n",
      "2018-05-27 14:00:18.471799: ep: 29900 \t reward: 3.232 \t loss: 0.0387 \t game len: 52.5 \t epsilon: 0.05\n",
      "2018-05-27 14:00:55.911418: ep: 30000 \t reward: 3.598 \t loss: 0.0382 \t game len: 50.2 \t epsilon: 0.05\n",
      "2018-05-27 14:00:59.965016: ep: 30000 \t Reward evaluation: 2.71\n",
      "2018-05-27 14:01:37.329647: ep: 30100 \t reward: 3.562 \t loss: 0.0394 \t game len: 48.8 \t epsilon: 0.05\n",
      "2018-05-27 14:02:14.738528: ep: 30200 \t reward: 3.611 \t loss: 0.0414 \t game len: 48.9 \t epsilon: 0.05\n",
      "2018-05-27 14:02:52.509854: ep: 30300 \t reward: 3.388 \t loss: 0.0384 \t game len: 54.1 \t epsilon: 0.05\n",
      "2018-05-27 14:03:29.786876: ep: 30400 \t reward: 3.430 \t loss: 0.0392 \t game len: 51.9 \t epsilon: 0.05\n",
      "2018-05-27 14:04:07.154551: ep: 30500 \t reward: 2.513 \t loss: 0.0328 \t game len: 49.6 \t epsilon: 0.05\n",
      "2018-05-27 14:04:44.687904: ep: 30600 \t reward: 2.958 \t loss: 0.0367 \t game len: 49.5 \t epsilon: 0.05\n",
      "2018-05-27 14:05:21.879565: ep: 30700 \t reward: 3.134 \t loss: 0.0360 \t game len: 48.1 \t epsilon: 0.05\n",
      "2018-05-27 14:05:59.118404: ep: 30800 \t reward: 2.957 \t loss: 0.0364 \t game len: 47.7 \t epsilon: 0.05\n",
      "2018-05-27 14:06:36.176501: ep: 30900 \t reward: 3.482 \t loss: 0.0396 \t game len: 45.6 \t epsilon: 0.05\n",
      "2018-05-27 14:07:13.836588: ep: 31000 \t reward: 3.684 \t loss: 0.0426 \t game len: 51.7 \t epsilon: 0.05\n",
      "2018-05-27 14:07:51.868268: ep: 31100 \t reward: 3.199 \t loss: 0.0376 \t game len: 54.8 \t epsilon: 0.05\n",
      "2018-05-27 14:08:29.088272: ep: 31200 \t reward: 3.038 \t loss: 0.0365 \t game len: 47.6 \t epsilon: 0.05\n",
      "2018-05-27 14:09:07.004504: ep: 31300 \t reward: 3.816 \t loss: 0.0363 \t game len: 55.7 \t epsilon: 0.05\n",
      "2018-05-27 14:09:44.558676: ep: 31400 \t reward: 3.272 \t loss: 0.0409 \t game len: 51.5 \t epsilon: 0.05\n",
      "2018-05-27 14:10:21.691483: ep: 31500 \t reward: 3.071 \t loss: 0.0388 \t game len: 46.4 \t epsilon: 0.05\n",
      "2018-05-27 14:10:59.218128: ep: 31600 \t reward: 3.311 \t loss: 0.0409 \t game len: 51.7 \t epsilon: 0.05\n",
      "2018-05-27 14:11:36.644439: ep: 31700 \t reward: 3.360 \t loss: 0.0373 \t game len: 51.8 \t epsilon: 0.05\n",
      "2018-05-27 14:12:14.058671: ep: 31800 \t reward: 3.563 \t loss: 0.0393 \t game len: 49.6 \t epsilon: 0.05\n",
      "2018-05-27 14:12:51.734203: ep: 31900 \t reward: 4.008 \t loss: 0.0394 \t game len: 53.6 \t epsilon: 0.05\n",
      "2018-05-27 14:13:28.713730: ep: 32000 \t reward: 2.988 \t loss: 0.0434 \t game len: 45.6 \t epsilon: 0.05\n",
      "2018-05-27 14:13:32.378614: ep: 32000 \t Reward evaluation: 3.02\n",
      "2018-05-27 14:14:09.352267: ep: 32100 \t reward: 2.698 \t loss: 0.0365 \t game len: 44.3 \t epsilon: 0.05\n",
      "2018-05-27 14:14:46.082101: ep: 32200 \t reward: 2.526 \t loss: 0.0359 \t game len: 42.3 \t epsilon: 0.05\n",
      "2018-05-27 14:15:23.367855: ep: 32300 \t reward: 3.077 \t loss: 0.0361 \t game len: 50.9 \t epsilon: 0.05\n",
      "2018-05-27 14:16:00.571181: ep: 32400 \t reward: 2.433 \t loss: 0.0377 \t game len: 46.5 \t epsilon: 0.05\n",
      "2018-05-27 14:16:37.861485: ep: 32500 \t reward: 2.908 \t loss: 0.0318 \t game len: 48.5 \t epsilon: 0.05\n",
      "2018-05-27 14:17:14.691668: ep: 32600 \t reward: 3.500 \t loss: 0.0373 \t game len: 50.0 \t epsilon: 0.05\n",
      "2018-05-27 14:17:52.171746: ep: 32700 \t reward: 3.406 \t loss: 0.0420 \t game len: 49.2 \t epsilon: 0.05\n",
      "2018-05-27 14:18:30.024443: ep: 32800 \t reward: 3.810 \t loss: 0.0371 \t game len: 56.2 \t epsilon: 0.05\n",
      "2018-05-27 14:19:07.243747: ep: 32900 \t reward: 3.599 \t loss: 0.0396 \t game len: 46.1 \t epsilon: 0.05\n",
      "2018-05-27 14:19:44.636951: ep: 33000 \t reward: 3.495 \t loss: 0.0465 \t game len: 49.4 \t epsilon: 0.05\n",
      "2018-05-27 14:20:21.642063: ep: 33100 \t reward: 3.237 \t loss: 0.0397 \t game len: 44.8 \t epsilon: 0.05\n",
      "2018-05-27 14:20:58.544418: ep: 33200 \t reward: 2.933 \t loss: 0.0416 \t game len: 42.0 \t epsilon: 0.05\n",
      "2018-05-27 14:21:35.374027: ep: 33300 \t reward: 2.895 \t loss: 0.0394 \t game len: 42.8 \t epsilon: 0.05\n",
      "2018-05-27 14:22:12.876804: ep: 33400 \t reward: 3.330 \t loss: 0.0390 \t game len: 51.8 \t epsilon: 0.05\n",
      "2018-05-27 14:22:50.406678: ep: 33500 \t reward: 3.597 \t loss: 0.0400 \t game len: 50.3 \t epsilon: 0.05\n",
      "2018-05-27 14:23:28.540370: ep: 33600 \t reward: 3.283 \t loss: 0.0407 \t game len: 56.5 \t epsilon: 0.05\n",
      "2018-05-27 14:24:06.051533: ep: 33700 \t reward: 3.425 \t loss: 0.0338 \t game len: 50.4 \t epsilon: 0.05\n",
      "2018-05-27 14:24:43.901622: ep: 33800 \t reward: 3.618 \t loss: 0.0381 \t game len: 54.3 \t epsilon: 0.05\n",
      "2018-05-27 14:25:21.003293: ep: 33900 \t reward: 2.994 \t loss: 0.0396 \t game len: 45.0 \t epsilon: 0.05\n",
      "2018-05-27 14:25:57.911232: ep: 34000 \t reward: 2.635 \t loss: 0.0404 \t game len: 43.5 \t epsilon: 0.05\n",
      "2018-05-27 14:26:02.109985: ep: 34000 \t Reward evaluation: 2.40\n",
      "2018-05-27 14:26:40.127520: ep: 34100 \t reward: 3.792 \t loss: 0.0322 \t game len: 57.0 \t epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "REPORT_INTERVAL = 100\n",
    "EVAL_INTERVAL = 2000\n",
    "R = []\n",
    "L = []\n",
    "play_length = []\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "decay = 0.1/2000\n",
    "start_ep = 0\n",
    "\n",
    "for ep in range(100000):\n",
    "    \n",
    "    # Play one game:\n",
    "    epsilon = max(EPS_START - decay*(ep), EPS_END)\n",
    "    r, i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "    R.append(r)\n",
    "    play_length.append(i)\n",
    "    \n",
    "    # Train:\n",
    "    for _ in range(10):\n",
    "        l = train_batch()\n",
    "        L.append(float(l))\n",
    "    \n",
    "    if ep % REPORT_INTERVAL == 0:\n",
    "        print(\"%s: ep: %s \\t reward: %.3f \\t loss: %.4f \\t game len: %.1f \\t epsilon: %.2f\" % \n",
    "              (str(datetime.datetime.now()), ep, np.mean(R), np.mean(L), np.mean(play_length), epsilon))\n",
    "        R = []\n",
    "        L = []\n",
    "        play_length = []\n",
    "    \n",
    "    if ep % EVAL_INTERVAL == 0:\n",
    "        eval_reward = evaluate_agent()\n",
    "        save_checkpoint()\n",
    "        print(\"%s: ep: %s \\t Reward evaluation: %.2f\" % (str(datetime.datetime.now()), ep, eval_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98510"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training time now:\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.785280000000004"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate agent with 5% epsilon greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.315360000000009"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate agent with greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.9900000000000014, 199)\n",
      "(13.100000000000016, 105)\n",
      "(-0.9900000000000014, 199)\n",
      "(8.760000000000005, 32)\n",
      "(19.689999999999994, 153)\n",
      "(13.930000000000021, 123)\n",
      "(2.670000000000001, 37)\n",
      "(20.569999999999993, 166)\n",
      "(12.75000000000002, 140)\n",
      "(11.030000000000017, 110)\n"
     ]
    }
   ],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "snake.on_init()\n",
    "state, reward, done = snake.on_feedback()\n",
    "\n",
    "for _ in range(10):\n",
    "    print(play_game(snake, deep_agent, epsilon = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
