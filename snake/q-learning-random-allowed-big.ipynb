{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mThe directory '/home/finn/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mThe directory '/home/finn/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[31mrequests 2.18.4 has requirement urllib3<1.23,>=1.21.1, but you'll have urllib3 1.23 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!sudo pip3 install pygame torchvision -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.23) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import snakai\n",
    "import agents\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n",
    "cpu = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how to play game and replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_size = (30, 30)\n",
    "def tuple_to_torch(tup):\n",
    "    return torch.from_numpy(np.array(tup))\n",
    "\n",
    "action2ind = {'right' : 0,\n",
    "             'left' : 1,\n",
    "             'up' : 2,\n",
    "             'down' : 3}\n",
    "ind2action = {val: key for key, val in action2ind.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(snake, agent, epsilon = 0.05):\n",
    "    cum_reward = 0.0\n",
    "    snake.on_init()\n",
    "    state, reward, ended = snake.on_feedback()\n",
    "\n",
    "    for i in range(200):\n",
    "        action = agent(state, th = epsilon)\n",
    "        next_state, reward, ended = snake.step(action)\n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        # Keep all the games:\n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        if ended == 1:\n",
    "            return cum_reward, i\n",
    "    return cum_reward, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 64\n",
    "ksize = 4\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=ksize, stride=2, padding = 0)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=2, padding = 0)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=2, padding = 0)\n",
    "        #self.dense1 = nn.Linear(2592, 1024)\n",
    "        self.head = nn.Linear(256, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #x = F.relu(self.conv4(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.relu(self.dense1(x))\n",
    "        return 2*F.tanh(self.head(x))\n",
    "    \n",
    "model = DQN().to(device)\n",
    "batch_size = 32\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001) # , weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch():\n",
    "    if len(memory) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    # GET SAMPLE OF DATA\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = tuple_to_torch(batch.state).float().to(device)\n",
    "    next_state_batch = tuple_to_torch(batch.next_state).float().to(device)\n",
    "    action_batch = tuple_to_torch(list(action2ind[a] for a in batch.action)).to(device)\n",
    "    reward_batch = tuple_to_torch(batch.reward).float().to(device)\n",
    "\n",
    "\n",
    "    ## Calculate expected reward:\n",
    "    GAMMA = 0.99\n",
    "    with torch.set_grad_enabled(False):\n",
    "        not_ended_batch = 1 -torch.ByteTensor(batch.ended)\n",
    "        next_states_non_final = next_state_batch[not_ended_batch]\n",
    "        next_state_values = torch.zeros(batch_size).to(device)\n",
    "        reward_hat = model(next_states_non_final)\n",
    "        next_state_values[not_ended_batch] = reward_hat.max(1)[0]\n",
    "        expected_state_action_values = next_state_values*GAMMA + reward_batch\n",
    "\n",
    "\n",
    "    # Predict value function:\n",
    "    yhat = model(state_batch)\n",
    "    state_action_values = yhat.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import random_legal_move\n",
    "def deep_agent(state, th):\n",
    "    \n",
    "    if random.random() < th:\n",
    "        return random_legal_move(state)\n",
    "    \n",
    "    state = torch.unsqueeze(torch.from_numpy(state),0).float().to(device)\n",
    "    yhat = model(state)\n",
    "    action = [ind2action[a] for a in yhat.argmax(1).data.cpu().numpy()]\n",
    "    if len(action) > 1:\n",
    "        raise Exception\n",
    "    action = action[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "\n",
    "# Warmup memory:\n",
    "for _ in range(10):\n",
    "    play_game(snake, deep_agent, epsilon = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(n = 100, epsilon = 0.05):\n",
    "    rewards = np.zeros(n)\n",
    "    for ep in range(n):\n",
    "        rewards[ep],i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "        \n",
    "    return np.mean(rewards)\n",
    "\n",
    "def save_checkpoint():\n",
    "    filename = \"models/snake_big_ep:%02d-reward:%.2f.pth\" %( ep, eval_reward)\n",
    "    torch.save(model.cpu().state_dict(), filename)\n",
    "    model.to(device)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPORT_INTERVAL = 500\n",
    "EVAL_INTERVAL = 2000\n",
    "R = []\n",
    "L = []\n",
    "play_length = []\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.01\n",
    "decay = 0.1/2000\n",
    "start_ep = 0\n",
    "\n",
    "for ep in range(start_ep,1000000):\n",
    "\n",
    "    # Play one game:\n",
    "    epsilon = max(EPS_START - decay*(ep), EPS_END)\n",
    "    r, i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "    R.append(r)\n",
    "    play_length.append(i)\n",
    "    \n",
    "    # Train:\n",
    "    for _ in range(i):\n",
    "        l = train_batch()\n",
    "        L.append(float(l))\n",
    "    \n",
    "    if ep % REPORT_INTERVAL == 0:\n",
    "        print(\"%s: ep: %s \\t reward: %.3f \\t loss: %.4f \\t game len: %.1f \\t epsilon: %.2f\" % \n",
    "              (str(datetime.datetime.now()), ep, np.mean(R), np.mean(L), np.mean(play_length), epsilon), file = open(\"log-q-learn-random-allowed-big.txt\",\"a\"))\n",
    "        R = []\n",
    "        L = []\n",
    "        play_length = []\n",
    "    \n",
    "    if ep % EVAL_INTERVAL == 0:\n",
    "        eval_reward = evaluate_agent()\n",
    "        save_checkpoint()\n",
    "        print(\"%s: ep: %s \\t Reward evaluation: %.2f\" % (str(datetime.datetime.now()), ep, eval_reward), file = open(\"log-q-learn-random-allowed-big.txt\",\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time now:\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate agent with 5% epsilon greedy policy:\n",
    "evaluate_agent(n = 100, epsilon = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agent with greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "snake.on_init()\n",
    "state, reward, done = snake.on_feedback()\n",
    "\n",
    "for _ in range(10):\n",
    "    print(play_game(snake, deep_agent, epsilon = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
