{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo pip3 install pygame -q "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snakai\n",
    "import agents\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as T\n",
    "import datetime\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define how to play game and replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_size = (50,50)\n",
    "def tuple_to_torch(tup):\n",
    "    return torch.from_numpy(np.array(tup))\n",
    "\n",
    "action2ind = {'right' : 0,\n",
    "             'left' : 1,\n",
    "             'up' : 2,\n",
    "             'down' : 3}\n",
    "ind2action = {val: key for key, val in action2ind.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(snake, agent, epsilon = 0.05):\n",
    "    cum_reward = 0.0\n",
    "    snake.on_init()\n",
    "    state, reward, ended = snake.on_feedback()\n",
    "\n",
    "    for i in range(200):\n",
    "        action = agent(state, th = epsilon)\n",
    "        next_state, reward, ended = snake.step(action)\n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        # Keep all the games:\n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        if ended == 1:\n",
    "            return cum_reward, i\n",
    "    return cum_reward, i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch = 64\n",
    "ksize = 4\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, ch, kernel_size=ksize, stride=2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=2, padding = 1)\n",
    "        self.conv4 = nn.Conv2d(ch, ch, kernel_size=ksize, stride=1, padding = 0)\n",
    "\n",
    "        #self.dense1 = nn.Linear(2592, 1024)\n",
    "        self.head = nn.Linear(576, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = F.relu(self.dense1(x))\n",
    "        return 2*F.tanh(self.head(x))\n",
    "    \n",
    "model = DQN()\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imitation_state_dict = torch.load(\"imitation_learning.pth\")\n",
    "#model.load_state_dict(torch.load(\"models/snake_ep:62000-reward:4.32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(model.head.parameters(), lr = 0.001) # , weight_decay = 0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001) # , weight_decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch():\n",
    "    if len(memory) < batch_size:\n",
    "        return 0\n",
    "    \n",
    "    # GET SAMPLE OF DATA\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = tuple_to_torch(batch.state).float()\n",
    "    next_state_batch = tuple_to_torch(batch.next_state).float()\n",
    "    action_batch = tuple_to_torch(list(action2ind[a] for a in batch.action))\n",
    "    reward_batch = tuple_to_torch(batch.reward).float()\n",
    "\n",
    "\n",
    "    ## Calculate expected reward:\n",
    "    GAMMA = 0.99\n",
    "    with torch.set_grad_enabled(False):\n",
    "        not_ended_batch = 1 -torch.ByteTensor(batch.ended)\n",
    "        next_states_non_final = next_state_batch[not_ended_batch]\n",
    "        next_state_values = torch.zeros(batch_size)\n",
    "        reward_hat = model(next_states_non_final)\n",
    "        next_state_values[not_ended_batch] = reward_hat.max(1)[0]\n",
    "        expected_state_action_values = next_state_values*GAMMA + reward_batch\n",
    "\n",
    "\n",
    "    # Predict value function:\n",
    "    yhat = model(state_batch)\n",
    "    state_action_values = yhat.gather(1, action_batch.unsqueeze(1)).squeeze()\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_agent(state, th):\n",
    "    \n",
    "    if random.random() < th:\n",
    "        return random.sample(list(ind2action.values()), 1)[0]\n",
    "    \n",
    "    state = torch.unsqueeze(torch.from_numpy(state),0).float()\n",
    "    yhat = model(state)\n",
    "    action = [ind2action[a] for a in yhat.argmax(1).data.numpy()]\n",
    "    if len(action) > 1:\n",
    "        raise Exception\n",
    "    action = action[0]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=False, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "\n",
    "# Warmup memory:\n",
    "for _ in range(10):\n",
    "    play_game(snake, deep_agent, epsilon = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(n = 100, epsilon = 0.05):\n",
    "    rewards = np.zeros(n)\n",
    "    for ep in range(n):\n",
    "        rewards[ep],i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "        \n",
    "    return np.mean(rewards)\n",
    "\n",
    "def save_checkpoint():\n",
    "    filename = \"models/snakeBig_ep:%02d-reward:%.2f.pth\" %( ep, eval_reward)\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-05-28 06:11:53.353026: ep: 0 \t reward: -1.100 \t loss: 0.0566 \t game len: 10.0 \t epsilon: 0.90\n",
      "2018-05-28 06:12:01.387666: ep: 0 \t Reward evaluation: -1.21\n",
      "2018-05-28 08:45:54.276223: ep: 3400 \t reward: -1.034 \t loss: 0.0134 \t game len: 3.4 \t epsilon: 0.73\n",
      "2018-05-28 08:50:24.036823: ep: 3500 \t reward: -1.039 \t loss: 0.0140 \t game len: 3.9 \t epsilon: 0.72\n",
      "2018-05-28 08:54:55.971099: ep: 3600 \t reward: -1.039 \t loss: 0.0125 \t game len: 3.9 \t epsilon: 0.72\n",
      "2018-05-28 08:59:24.513618: ep: 3700 \t reward: -1.036 \t loss: 0.0139 \t game len: 3.6 \t epsilon: 0.72\n",
      "2018-05-28 09:03:12.413293: ep: 3800 \t reward: -1.040 \t loss: 0.0120 \t game len: 4.0 \t epsilon: 0.71\n",
      "2018-05-28 09:05:57.748268: ep: 3900 \t reward: -1.048 \t loss: 0.0136 \t game len: 4.8 \t epsilon: 0.71\n",
      "2018-05-28 09:08:13.827888: ep: 4000 \t reward: -1.036 \t loss: 0.0110 \t game len: 3.6 \t epsilon: 0.70\n",
      "2018-05-28 09:08:21.119081: ep: 4000 \t Reward evaluation: -1.21\n",
      "2018-05-28 09:10:40.585103: ep: 4100 \t reward: -1.042 \t loss: 0.0245 \t game len: 4.2 \t epsilon: 0.70\n",
      "2018-05-28 09:12:59.771582: ep: 4200 \t reward: -1.046 \t loss: 0.0108 \t game len: 4.6 \t epsilon: 0.69\n",
      "2018-05-28 09:15:19.069560: ep: 4300 \t reward: -1.032 \t loss: 0.0116 \t game len: 4.3 \t epsilon: 0.69\n",
      "2018-05-28 09:17:38.167289: ep: 4400 \t reward: -1.035 \t loss: 0.0086 \t game len: 3.5 \t epsilon: 0.68\n",
      "2018-05-28 09:19:57.537361: ep: 4500 \t reward: -1.039 \t loss: 0.0095 \t game len: 3.9 \t epsilon: 0.68\n",
      "2018-05-28 09:22:16.770130: ep: 4600 \t reward: -1.046 \t loss: 0.0094 \t game len: 4.6 \t epsilon: 0.67\n",
      "2018-05-28 09:24:36.231668: ep: 4700 \t reward: -1.052 \t loss: 0.0097 \t game len: 5.2 \t epsilon: 0.67\n",
      "2018-05-28 09:26:54.660022: ep: 4800 \t reward: -1.040 \t loss: 0.0085 \t game len: 4.0 \t epsilon: 0.66\n",
      "2018-05-28 09:29:13.415041: ep: 4900 \t reward: -1.053 \t loss: 0.0082 \t game len: 5.3 \t epsilon: 0.66\n",
      "2018-05-28 09:31:31.195580: ep: 5000 \t reward: -1.053 \t loss: 0.0084 \t game len: 5.3 \t epsilon: 0.65\n",
      "2018-05-28 09:33:50.332376: ep: 5100 \t reward: -1.049 \t loss: 0.0073 \t game len: 4.9 \t epsilon: 0.65\n",
      "2018-05-28 09:36:13.558709: ep: 5200 \t reward: -1.052 \t loss: 0.0065 \t game len: 5.2 \t epsilon: 0.64\n",
      "2018-05-28 09:38:33.241375: ep: 5300 \t reward: -1.049 \t loss: 0.0075 \t game len: 4.9 \t epsilon: 0.64\n",
      "2018-05-28 09:40:51.882406: ep: 5400 \t reward: -1.051 \t loss: 0.0062 \t game len: 5.1 \t epsilon: 0.63\n",
      "2018-05-28 09:43:11.318803: ep: 5500 \t reward: -1.049 \t loss: 0.0060 \t game len: 4.9 \t epsilon: 0.62\n",
      "2018-05-28 09:45:30.644173: ep: 5600 \t reward: -1.052 \t loss: 0.0054 \t game len: 5.2 \t epsilon: 0.62\n",
      "2018-05-28 09:47:50.152141: ep: 5700 \t reward: -1.052 \t loss: 0.0050 \t game len: 5.2 \t epsilon: 0.61\n",
      "2018-05-28 09:50:10.071985: ep: 5800 \t reward: -1.057 \t loss: 0.0046 \t game len: 5.7 \t epsilon: 0.61\n",
      "2018-05-28 09:52:29.600177: ep: 5900 \t reward: -1.016 \t loss: 0.0048 \t game len: 4.6 \t epsilon: 0.60\n",
      "2018-05-28 09:54:49.185333: ep: 6000 \t reward: -1.049 \t loss: 0.0041 \t game len: 4.9 \t epsilon: 0.60\n",
      "2018-05-28 09:55:09.364106: ep: 6000 \t Reward evaluation: -1.54\n",
      "2018-05-28 09:57:34.732187: ep: 6100 \t reward: -1.062 \t loss: 0.0057 \t game len: 6.2 \t epsilon: 0.59\n",
      "2018-05-28 10:35:34.349758: ep: 7700 \t reward: -1.057 \t loss: 0.0021 \t game len: 5.7 \t epsilon: 0.52\n",
      "2018-05-28 10:55:16.078186: ep: 8500 \t reward: -1.063 \t loss: 0.0019 \t game len: 7.4 \t epsilon: 0.47\n",
      "2018-05-28 10:57:40.567962: ep: 8600 \t reward: -1.069 \t loss: 0.0019 \t game len: 6.9 \t epsilon: 0.47\n",
      "2018-05-28 11:00:04.393546: ep: 8700 \t reward: -1.075 \t loss: 0.0018 \t game len: 7.5 \t epsilon: 0.47\n",
      "2018-05-28 11:02:28.744062: ep: 8800 \t reward: -1.079 \t loss: 0.0020 \t game len: 7.8 \t epsilon: 0.46\n",
      "2018-05-28 11:04:52.900302: ep: 8900 \t reward: -1.076 \t loss: 0.0020 \t game len: 7.6 \t epsilon: 0.46\n",
      "2018-05-28 11:07:17.193611: ep: 9000 \t reward: -1.069 \t loss: 0.0018 \t game len: 6.9 \t epsilon: 0.45\n",
      "2018-05-28 11:09:41.435906: ep: 9100 \t reward: -1.072 \t loss: 0.0014 \t game len: 7.2 \t epsilon: 0.45\n",
      "2018-05-28 11:12:06.339822: ep: 9200 \t reward: -1.076 \t loss: 0.0019 \t game len: 7.6 \t epsilon: 0.44\n",
      "2018-05-28 11:14:30.581120: ep: 9300 \t reward: -1.068 \t loss: 0.0013 \t game len: 6.8 \t epsilon: 0.43\n",
      "2018-05-28 11:19:20.278297: ep: 9500 \t reward: -1.067 \t loss: 0.0014 \t game len: 7.7 \t epsilon: 0.42\n",
      "2018-05-28 11:21:45.924463: ep: 9600 \t reward: -1.079 \t loss: 0.0016 \t game len: 8.9 \t epsilon: 0.42\n",
      "2018-05-28 11:24:10.778604: ep: 9700 \t reward: -1.071 \t loss: 0.0019 \t game len: 7.2 \t epsilon: 0.41\n",
      "2018-05-28 11:26:36.239003: ep: 9800 \t reward: -1.085 \t loss: 0.0013 \t game len: 8.5 \t epsilon: 0.41\n",
      "2018-05-28 11:29:02.321583: ep: 9900 \t reward: -1.086 \t loss: 0.0016 \t game len: 9.6 \t epsilon: 0.40\n",
      "2018-05-28 11:31:27.286180: ep: 10000 \t reward: -1.083 \t loss: 0.0016 \t game len: 8.3 \t epsilon: 0.40\n",
      "2018-05-28 11:31:48.866822: ep: 10000 \t Reward evaluation: -1.62\n",
      "2018-05-28 11:34:16.748429: ep: 10100 \t reward: -1.079 \t loss: 0.0023 \t game len: 7.9 \t epsilon: 0.40\n",
      "2018-05-28 11:36:43.916058: ep: 10200 \t reward: -1.072 \t loss: 0.0015 \t game len: 7.2 \t epsilon: 0.39\n",
      "2018-05-28 11:39:30.460031: ep: 10300 \t reward: -1.083 \t loss: 0.0016 \t game len: 9.3 \t epsilon: 0.39\n",
      "2018-05-28 11:41:57.437585: ep: 10400 \t reward: -1.101 \t loss: 0.0012 \t game len: 10.1 \t epsilon: 0.38\n",
      "2018-05-28 11:44:24.166938: ep: 10500 \t reward: -1.100 \t loss: 0.0014 \t game len: 10.0 \t epsilon: 0.38\n",
      "2018-05-28 11:46:49.728210: ep: 10600 \t reward: -1.092 \t loss: 0.0019 \t game len: 9.2 \t epsilon: 0.37\n",
      "2018-05-28 11:49:15.603700: ep: 10700 \t reward: -1.088 \t loss: 0.0011 \t game len: 8.8 \t epsilon: 0.36\n",
      "2018-05-28 11:51:41.730961: ep: 10800 \t reward: -1.093 \t loss: 0.0013 \t game len: 9.3 \t epsilon: 0.36\n",
      "2018-05-28 11:54:07.724606: ep: 10900 \t reward: -1.067 \t loss: 0.0013 \t game len: 8.8 \t epsilon: 0.35\n",
      "2018-05-28 11:56:38.759797: ep: 11000 \t reward: -1.078 \t loss: 0.0013 \t game len: 8.8 \t epsilon: 0.35\n",
      "2018-05-28 11:59:03.987006: ep: 11100 \t reward: -1.094 \t loss: 0.0010 \t game len: 9.4 \t epsilon: 0.34\n",
      "2018-05-28 12:01:28.704892: ep: 11200 \t reward: -1.083 \t loss: 0.0011 \t game len: 8.3 \t epsilon: 0.34\n",
      "2018-05-28 12:03:52.735463: ep: 11300 \t reward: -1.085 \t loss: 0.0012 \t game len: 8.5 \t epsilon: 0.33\n",
      "2018-05-28 12:06:17.627044: ep: 11400 \t reward: -1.109 \t loss: 0.0017 \t game len: 10.9 \t epsilon: 0.33\n",
      "2018-05-28 12:08:51.513249: ep: 11500 \t reward: -1.107 \t loss: 0.0010 \t game len: 10.7 \t epsilon: 0.32\n",
      "2018-05-28 12:11:17.162988: ep: 11600 \t reward: -1.065 \t loss: 0.0011 \t game len: 9.5 \t epsilon: 0.32\n",
      "2018-05-28 12:13:48.939168: ep: 11700 \t reward: -1.100 \t loss: 0.0013 \t game len: 10.0 \t epsilon: 0.31\n",
      "2018-05-28 12:16:15.685947: ep: 11800 \t reward: -1.118 \t loss: 0.0012 \t game len: 11.8 \t epsilon: 0.31\n",
      "2018-05-28 12:18:42.842983: ep: 11900 \t reward: -1.121 \t loss: 0.0009 \t game len: 12.1 \t epsilon: 0.31\n",
      "2018-05-28 12:21:08.531475: ep: 12000 \t reward: -1.113 \t loss: 0.0018 \t game len: 11.3 \t epsilon: 0.30\n",
      "2018-05-28 12:21:27.878311: ep: 12000 \t Reward evaluation: -1.55\n",
      "2018-05-28 12:23:56.777264: ep: 12100 \t reward: -1.102 \t loss: 0.0024 \t game len: 12.2 \t epsilon: 0.30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d8fba3a10e5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Train:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-6eae6135543f>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mnext_states_non_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnot_ended_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnext_state_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mreward_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states_non_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mnext_state_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnot_ended_batch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mexpected_state_action_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state_values\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreward_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-83602c9f78fd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "REPORT_INTERVAL = 100\n",
    "EVAL_INTERVAL = 2000\n",
    "R = []\n",
    "L = []\n",
    "play_length = []\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "decay = 0.1/2000\n",
    "start_ep = 0\n",
    "\n",
    "for ep in range(100000):\n",
    "    \n",
    "    # Play one game:\n",
    "    epsilon = max(EPS_START - decay*(ep), EPS_END)\n",
    "    r, i = play_game(snake, deep_agent, epsilon = epsilon)\n",
    "    R.append(r)\n",
    "    play_length.append(i)\n",
    "    \n",
    "    # Train:\n",
    "    for _ in range(10):\n",
    "        l = train_batch()\n",
    "        L.append(float(l))\n",
    "    \n",
    "    if ep % REPORT_INTERVAL == 0:\n",
    "        print(\"%s: ep: %s \\t reward: %.3f \\t loss: %.4f \\t game len: %.1f \\t epsilon: %.2f\" % \n",
    "              (str(datetime.datetime.now()), ep, np.mean(R), np.mean(L), np.mean(play_length), epsilon))\n",
    "        R = []\n",
    "        L = []\n",
    "        play_length = []\n",
    "    \n",
    "    if ep % EVAL_INTERVAL == 0:\n",
    "        eval_reward = evaluate_agent()\n",
    "        save_checkpoint()\n",
    "        print(\"%s: ep: %s \\t Reward evaluation: %.2f\" % (str(datetime.datetime.now()), ep, eval_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time now:\n",
    "ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate agent with 5% epsilon greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate agent with greedy policy:\n",
    "evaluate_agent(n = 1000, epsilon = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake = snakai.Snake(render=True, \n",
    "                     game_size = game_size, \n",
    "                     time_reward = -0.01)\n",
    "snake.on_init()\n",
    "state, reward, done = snake.on_feedback()\n",
    "\n",
    "for _ in range(10):\n",
    "    print(play_game(snake, deep_agent, epsilon = 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
