{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from wrappers import wrap_deepmind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward','ended'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayMemory(5000)\n",
    "# plt.imshow(memory.memory[0].state[:3,].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def n2t(vec):\n",
    "    return torch.from_numpy(vec).to(device)\n",
    "    \n",
    "def t2n(tensor):\n",
    "    return tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env = wrap_deepmind(gym.make(\"Pong-v0\"), frame_stack = True), agent = None, th = 0, maxstep = 5000, render = False):\n",
    "    cum_reward = 0.0\n",
    "    render_frames = []\n",
    "    state = env.reset()\n",
    "    \n",
    "\n",
    "    for i in range(maxstep):\n",
    "        # take action:\n",
    "        action = agent(state, th = th)\n",
    "        next_state, reward, ended, info = env.step(action)\n",
    "        \n",
    "        cum_reward += float(reward)\n",
    "        \n",
    "        # push to replay buffer:\n",
    "        memory.push(state, action, next_state, reward, ended)\n",
    "        state = next_state\n",
    "        \n",
    "        if render:\n",
    "            if i % 3 == 0:\n",
    "                render_frames.append(torch.from_numpy(env.render(mode=\"rgb_array\")).unsqueeze(0))\n",
    "        if ended == 1:\n",
    "            break\n",
    "            \n",
    "    out = {'cum_reward' : cum_reward, 'steps' :  i}\n",
    "    if render:\n",
    "        out['frames'] = torch.cat(render_frames).permute(3,0,1,2).unsqueeze(0)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'env' : 'Pong-v0',\n",
    "         'batch_size' : 16,\n",
    "        'GAMMA' : 0.7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_agent(state, th = None):\n",
    "    return random.randint(a=0,b=env.action_space.n-1)\n",
    "\n",
    "def dqn_epsilon_agent(state, th = 0.05):\n",
    "    if random.random() > th:\n",
    "        yhat = model(default_states_preprocessor(state))\n",
    "        return int(yhat.argmax().cpu().numpy())\n",
    "    else:\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model' from '/Users/simeide/Sync/edm/RL/atari/model.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from importlib import reload \n",
    "import model\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "reload(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_states_preprocessor(states):\n",
    "    \"\"\"\n",
    "    Convert list of states into the form suitable for model. By default we assume Variable\n",
    "    :param states: list of numpy arrays with states\n",
    "    :return: Variable\n",
    "    \n",
    "    Obtained from https://github.com/Shmuma/ptan/blob/master/ptan/agent.py\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(states,list):\n",
    "        states = [states]\n",
    "    \n",
    "    if len(states) == 1:\n",
    "        np_states = np.expand_dims(states[0], 0)\n",
    "    else:\n",
    "        np_states = np.array([np.array(s, copy=False) for s in states], copy=False)\n",
    "    return torch.tensor(np_states).permute(0,3,1,2).float().to(device)\n",
    "\n",
    "\n",
    "def train_batch(param):\n",
    "    if len(memory) < param['batch_size']:\n",
    "        return 0\n",
    "    batch = memory.sample(param['batch_size'])\n",
    "    batch_states = default_states_preprocessor([m.state for m in batch])\n",
    "    batch_next_states = default_states_preprocessor([m.next_state for m in batch])\n",
    "    batch_ended = torch.tensor([m.ended for m in batch])\n",
    "    batch_rewards = torch.tensor([m.reward for m in batch])\n",
    "    batch_actions = torch.tensor([m.action for m in batch])\n",
    "\n",
    "    ## Calculate expected reward:\n",
    "    with torch.set_grad_enabled(False):\n",
    "        not_ended_batch = 1 -torch.ByteTensor(batch_ended)\n",
    "        next_states_non_final = batch_next_states[not_ended_batch]\n",
    "        next_state_values = torch.zeros(param['batch_size']).to(device)\n",
    "        reward_hat = model(next_states_non_final)\n",
    "        next_state_values[not_ended_batch] = reward_hat.max(1)[0]\n",
    "        expected_state_action_values = next_state_values*param['GAMMA'] + batch_rewards\n",
    "\n",
    "    # Predict value function:\n",
    "    yhat = model(batch_states)\n",
    "    state_action_values = yhat.gather(1, batch_actions.unsqueeze(1)).squeeze()\n",
    "\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in model.parameters():\n",
    "        param.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    return loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "version = \", \".join([ \"{}:{}\".format(key,val) for key, val in param.items()]) + \" \"+str(datetime.datetime.now())[:16]\n",
    "writer = SummaryWriter(log_dir = \"tensorboard/\" + version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MoviePy] Building file /var/folders/qq/j7fg85k93z59k9lkx9867qm4yy4xsx/T/tmp8z53m2ar.gif with imageio\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 339/340 [00:00<00:00, 347.87it/s]\n"
     ]
    }
   ],
   "source": [
    "env = wrap_deepmind(gym.make(param['env']), frame_stack = True)\n",
    "model = model.DQN(num_actions = env.action_space.n).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001) # , weight_decay = 0.001\n",
    "\n",
    "# Warmup buffer\n",
    "for _ in range(5):\n",
    "    game = play_game(env, agent = dqn_epsilon_agent, th = 0.5)\n",
    "\n",
    "step = 0\n",
    "loss, rewards, episode_steps = {}, {}, {}\n",
    "for episode in range(10000):\n",
    "    \n",
    "    ## PLAY GAME\n",
    "    game = play_game(env, agent = dqn_epsilon_agent, th = 0.5)\n",
    "    rewards['run_reward'], episode_steps['run_episode_steps'] = game['cum_reward'], game['steps']\n",
    "    step += episode_steps['run_episode_steps']\n",
    "    \n",
    "    ## TRAIN\n",
    "    for _ in range(episode_steps['run_episode_steps']//param['batch_size']):\n",
    "        loss['run_loss'] = train_batch(param)\n",
    "    \n",
    "    \n",
    "    # Test agent:\n",
    "    if episode % 10 == 0:\n",
    "        game = play_game(env, agent = dqn_epsilon_agent, th = 0.05)\n",
    "        rewards['test_reward'], episode_steps['test_episode_steps'] = game['cum_reward'], game['steps']\n",
    "    \n",
    "    \n",
    "    # REPORTING\n",
    "    if episode % 5 == 0:\n",
    "        writer.add_scalars(\"loss\", tag_scalar_dict=loss, global_step= step)\n",
    "        writer.add_scalars(\"rewards\", rewards, step)\n",
    "        writer.add_scalar(\"episode\", episode, global_step = step)\n",
    "        \n",
    "    # Animate agent:\n",
    "    if episode % 100 == 0:\n",
    "        game = play_game(env, agent = dqn_epsilon_agent, th = 0.02, render = True)\n",
    "        writer.add_video(\"test_game\", game['frames'], global_step = step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
